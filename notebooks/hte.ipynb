{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2584a5c5",
   "metadata": {},
   "source": [
    "# H T E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd4a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  5 15:22:39 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   31C    P0              26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee578c",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1335cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers bitsandbytes datasets huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6594aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa75b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17adb1bfc8b047a6b5275f26c6ac2716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b688705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /home/ec2-user/SageMaker/sandbox/hte\n",
      "PYTHONPATH updated with: /home/ec2-user/SageMaker/sandbox/hte\n"
     ]
    }
   ],
   "source": [
    "project_dir = '/home/ec2-user/SageMaker/sandbox/hte'\n",
    "\n",
    "os.chdir(project_dir)\n",
    "print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "    \n",
    "    \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)  # Add it to the front of PYTHONPATH\n",
    "    print(f\"PYTHONPATH updated with: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d06bcf",
   "metadata": {},
   "source": [
    "## Load the data and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e4a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/importlib/__init__.py:126: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 165359\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9344\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wiki40b\", \"he\")  # Specific version and Hebrew language code\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d8da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def decode_text(text):\n",
    "    decoded_text = bytes(text, \"utf-8\").decode(\"unicode_escape\").encode(\"latin1\").decode(\"utf-8\")\n",
    "    return decoded_text\n",
    "\n",
    "# Apply the decoding function to the dataset\n",
    "decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5773d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_article(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    PARAGRAPH_DIVIDER = '_NEWLINE_'\n",
    "\n",
    "    # Initialize variables\n",
    "    article_dict = {'title': '', 'abstract': '', 'sections': []}\n",
    "    current_section = None\n",
    "    abstract_parsed = False\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if line == \"_START_ARTICLE_\":\n",
    "            # The next line is the title\n",
    "            article_dict['title'] = lines[i + 1].strip()\n",
    "            i += 2  # Move to the next relevant line\n",
    "        elif line == \"_START_PARAGRAPH_\":\n",
    "            # If the abstract has not been parsed and the current section is None, this is the abstract\n",
    "            paragraph = lines[i + 1].strip()\n",
    "            if not abstract_parsed and not current_section:\n",
    "                article_dict['abstract'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "                abstract_parsed = True\n",
    "            elif current_section:\n",
    "                current_section['paragraphs'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "            i += 2\n",
    "        elif line == \"_START_SECTION_\":\n",
    "            # The next line is the section name\n",
    "            section_name = lines[i + 1].strip()\n",
    "            current_section = {'section': section_name, 'paragraphs': ''}\n",
    "            article_dict['sections'].append(current_section)\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1  # Move to the next line if none of the cases match\n",
    "\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a522de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'הקמרילה של קרול השני, מלך רומניה',\n",
       " 'abstract': ['הקמרילה של קרול השני, מלך רומניה (ברומנית: Camarila lui Carol al II lea) הוא השם המקובל של החוג הפנימי של קרול השני, מלך רומניה. הקמרילה השפיעה על הכלכלה והפוליטיקה הרומנית ותפסה חלק ניכר מהכיסוי התקשורתי של התקופה.',\n",
       "  'הקמרילה עמדה במרכז התקפות האופוזיציה על ממשלו של המלך קרול השני ועם הדחתו הגיעה לקץ השפעתה וחלק מחבריה עזבו את רומניה יחד עם המלך.'],\n",
       " 'sections': [{'section': 'חברי הקמרילה',\n",
       "   'paragraphs': ['במרכז הקמרילה עמדו בעלי תפקידים בארמון, פילגש המלך, אלנה לופסקו, תעשיינים ובנקאים גדולים ותופסי עמדות מפתח בממשל. בין חברי הקמרילה היו אנטישמיים ויהודים, חלק פעלו בשיתוף פעולה וחלקם זממו האחד נגד חברו. היו חברי קמרילה שהודרו והיו אחרים שצורפו.',\n",
       "    'פויו (קונסטנטין) דומיטרסקו (Puiu (Constantin) Dumitrescu), בנו של קולונל רומני, היה סטודנט בפריז בתקופה בה שהה שם הזוג קרול קאראימאן (השם של קרול השני בתקופה בה ויתר על ירושת המלוכה וגלה מארצו) ואלנה לופסקו. הוא ביצע עבור קרול שירותים אישיים וליווה אותו לבתי הימורים. עם שובו של קרול לרומניה והכתרתו ב-1930, ליווה אותו דומיטרסקו והפך למזכיר האישי של המלך ועמוד ראשי בקמרילה שלו. הוא זכור גם בגלל \"פרשת המטפחות\", כאשר אביו של דומיטרסקו (שקודם לדרגת גנרל על ידי המלך ומונה למפקד הז\\'נדרמריה) ובן דוד של אלנה לופסקו הפיקו מטפחות עם דיוקנו של המלך קרול וכל אנשי הצבא והמשטרה חויבו לרכוש מטפחת כזאת. בינואר 1934, לאחר הרצח של יון ג. דוקה, אולץ דומיטרסקו לעזוב את רומניה ומאוחר יותר, ב-1935, האב הועמד למשפט באשמת שחיתות ונידון לחמש שנות מאסר ומת בכלא - היו שמועות על הרעלה, אך לא נערכה בדיקה. באותה תקופה צונזרה התקשורת בכל הנוגע לדומיטרסקו, לכן לא ברור מה הייתה הסיבה להרחקתו, יש הקושרים זאת לעימות בינו ובין אלנה לופסקו ויש הקושרים זאת לתביעות של פוליטיקאים, שבאו במקום דוקה.',\n",
       "    'ארנסט אורדראנו, שהתמנה במקומו של דומיטרסקו, היה שחקן מרכזי בקמרילה ושיתף פעולה היטב עם אלנה לופסקו. הוא היה נאמן למלך וליווה אותו לאחר יציאתו לגלות ועד מותו.',\n",
       "    'הבנקאי היהודי אריסטידה בלנק סייע בסכומי כסף גדולים למלך קרול לפני שהיה מלך, בתקופה בה הודח מזכויותיו כבן משפחת המלוכה וחי בצרפת ללא שום הכנסה. הסיוע של בלנק, שניתן ללא התניה כלשהי, הפך את בלנק לאחד המקורבים ביותר למלך לאחר המלכתו.',\n",
       "    'פרופסור נאה יונסקו היה פילוסוף ותאולוג נוצרי אורתודוקסי, אוהד התנועה הלגיונרית והמשטרים הפשיסטיים. הוא היה בעל קשרים טובים בגרמניה הנאצית ועזר לניקולאה מאלאקסה להשיג חוזים משתלמים איתה וקיבל ממנו תמורה יפה.',\n",
       "    \"גבריאל מרינסקו היה במשך שנים מפקד משטרת בוקרשט ובהמשך גם תת-שר הפנים, שר הפנים ושר הממונה על הסדר הציבורי. נרצח בטבח ז'ילבה.\",\n",
       "    'מקס אאושניט היה תעשיין יהודי ברומניה, מתחרה של ניקולאה מאלאקסה וחבר נכבד בקמרילה המלכותית עד שהמלך שינה את טעמו ודרש ממנו להעביר לו את הבעלות על עסקיו וכשסירב שלח אותו לבית סוהר.',\n",
       "    'ניקולאה מאלאקסה היה תעשיין רומני ממוצא יווני וחבר נכבד בקמרילה המלכותית, שהתחרה בהתמדה במקס אאושניט. כשאאושניט נתן למלך במתנה סוס מרוצים ערבי טהור גזע, נתן מאלאקסה למלך אורווה מודרנית מצוידת היטב.',\n",
       "    \"מיכאיל מורוזוב היה ראש הסיגורנצה ועשה שירותים שונים למען המלך ופילגשו. נרצח בטבח ז'ילבה.\",\n",
       "    \"גאורגה טטרסקו, בן אצולה רומני בעל הכשרה של משפטן, פנה לפוליטיקה וקרבתו למלך הביאה אותו פעמיים לכס ראשות הממשלה.'\"]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "text = decoded_dataset['train'][12]['text']\n",
    "parsed_article = parse_wiki_article(text)\n",
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4a28a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac91230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_wiki40b(tokenizer):\n",
    "    dataset = load_dataset(\"wiki40b\", \"he\")\n",
    "    decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})\n",
    "\n",
    "    def transform_entry(entry):\n",
    "        # Process the 'text' using parse_wiki_article\n",
    "        article = parse_wiki_article(entry['text'])\n",
    "\n",
    "        # Extract anchor_text and positive_text based on the parsed output\n",
    "        anchor_text = article['title']\n",
    "        if 'sections' in article and len(article['sections']) > 0:\n",
    "            anchor_text += \" \" + article['sections'][0]['section']\n",
    "            positive_text = article['sections'][0]['paragraphs'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "        else:\n",
    "            positive_text = article['abstract'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "\n",
    "        # Return the transformed data\n",
    "        return {\n",
    "            'anchor_text': anchor_text,\n",
    "            'positive_text': positive_text\n",
    "        }\n",
    "\n",
    "    # Apply the transformation to the train, validation, and test subsets\n",
    "    transformed_dataset = {}\n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        # Transform each subset of the dataset using map (this processes each 'text' entry)\n",
    "        transformed_subset = decoded_dataset[subset].map(transform_entry)\n",
    "        transformed_dataset[subset] = transformed_subset\n",
    "\n",
    "    # Return the transformed dataset as a DatasetDict\n",
    "    return DatasetDict(transformed_dataset)\n",
    "\n",
    "\n",
    "def transform_dataset(dataset_name, **kwargs):\n",
    "    if dataset_name == 'wiki40b':\n",
    "        return transform_dataset_wiki40b(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253b117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the logger\n",
    "def setup_logger(log_file):\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create handlers for both console and file output\n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # Set up the format for logging\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b041f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - temperature: Scaling factor applied to the logits before applying the softmax function.\n",
    "        \"\"\"\n",
    "        super(InfoNCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negatives):\n",
    "        \"\"\"\n",
    "        Compute the InfoNCE loss.\n",
    "\n",
    "        Parameters:\n",
    "        - anchor: Tensor of shape (batch_size, embedding_dim) - anchor samples\n",
    "        - positive: Tensor of shape (batch_size, embedding_dim) - positive samples corresponding to each anchor\n",
    "        - negatives: Tensor of shape (batch_size, num_negatives, embedding_dim) - negative samples\n",
    "\n",
    "        Returns:\n",
    "        - loss: Computed InfoNCE loss\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        num_negatives = negatives.size(1)\n",
    "\n",
    "        # Normalize embeddings to unit vectors\n",
    "        anchor = F.normalize(anchor, dim=-1)\n",
    "        positive = F.normalize(positive, dim=-1)\n",
    "        negatives = F.normalize(negatives, dim=-1)\n",
    "\n",
    "        # Calculate the positive logits (similarity between anchor and positive)\n",
    "        positive_logits = torch.sum(anchor * positive, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Calculate the negative logits (similarity between anchor and negatives)\n",
    "        negative_logits = torch.bmm(negatives, anchor.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, num_negatives)\n",
    "\n",
    "        # Concatenate positive and negative logits\n",
    "        logits = torch.cat([positive_logits, negative_logits], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        # Create labels - 0 for the positive samples, as it is the first in the concatenated logits\n",
    "        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)\n",
    "\n",
    "        # Compute the InfoNCE loss using cross-entropy\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bad7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, checkpoint_dir):\n",
    "    latest_checkpoint = None\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = sorted(checkpoint_files)[-1]  # Get the latest checkpoint\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        logger.info(f\"Loading checkpoint {latest_checkpoint}\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch']  # return the epoch to resume from\n",
    "\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
    "    return 0  # Start from the first epoch if no checkpoint found\n",
    "\n",
    "\n",
    "# Save model and optimizer state\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
    "    create_directory(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            anchor_ids, positive_ids, *negative_ids = batch\n",
    "            \n",
    "            # Forward pass to get embeddings for validation\n",
    "            anchor_embeds = model(anchor_ids).last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "            positive_embeds = model(positive_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Process multiple negatives\n",
    "            negatives_embeds = torch.stack([\n",
    "                model(negative_id_batch).last_hidden_state[:, 0, :] for negative_id_batch in negative_ids\n",
    "            ], dim=1)\n",
    "\n",
    "            # Compute the validation loss\n",
    "            val_loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    criterion, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    epochs, \n",
    "    start_epoch=0, \n",
    "    checkpoint_dir='checkpoints', \n",
    "):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            anchor_ids, anchor_mask, positive_ids, positive_mask = batch\n",
    "\n",
    "            # Forward pass to get the embeddings\n",
    "            anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "            anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n",
    "            positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            # Set negatives as the other positives in the batch\n",
    "            # Create a matrix where the negatives are shifted versions of positives\n",
    "            batch_size = positive_embeds.size(0)\n",
    "            negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n",
    "\n",
    "            # Compute the InfoNCE loss\n",
    "            loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Compute validation loss after each epoch\n",
    "        avg_val_loss = validate(model, val_dataloader, criterion)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch, \"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90891a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe5f5674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720181eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'intfloat/multilingual-e5-large'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "INFONCE_TEMPERATURE = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94154f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "logger = setup_logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 15:23:02,479 - INFO - Start train base model: intfloat/multilingual-e5-large\n",
      "2024-09-05 15:23:03,829 - INFO - Switching to new dataset: wiki40b\n",
      "2024-09-05 15:24:13,890 - INFO - No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Log file setup\n",
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "\n",
    "\n",
    "# Define model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "logger.info(f\"Start train base model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize the InfoNCE loss and the optimizer\n",
    "criterion = InfoNCELoss(temperature=INFONCE_TEMPERATURE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Datasets to train on\n",
    "dataset_names = ['wiki40b']\n",
    "\n",
    "# Iterate over datasets and train\n",
    "for dataset_name in dataset_names:\n",
    "    start_datetime = datetime.now()\n",
    "    \n",
    "    logger.info(f\"Switching to new dataset: {dataset_name}\")\n",
    "    dataset = transform_dataset(dataset_name, tokenizer=tokenizer)\n",
    "    \n",
    "    # Tokenize the train dataset\n",
    "    anchor_inputs_train = tokenizer(dataset['train']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_train = tokenizer(dataset['train']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    train_dataset = TensorDataset(anchor_inputs_train['input_ids'], anchor_inputs_train['attention_mask'], \n",
    "                                  positive_inputs_train['input_ids'], positive_inputs_train['attention_mask'])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Tokenize the validation dataset\n",
    "    anchor_inputs_val = tokenizer(dataset['validation']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_val = tokenizer(dataset['validation']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Create DataLoader for validation\n",
    "    val_dataset = TensorDataset(anchor_inputs_val['input_ids'], anchor_inputs_val['attention_mask'], \n",
    "                                positive_inputs_val['input_ids'], positive_inputs_val['attention_mask'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load the latest checkpoint if available and resume training\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_dir)\n",
    "\n",
    "    # Train the model for this dataset\n",
    "    train(\n",
    "        model=model, \n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_dataloader=train_dataloader, \n",
    "        val_dataloader=val_dataloader, \n",
    "        epochs=3, \n",
    "        start_epoch=start_epoch,\n",
    "    )\n",
    "    \n",
    "    end_datetime = datetime.now()\n",
    "    logger.info(f\"Total training on {dataset_name} elapsed time is {(end_datetime - start_datetime).total_seconds()} seconds\")\n",
    "    \n",
    "logger.info(f\"End train base model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b11d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f01cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2252d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
