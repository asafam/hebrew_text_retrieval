{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2584a5c5",
   "metadata": {},
   "source": [
    "# H T E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd4a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee578c",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1335cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers bitsandbytes datasets huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6594aa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa75b708",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/huggingface_hub/_login.py:238\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_jSKEIpWrXQwCpiFYHPaGQthzOkWYzSYZfq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m notebook_login()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/huggingface_hub/_login.py:241\u001b[0m, in \u001b[0;36mnotebook_login\u001b[0;34m(new_session, write_permission)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `notebook_login` function can only be used in a notebook (Jupyter or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_session \u001b[38;5;129;01mand\u001b[39;00m _current_token_okay(write_permission\u001b[38;5;241m=\u001b[39mwrite_permission):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser is already logged in.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`."
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2734386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /Users/asafam/Workspace/biu/hebrew_text_encoder\n",
      "PYTHONPATH updated with: /Users/asafam/Workspace/biu/hebrew_text_encoder/src\n"
     ]
    }
   ],
   "source": [
    "project_dir = os.getcwd() if not os.getcwd().split(\"/\")[-1] == 'notebooks' else '/'.join(os.getcwd().split(\"/\")[0:-1])\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "\n",
    "os.chdir(project_dir)\n",
    "print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.insert(0, src_dir)  # Add it to the front of PYTHONPATH\n",
    "    print(f\"PYTHONPATH updated with: {src_dir}\")\n",
    "else:\n",
    "    print(f\"PYTHONPATH already contains: {src_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d06bcf",
   "metadata": {},
   "source": [
    "## Load the data and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e4a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/importlib/__init__.py:126: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 165359\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9344\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wiki40b\", \"he\")  # Specific version and Hebrew language code\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d8da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def decode_text(text):\n",
    "    decoded_text = bytes(text, \"utf-8\").decode(\"unicode_escape\").encode(\"latin1\").decode(\"utf-8\")\n",
    "    return decoded_text\n",
    "\n",
    "# Apply the decoding function to the dataset\n",
    "decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5773d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_article(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    PARAGRAPH_DIVIDER = '_NEWLINE_'\n",
    "\n",
    "    # Initialize variables\n",
    "    article_dict = {'title': '', 'abstract': '', 'sections': []}\n",
    "    current_section = None\n",
    "    abstract_parsed = False\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if line == \"_START_ARTICLE_\":\n",
    "            # The next line is the title\n",
    "            article_dict['title'] = lines[i + 1].strip()\n",
    "            i += 2  # Move to the next relevant line\n",
    "        elif line == \"_START_PARAGRAPH_\":\n",
    "            # If the abstract has not been parsed and the current section is None, this is the abstract\n",
    "            paragraph = lines[i + 1].strip()\n",
    "            if not abstract_parsed and not current_section:\n",
    "                article_dict['abstract'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "                abstract_parsed = True\n",
    "            elif current_section:\n",
    "                current_section['paragraphs'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "            i += 2\n",
    "        elif line == \"_START_SECTION_\":\n",
    "            # The next line is the section name\n",
    "            section_name = lines[i + 1].strip()\n",
    "            current_section = {'section': section_name, 'paragraphs': ''}\n",
    "            article_dict['sections'].append(current_section)\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1  # Move to the next line if none of the cases match\n",
    "\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a522de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'הקמרילה של קרול השני, מלך רומניה',\n",
       " 'abstract': ['הקמרילה של קרול השני, מלך רומניה (ברומנית: Camarila lui Carol al II lea) הוא השם המקובל של החוג הפנימי של קרול השני, מלך רומניה. הקמרילה השפיעה על הכלכלה והפוליטיקה הרומנית ותפסה חלק ניכר מהכיסוי התקשורתי של התקופה.',\n",
       "  'הקמרילה עמדה במרכז התקפות האופוזיציה על ממשלו של המלך קרול השני ועם הדחתו הגיעה לקץ השפעתה וחלק מחבריה עזבו את רומניה יחד עם המלך.'],\n",
       " 'sections': [{'section': 'חברי הקמרילה',\n",
       "   'paragraphs': ['במרכז הקמרילה עמדו בעלי תפקידים בארמון, פילגש המלך, אלנה לופסקו, תעשיינים ובנקאים גדולים ותופסי עמדות מפתח בממשל. בין חברי הקמרילה היו אנטישמיים ויהודים, חלק פעלו בשיתוף פעולה וחלקם זממו האחד נגד חברו. היו חברי קמרילה שהודרו והיו אחרים שצורפו.',\n",
       "    'פויו (קונסטנטין) דומיטרסקו (Puiu (Constantin) Dumitrescu), בנו של קולונל רומני, היה סטודנט בפריז בתקופה בה שהה שם הזוג קרול קאראימאן (השם של קרול השני בתקופה בה ויתר על ירושת המלוכה וגלה מארצו) ואלנה לופסקו. הוא ביצע עבור קרול שירותים אישיים וליווה אותו לבתי הימורים. עם שובו של קרול לרומניה והכתרתו ב-1930, ליווה אותו דומיטרסקו והפך למזכיר האישי של המלך ועמוד ראשי בקמרילה שלו. הוא זכור גם בגלל \"פרשת המטפחות\", כאשר אביו של דומיטרסקו (שקודם לדרגת גנרל על ידי המלך ומונה למפקד הז\\'נדרמריה) ובן דוד של אלנה לופסקו הפיקו מטפחות עם דיוקנו של המלך קרול וכל אנשי הצבא והמשטרה חויבו לרכוש מטפחת כזאת. בינואר 1934, לאחר הרצח של יון ג. דוקה, אולץ דומיטרסקו לעזוב את רומניה ומאוחר יותר, ב-1935, האב הועמד למשפט באשמת שחיתות ונידון לחמש שנות מאסר ומת בכלא - היו שמועות על הרעלה, אך לא נערכה בדיקה. באותה תקופה צונזרה התקשורת בכל הנוגע לדומיטרסקו, לכן לא ברור מה הייתה הסיבה להרחקתו, יש הקושרים זאת לעימות בינו ובין אלנה לופסקו ויש הקושרים זאת לתביעות של פוליטיקאים, שבאו במקום דוקה.',\n",
       "    'ארנסט אורדראנו, שהתמנה במקומו של דומיטרסקו, היה שחקן מרכזי בקמרילה ושיתף פעולה היטב עם אלנה לופסקו. הוא היה נאמן למלך וליווה אותו לאחר יציאתו לגלות ועד מותו.',\n",
       "    'הבנקאי היהודי אריסטידה בלנק סייע בסכומי כסף גדולים למלך קרול לפני שהיה מלך, בתקופה בה הודח מזכויותיו כבן משפחת המלוכה וחי בצרפת ללא שום הכנסה. הסיוע של בלנק, שניתן ללא התניה כלשהי, הפך את בלנק לאחד המקורבים ביותר למלך לאחר המלכתו.',\n",
       "    'פרופסור נאה יונסקו היה פילוסוף ותאולוג נוצרי אורתודוקסי, אוהד התנועה הלגיונרית והמשטרים הפשיסטיים. הוא היה בעל קשרים טובים בגרמניה הנאצית ועזר לניקולאה מאלאקסה להשיג חוזים משתלמים איתה וקיבל ממנו תמורה יפה.',\n",
       "    \"גבריאל מרינסקו היה במשך שנים מפקד משטרת בוקרשט ובהמשך גם תת-שר הפנים, שר הפנים ושר הממונה על הסדר הציבורי. נרצח בטבח ז'ילבה.\",\n",
       "    'מקס אאושניט היה תעשיין יהודי ברומניה, מתחרה של ניקולאה מאלאקסה וחבר נכבד בקמרילה המלכותית עד שהמלך שינה את טעמו ודרש ממנו להעביר לו את הבעלות על עסקיו וכשסירב שלח אותו לבית סוהר.',\n",
       "    'ניקולאה מאלאקסה היה תעשיין רומני ממוצא יווני וחבר נכבד בקמרילה המלכותית, שהתחרה בהתמדה במקס אאושניט. כשאאושניט נתן למלך במתנה סוס מרוצים ערבי טהור גזע, נתן מאלאקסה למלך אורווה מודרנית מצוידת היטב.',\n",
       "    \"מיכאיל מורוזוב היה ראש הסיגורנצה ועשה שירותים שונים למען המלך ופילגשו. נרצח בטבח ז'ילבה.\",\n",
       "    \"גאורגה טטרסקו, בן אצולה רומני בעל הכשרה של משפטן, פנה לפוליטיקה וקרבתו למלך הביאה אותו פעמיים לכס ראשות הממשלה.'\"]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "text = decoded_dataset['train'][12]['text']\n",
    "parsed_article = parse_wiki_article(text)\n",
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4a28a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac91230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_wiki40b(tokenizer):\n",
    "    dataset = load_dataset(\"wiki40b\", \"he\")\n",
    "    decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})\n",
    "\n",
    "    def transform_entry(entry):\n",
    "        # Process the 'text' using parse_wiki_article\n",
    "        article = parse_wiki_article(entry['text'])\n",
    "\n",
    "        # Extract anchor_text and positive_text based on the parsed output\n",
    "        anchor_text = article['title']\n",
    "        if 'sections' in article and len(article['sections']) > 0:\n",
    "            anchor_text += \" \" + article['sections'][0]['section']\n",
    "            positive_text = article['sections'][0]['paragraphs'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "        else:\n",
    "            positive_text = article['abstract'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "\n",
    "        # Return the transformed data\n",
    "        return {\n",
    "            'anchor_text': anchor_text,\n",
    "            'positive_text': positive_text\n",
    "        }\n",
    "\n",
    "    # Apply the transformation to the train, validation, and test subsets\n",
    "    transformed_dataset = {}\n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        # Transform each subset of the dataset using map (this processes each 'text' entry)\n",
    "        transformed_subset = decoded_dataset[subset].map(transform_entry)\n",
    "        transformed_dataset[subset] = transformed_subset\n",
    "\n",
    "    # Return the transformed dataset as a DatasetDict\n",
    "    return DatasetDict(transformed_dataset)\n",
    "\n",
    "\n",
    "def transform_dataset(dataset_name, **kwargs):\n",
    "    if dataset_name == 'wiki40b':\n",
    "        return transform_dataset_wiki40b(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253b117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the logger\n",
    "def setup_logger(log_file):\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create handlers for both console and file output\n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # Set up the format for logging\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b041f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - temperature: Scaling factor applied to the logits before applying the softmax function.\n",
    "        \"\"\"\n",
    "        super(InfoNCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negatives):\n",
    "        \"\"\"\n",
    "        Compute the InfoNCE loss.\n",
    "\n",
    "        Parameters:\n",
    "        - anchor: Tensor of shape (batch_size, embedding_dim) - anchor samples\n",
    "        - positive: Tensor of shape (batch_size, embedding_dim) - positive samples corresponding to each anchor\n",
    "        - negatives: Tensor of shape (batch_size, num_negatives, embedding_dim) - negative samples\n",
    "\n",
    "        Returns:\n",
    "        - loss: Computed InfoNCE loss\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        num_negatives = negatives.size(1)\n",
    "\n",
    "        # Normalize embeddings to unit vectors\n",
    "        anchor = F.normalize(anchor, dim=-1)\n",
    "        positive = F.normalize(positive, dim=-1)\n",
    "        negatives = F.normalize(negatives, dim=-1)\n",
    "\n",
    "        # Calculate the positive logits (similarity between anchor and positive)\n",
    "        positive_logits = torch.sum(anchor * positive, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Calculate the negative logits (similarity between anchor and negatives)\n",
    "        negative_logits = torch.bmm(negatives, anchor.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, num_negatives)\n",
    "\n",
    "        # Concatenate positive and negative logits\n",
    "        logits = torch.cat([positive_logits, negative_logits], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        # Create labels - 0 for the positive samples, as it is the first in the concatenated logits\n",
    "        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)\n",
    "\n",
    "        # Compute the InfoNCE loss using cross-entropy\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bad7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, checkpoint_dir):\n",
    "    latest_checkpoint = None\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = sorted(checkpoint_files)[-1]  # Get the latest checkpoint\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        logger.info(f\"Loading checkpoint {latest_checkpoint}\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch']  # return the epoch to resume from\n",
    "\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
    "    return 0  # Start from the first epoch if no checkpoint found\n",
    "\n",
    "\n",
    "# Save model and optimizer state\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
    "    create_directory(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            anchor_ids, positive_ids, *negative_ids = batch\n",
    "            \n",
    "            # Forward pass to get embeddings for validation\n",
    "            anchor_embeds = model(anchor_ids).last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "            positive_embeds = model(positive_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Process multiple negatives\n",
    "            negatives_embeds = torch.stack([\n",
    "                model(negative_id_batch).last_hidden_state[:, 0, :] for negative_id_batch in negative_ids\n",
    "            ], dim=1)\n",
    "\n",
    "            # Compute the validation loss\n",
    "            val_loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    criterion, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    epochs, \n",
    "    start_epoch=0, \n",
    "    checkpoint_dir='checkpoints', \n",
    "):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            anchor_ids, anchor_mask, positive_ids, positive_mask = batch\n",
    "\n",
    "            # Forward pass to get the embeddings\n",
    "            anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "            anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n",
    "            positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            # Set negatives as the other positives in the batch\n",
    "            # Create a matrix where the negatives are shifted versions of positives\n",
    "            batch_size = positive_embeds.size(0)\n",
    "#             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n",
    "            # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n",
    "            negatives_embeds_list = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Exclude the current index `i` using slicing\n",
    "                negatives_embeds = torch.cat([positive_embeds[:i], positive_embeds[i+1:]], dim=0)\n",
    "\n",
    "                # Append the result to the list\n",
    "                negatives_embeds_list.append(negatives_embeds)\n",
    "\n",
    "            # Stack the negatives for each sample in the batch\n",
    "            # Each entry in the batch now has (batch_size - 1) negative embeddings\n",
    "            negatives_embeds = torch.stack(negatives_embeds_list)\n",
    "\n",
    "            # Compute the InfoNCE loss\n",
    "            loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Compute validation loss after each epoch\n",
    "        avg_val_loss = validate(model, val_dataloader, criterion)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch, \"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90891a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "183ab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720181eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'intfloat/multilingual-e5-base'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "INFONCE_TEMPERATURE = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e252173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "logger = setup_logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:13:39,350 - INFO - Start train base model: intfloat/multilingual-e5-base\n",
      "2024-09-05 16:13:40,397 - INFO - Switching to new dataset: wiki40b\n",
      "2024-09-05 16:14:50,239 - INFO - No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Log file setup\n",
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "\n",
    "\n",
    "# Define model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "logger.info(f\"Start train base model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize the InfoNCE loss and the optimizer\n",
    "criterion = InfoNCELoss(temperature=INFONCE_TEMPERATURE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Datasets to train on\n",
    "dataset_names = ['wiki40b']\n",
    "\n",
    "# Iterate over datasets and train\n",
    "for dataset_name in dataset_names:\n",
    "    start_datetime = datetime.now()\n",
    "    \n",
    "    logger.info(f\"Switching to new dataset: {dataset_name}\")\n",
    "    dataset = transform_dataset(dataset_name, tokenizer=tokenizer)\n",
    "    \n",
    "    # Tokenize the train dataset\n",
    "    anchor_inputs_train = tokenizer(dataset['train']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_train = tokenizer(dataset['train']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    train_dataset = TensorDataset(anchor_inputs_train['input_ids'], anchor_inputs_train['attention_mask'], \n",
    "                                  positive_inputs_train['input_ids'], positive_inputs_train['attention_mask'])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Tokenize the validation dataset\n",
    "    anchor_inputs_val = tokenizer(dataset['validation']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_val = tokenizer(dataset['validation']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Create DataLoader for validation\n",
    "    val_dataset = TensorDataset(anchor_inputs_val['input_ids'], anchor_inputs_val['attention_mask'], \n",
    "                                positive_inputs_val['input_ids'], positive_inputs_val['attention_mask'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load the latest checkpoint if available and resume training\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_dir)\n",
    "\n",
    "    # Train the model for this dataset\n",
    "    train(\n",
    "        model=model, \n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_dataloader=train_dataloader, \n",
    "        val_dataloader=val_dataloader, \n",
    "        epochs=3, \n",
    "        start_epoch=start_epoch,\n",
    "    )\n",
    "    \n",
    "    end_datetime = datetime.now()\n",
    "    logger.info(f\"Total training on {dataset_name} elapsed time is {(end_datetime - start_datetime).total_seconds()} seconds\")\n",
    "    \n",
    "logger.info(f\"End train base model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f18ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03137d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../data/synthetic_data_20240906_0018.pkl ./data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449c6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ed50ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ce3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'דוחות בדיקה ספרותיים לתקופה הנצרות',\n",
       " 'positive_document': \"הנצרות המוקדמת התפתחה בתוך הקהילה היהודית המתרחבת של אותה תקופה. בעוד שהיהדות המקורית התבססה על כתבי הקודש העבריים, הנצרות אימצה גם את הברית החדשה, הכוללת את הבשורות על חייו של ישו והאיגרות של שליחיו. עם זאת, דוחות מפורטים על חייו של ישו ועל התפתחות הנצרות המוקדמת בתקופה זו הם מועטים ביותר. מרבית הידע שלנו על התקופה נשען על האמונות והמסורות שהתפתחו מאוחר יותר בכנסייה הנוצרית. דוחות ספרותיים מאותה תקופה מתארים במקרים רבים את היחסים המורכבים בין הנצרים לבין הקהילות היהודיות והרומיות ששלטו בארץ ישראל. כמה דוחות משקפים גם את המאבקים הפנימיים בין קבוצות שונות של נוצרים על פרשנות התנ'ך החדש והאמונות האמיתיות של ישו.\",\n",
       " 'hard_negative_document': \"פילוסופיה יהודית היא תחום עיון עתיק יומין, הנוגע בהיבטים המטאפיזיים והאתיים של המחשבה היהודית. תורת הקבלה, למשל, חוקרת את הטבע האלוהי ואת הקשרים המיסטיים בין האל לבריאה. בנוסף, חכמת המוסר העברית דנה ברעיונות מוסריים כמו חמלה, צדקה וחסד. עם זאת, בניגוד לפילוסופיה המערבית, המחשבה היהודית מסורתית אינה מחולקת לתחומים מובחנים כמו מטאפיזיקה, אפיסטמולוגיה ואתיקה. במקום זאת, היא נטועה בתוך עולם התורה והמצוות, כשהיא מחפשת תובנות בתוך המקורות הקדושים של העם היהודי. מקורות אלה כוללים את התנ'ך, התלמוד, כתבי קבלה וספרי מוסר יהודיים מזמנים שונים. לכן, הפילוסופיה היהודית אינה תחום עצמאי, אלא חלק בלתי נפרד מהמסורת הרוחנית והתרבותית של היהדות.\",\n",
       " 'timestamp': datetime.datetime(2024, 9, 6, 5, 10, 58, 915768),\n",
       " 'random_seed': 1042,\n",
       " 'task': 'Find relevant news articles discussing a current event or trending topic (e.g., \"climate change impact on coastal cities\").',\n",
       " 'query_type': 'extremely long-tail',\n",
       " 'query_length': 'less than 5 words',\n",
       " 'difficulty': 'college',\n",
       " 'clarity': 'clear',\n",
       " 'num_words': '200',\n",
       " 'language': 'Hebrew',\n",
       " 'input_tokens': 348,\n",
       " 'output_tokens': 887,\n",
       " 'success': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e432e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cd492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32af3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial allocated memory: 1.953125 MB\n",
      "Initial reserved memory: 20.0 MB\n",
      "Final allocated memory: 1956.8583984375 MB\n",
      "Final reserved memory: 1974.0 MB\n",
      "Memory increase from initial to final (allocated): 1954.9052734375 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the batch size and embedding dimension\n",
    "batch_size = 1000\n",
    "embed_dim = 512\n",
    "\n",
    "# Create dummy positive embeddings (batch_size, embed_dim)\n",
    "positive_embeds = torch.randn(batch_size, embed_dim, device=device)\n",
    "\n",
    "# Measure initial memory usage\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "initial_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Initial allocated memory: {initial_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Initial reserved memory: {initial_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Pre-allocate a tensor for negatives (shape: batch_size, batch_size - 1, embed_dim)\n",
    "negatives_embeds = torch.zeros(batch_size, batch_size - 1, embed_dim, device=device)\n",
    "\n",
    "# Create an identity mask to exclude diagonal elements (positives)\n",
    "negatives_mask = torch.eye(batch_size, dtype=torch.bool).to(device)\n",
    "\n",
    "# Fill the negatives_embeds in place, row by row\n",
    "for i in range(batch_size):\n",
    "    # Select all positive embeddings except the current index `i`\n",
    "    negatives_i = positive_embeds[~negatives_mask[i]]  # Exclude diagonal\n",
    "    negatives_embeds[i] = negatives_i  # In-place assignment\n",
    "\n",
    "# Measure memory usage after in-place assignments\n",
    "final_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "final_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Final allocated memory: {final_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Final reserved memory: {final_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Compare memory usage before and after assignments\n",
    "print(f\"Memory increase from initial to final (allocated): {(final_allocated_memory - initial_allocated_memory) / (1024 ** 2)} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da094db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial allocated memory: 1.953125 MB\n",
      "Initial reserved memory: 20.0 MB\n",
      "Final allocated memory: 1954.9072265625 MB\n",
      "Final reserved memory: 1974.0 MB\n",
      "Memory increase from initial to final (allocated): 1952.9541015625 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the batch size and embedding dimension\n",
    "batch_size = 1000\n",
    "embed_dim = 512\n",
    "\n",
    "# Create dummy positive embeddings (batch_size, embed_dim)\n",
    "positive_embeds = torch.randn(batch_size, embed_dim, device=device)\n",
    "\n",
    "# Measure initial memory usage\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "initial_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Initial allocated memory: {initial_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Initial reserved memory: {initial_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "batch_size = positive_embeds.size(0)\n",
    "negatives_mask = torch.eye(batch_size, dtype=torch.bool).to(device)  # Identity matrix to mask out positives\n",
    "positive_embeds_reshaped = positive_embeds.unsqueeze(0)  # Shape: (1, batch_size, embed_dim)\n",
    "\n",
    "# Use the mask to select negatives (all non-diagonal elements are negatives)\n",
    "# negatives_embeds = positive_embeds_reshaped.masked_select(~negatives_mask.unsqueeze(-1))#.view(batch_size, batch_size - 1, -1)\n",
    "# Pre-allocate a tensor for negatives (shape: batch_size, batch_size - 1, embed_dim)\n",
    "negatives_embeds = torch.zeros(batch_size, batch_size - 1, embed_dim, device=device)\n",
    "\n",
    "# Measure memory usage after in-place assignments\n",
    "final_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "final_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Final allocated memory: {final_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Final reserved memory: {final_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Compare memory usage before and after assignments\n",
    "print(f\"Memory increase from initial to final (allocated): {(final_allocated_memory - initial_allocated_memory) / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f001ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /home/nlp/achimoa/projects/hebrew_text_encoder\n",
      "PYTHONPATH updated with: /home/nlp/achimoa/projects/hebrew_text_encoder/src\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_dir = os.getcwd() if not os.getcwd().split(\"/\")[-1] == 'notebooks' else '/'.join(os.getcwd().split(\"/\")[0:-1])\n",
    "src_dir = os.path.join(project_dir, 'src')\n",
    "\n",
    "os.chdir(project_dir)\n",
    "print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.insert(0, src_dir)  # Add it to the front of PYTHONPATH\n",
    "    print(f\"PYTHONPATH updated with: {src_dir}\")\n",
    "else:\n",
    "    print(f\"PYTHONPATH already contains: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47351954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 01:27:13,063 - default - INFO - Arguments:\n",
      "2024-10-11 01:27:13,064 - default - INFO - Dataset: heq\n",
      "2024-10-11 01:27:13,065 - default - INFO - Model: onlplab/alephbert-base\n",
      "2024-10-11 01:27:13,065 - default - INFO - Target checkpoint path: checkpoints/temp\n",
      "2024-10-11 01:27:13,066 - default - INFO - Source checkpoint path: checkpoints/temp\n",
      "2024-10-11 01:27:13,066 - default - INFO - Source checkpoint epoch: None\n",
      "2024-10-11 01:27:13,067 - default - INFO - Learning rate: 5e-05\n",
      "2024-10-11 01:27:13,067 - default - INFO - Weight decay: 0.0001\n",
      "2024-10-11 01:27:13,068 - default - INFO - Clip value: 1.0\n",
      "2024-10-11 01:27:13,069 - default - INFO - InfoNCE temperature: 0.07\n",
      "2024-10-11 01:27:13,069 - default - INFO - Epochs: 10\n",
      "2024-10-11 01:27:13,069 - default - INFO - CUDA_VISIBLE_DEVICES: 0\n",
      "2024-10-11 01:27:13,463 - default - INFO - Using device: cuda\n",
      "Some weights of BertModel were not initialized from the model checkpoint at onlplab/alephbert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-10-11 01:27:16,632 - default - DEBUG - Adding special tokens: ['[QUERY]', '[DOCUMENT]', '[TASK_QUERY_DOC]', '[TASK_TITLE_DOC]', '[TASK_QUESTION_DOC]']\n",
      "2024-10-11 01:27:16,640 - default - INFO - Initialize the InfoNCE loss and the optimizer\n",
      "2024-10-11 01:27:17,602 - default - INFO - Loading checkpoint\n",
      "2024-10-11 01:27:17,605 - default - INFO - No checkpoint found. Starting from scratch.\n",
      "2024-10-11 01:27:17,606 - default - INFO - Load dataset: heq\n",
      "2024-10-11 01:27:17,610 - default - INFO - Building HeQ dataset\n",
      "2024-10-11 01:27:17,611 - default - INFO - Loading json file from https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/train.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 01:27:18,484 - default - INFO - Loading json file from https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/val.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/val.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 01:27:19,074 - default - INFO - Tokenizing train dataset\n",
      "2024-10-11 01:27:20,425 - default - INFO - Creating train dataloader\n",
      "2024-10-11 01:27:20,427 - default - INFO - Tokenizing validation dataset\n",
      "2024-10-11 01:27:20,831 - default - INFO - Creating validation dataloader\n",
      "2024-10-11 01:27:20,832 - default - INFO - Start training...\n",
      "Epoch 1/10 [Train]:   0%|          | 0/140 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1: Start of Batch - Allocated memory: 0.47 GB\n",
      "Epoch 1, Batch 1: Start of Batch - Reserved memory: 0.67 GB\n",
      "Epoch 1, Batch 1: Start of Batch - Max allocated memory: 0.62 GB\n",
      "\n",
      "Epoch 1, Batch 1: After Query Forward Pass - Allocated memory: 1.15 GB\n",
      "Epoch 1, Batch 1: After Query Forward Pass - Reserved memory: 1.26 GB\n",
      "Epoch 1, Batch 1: After Query Forward Pass - Max allocated memory: 1.15 GB\n",
      "\n",
      "Epoch 1, Batch 1: After Positive Forward Pass - Allocated memory: 9.34 GB\n",
      "Epoch 1, Batch 1: After Positive Forward Pass - Reserved memory: 9.72 GB\n",
      "Epoch 1, Batch 1: After Positive Forward Pass - Max allocated memory: 9.40 GB\n",
      "\n",
      "Epoch 1, Batch 1: After Loss Calculation - Allocated memory: 9.34 GB\n",
      "Epoch 1, Batch 1: After Loss Calculation - Reserved memory: 9.72 GB\n",
      "Epoch 1, Batch 1: After Loss Calculation - Max allocated memory: 9.40 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/140 [00:00<?, ?it/s, Batch=1, Train Loss=2.79]2024-10-11 01:27:21,642 - default - INFO - Epoch 1 / 10, Batch 1 / 140, Train Loss: 2.7930428981781006\n",
      "Epoch 1/10 [Train]:   1%|          | 1/140 [00:00<01:51,  1.25it/s, Batch=1, Train Loss=2.79]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1: After Backward Pass - Allocated memory: 1.01 GB\n",
      "Epoch 1, Batch 1: After Backward Pass - Reserved memory: 10.00 GB\n",
      "Epoch 1, Batch 1: After Backward Pass - Max allocated memory: 9.62 GB\n",
      "\n",
      "Epoch 1, Batch 1: After Optimizer Step - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 1: After Optimizer Step - Reserved memory: 2.23 GB\n",
      "Epoch 1, Batch 1: After Optimizer Step - Max allocated memory: 9.62 GB\n",
      "\n",
      "Epoch 1, Batch 2: Start of Batch - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 2: Start of Batch - Reserved memory: 2.23 GB\n",
      "Epoch 1, Batch 2: Start of Batch - Max allocated memory: 9.62 GB\n",
      "\n",
      "Epoch 1, Batch 2: After Query Forward Pass - Allocated memory: 2.63 GB\n",
      "Epoch 1, Batch 2: After Query Forward Pass - Reserved memory: 2.79 GB\n",
      "Epoch 1, Batch 2: After Query Forward Pass - Max allocated memory: 9.62 GB\n",
      "\n",
      "Epoch 1, Batch 2: After Positive Forward Pass - Allocated memory: 10.77 GB\n",
      "Epoch 1, Batch 2: After Positive Forward Pass - Reserved memory: 11.25 GB\n",
      "Epoch 1, Batch 2: After Positive Forward Pass - Max allocated memory: 10.87 GB\n",
      "\n",
      "Epoch 1, Batch 2: After Loss Calculation - Allocated memory: 10.77 GB\n",
      "Epoch 1, Batch 2: After Loss Calculation - Reserved memory: 11.25 GB\n",
      "Epoch 1, Batch 2: After Loss Calculation - Max allocated memory: 10.87 GB\n",
      "\n",
      "Epoch 1, Batch 2: After Backward Pass - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 2: After Backward Pass - Reserved memory: 11.33 GB\n",
      "Epoch 1, Batch 2: After Backward Pass - Max allocated memory: 10.87 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   1%|▏         | 2/140 [00:01<01:34,  1.46it/s, Batch=2, Train Loss=2.45]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 2: After Optimizer Step - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 2: After Optimizer Step - Reserved memory: 2.37 GB\n",
      "Epoch 1, Batch 2: After Optimizer Step - Max allocated memory: 10.87 GB\n",
      "\n",
      "Epoch 1, Batch 3: Start of Batch - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 3: Start of Batch - Reserved memory: 2.37 GB\n",
      "Epoch 1, Batch 3: Start of Batch - Max allocated memory: 10.87 GB\n",
      "\n",
      "Epoch 1, Batch 3: After Query Forward Pass - Allocated memory: 2.63 GB\n",
      "Epoch 1, Batch 3: After Query Forward Pass - Reserved memory: 2.78 GB\n",
      "Epoch 1, Batch 3: After Query Forward Pass - Max allocated memory: 10.87 GB\n",
      "\n",
      "Epoch 1, Batch 3: After Positive Forward Pass - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 3: After Positive Forward Pass - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 3: After Positive Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 3: After Loss Calculation - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 3: After Loss Calculation - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 3: After Loss Calculation - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 3: After Backward Pass - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 3: After Backward Pass - Reserved memory: 11.32 GB\n",
      "Epoch 1, Batch 3: After Backward Pass - Max allocated memory: 10.88 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   2%|▏         | 3/140 [00:02<01:28,  1.55it/s, Batch=3, Train Loss=2.23]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 3: After Optimizer Step - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 3: After Optimizer Step - Reserved memory: 2.35 GB\n",
      "Epoch 1, Batch 3: After Optimizer Step - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 4: Start of Batch - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 4: Start of Batch - Reserved memory: 2.35 GB\n",
      "Epoch 1, Batch 4: Start of Batch - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 4: After Query Forward Pass - Allocated memory: 2.63 GB\n",
      "Epoch 1, Batch 4: After Query Forward Pass - Reserved memory: 2.77 GB\n",
      "Epoch 1, Batch 4: After Query Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 4: After Positive Forward Pass - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 4: After Positive Forward Pass - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 4: After Positive Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 4: After Loss Calculation - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 4: After Loss Calculation - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 4: After Loss Calculation - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 4: After Backward Pass - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 4: After Backward Pass - Reserved memory: 11.32 GB\n",
      "Epoch 1, Batch 4: After Backward Pass - Max allocated memory: 10.88 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   3%|▎         | 4/140 [00:02<01:25,  1.59it/s, Batch=4, Train Loss=2.02]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 4: After Optimizer Step - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 4: After Optimizer Step - Reserved memory: 2.39 GB\n",
      "Epoch 1, Batch 4: After Optimizer Step - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 5: Start of Batch - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 5: Start of Batch - Reserved memory: 2.39 GB\n",
      "Epoch 1, Batch 5: Start of Batch - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 5: After Query Forward Pass - Allocated memory: 2.63 GB\n",
      "Epoch 1, Batch 5: After Query Forward Pass - Reserved memory: 2.78 GB\n",
      "Epoch 1, Batch 5: After Query Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 5: After Positive Forward Pass - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 5: After Positive Forward Pass - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 5: After Positive Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 5: After Loss Calculation - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 5: After Loss Calculation - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 5: After Loss Calculation - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 5: After Backward Pass - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 5: After Backward Pass - Reserved memory: 11.32 GB\n",
      "Epoch 1, Batch 5: After Backward Pass - Max allocated memory: 10.88 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   4%|▎         | 5/140 [00:03<01:23,  1.62it/s, Batch=5, Train Loss=2.03]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 5: After Optimizer Step - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 5: After Optimizer Step - Reserved memory: 2.39 GB\n",
      "Epoch 1, Batch 5: After Optimizer Step - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 6: Start of Batch - Allocated memory: 1.96 GB\n",
      "Epoch 1, Batch 6: Start of Batch - Reserved memory: 2.39 GB\n",
      "Epoch 1, Batch 6: Start of Batch - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 6: After Query Forward Pass - Allocated memory: 2.63 GB\n",
      "Epoch 1, Batch 6: After Query Forward Pass - Reserved memory: 2.77 GB\n",
      "Epoch 1, Batch 6: After Query Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 6: After Positive Forward Pass - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 6: After Positive Forward Pass - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 6: After Positive Forward Pass - Max allocated memory: 10.88 GB\n",
      "\n",
      "Epoch 1, Batch 6: After Loss Calculation - Allocated memory: 10.78 GB\n",
      "Epoch 1, Batch 6: After Loss Calculation - Reserved memory: 11.24 GB\n",
      "Epoch 1, Batch 6: After Loss Calculation - Max allocated memory: 10.88 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrain_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43monlplab/alephbert-base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcuda_visible_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_checkpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/temp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoints/temp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/hebrew_text_encoder/src/train_model.py:109\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(model_name, dataset_name, batch_size, learning_rate, weight_decay, clip_value, infonce_temperature, epochs, checkpoint_dir, source_checkpoint_dir, source_checkpoint_epoch, cuda_visible_devices)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dataloaders[split] \u001b[38;5;241m=\u001b[39m DataLoader(tensor_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m(split \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Train the model for this dataset\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_value\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m end_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m    123\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal training of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elapsed time is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end_datetime\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_datetime)\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/projects/hebrew_text_encoder/src/trainings.py:71\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, train_dataloader, val_dataloader, device, epochs, start_epoch, checkpoint_dir, clip_value, always_save_checkpoint)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     70\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 71\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m print_memory_usage(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: After Backward Pass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/biu/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biu/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biu/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train_model import main\n",
    "\n",
    "main(\n",
    "    model_name='onlplab/alephbert-base',\n",
    "    dataset_name='heq',\n",
    "    batch_size=32,\n",
    "    cuda_visible_devices=\"0\",\n",
    "    source_checkpoint_dir='checkpoints/temp',\n",
    "    checkpoint_dir='checkpoints/temp',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f85120e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 00:11:56,620 - default - INFO - Building HeQ dataset\n",
      "2024-10-11 00:11:56,622 - default - INFO - Loading json file from https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/train.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/train.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-11 00:11:58,115 - default - INFO - Loading json file from https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/val.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/val.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor_text', 'positive_text'],\n",
       "        num_rows: 4462\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['anchor_text', 'positive_text'],\n",
       "        num_rows: 239\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import build_dataset\n",
    "\n",
    "dataset = build_dataset(dataset_name='heq')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291eeb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data files: 100%|██████████| 13/13 [00:03<00:00,  3.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'negative_text'],\n",
       "        num_rows: 73595\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'negative_text'],\n",
       "        num_rows: 9199\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import build_dataset\n",
    "\n",
    "dataset = build_dataset(dataset_name='synthesized_query_document')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db65e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322.50s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "ls: /Users/asafam/Workspace/biu/hebrew_text_encoder/data/synthetic_data_202409: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls /Users/asafam/Workspace/biu/hebrew_text_encoder/data/synthetic_data_202409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a91dd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreload_ext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoreload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_dataset\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m build_dataset(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwiki40b\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m dataset\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data'"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from data import build_dataset\n",
    "\n",
    "dataset = build_dataset(dataset_name='wiki40b')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b303523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wikidata_id': \"b'Q2336243'\",\n",
       " 'text': 'b\\'\\n_START_ARTICLE_\\nהקרב על גבעת התחמושת\\n_START_SECTION_\\nתחילת המלחמה בירושלים\\n_START_PARAGRAPH_\\nבבוקר יום 5 ביוני 1967 פתחו כוחות הלגיון הירדני בהרעשה ארטילרית על ירושלים המערבית, וחיילי הלגיון הירדני השתלטו על מטה משקיפי האו\"ם ששכן בארמון הנציב. מלך ירדן, חוסיין, שעד אותו בוקר היסס בשאלה האם להצטרף למלחמה נגד ישראל, הכריע בעד שיתוף הפעולה עם מצרים וסוריה, והחליט להעביר את צבאו לפיקוד מצרי. המלך הסתמך על ידיעות מצריות מוטעות, והשתכנע כי הקרב בסיני הוכרע לטובת הכוחות המצריים, ומכאן שיהיה זה לטובת האינטרסים הירדניים להיכנס למלחמה. יחד עם זאת, הוצבו לצבא הירדני מטרות מוגבלות בלבד, מתוך כוונה לבצע מחטפים מוצלחים בנקודות בעלות חשיבות אסטרטגית לאורך הקו העירוני._NEWLINE_הצד הישראלי הופתע מכניסתה של ירדן למלחמה. ישראל העבירה מסרים לירדן לפני המלחמה, בתקווה לשכנעה שלא להתערב בעימות, ולפי הערכת מטכ\"ל צה\"ל, פיקוד המרכז לא היה אמור כלל להשתתף בלחימה. עם תחילת הקרבות הסתבר שלפיקוד לא היו כוחות לביצוע מהלך התקפי. לרשות הפיקוד עמדו במרחב ירושלים שתי חטיבות מילואים לוחמות בלבד, מעבר לכוחות מדרג שני ושלישי: חטיבת ירושלים (חטיבה 16), בפיקוד אליעזר אמיתי, וחטיבת השריון הראל (חטיבה 10), בפיקוד אורי בן ארי. כשהסתבר לפיקוד צה\"ל שגם בגזרה זו מתפתחת לחימה, סופחה לסדר הכוחות של הפיקוד גם חטיבה 55, חטיבת צנחנים בפיקוד מרדכי גור.\\n_START_SECTION_\\nחטיבת הצנחנים\\n_START_PARAGRAPH_\\nבתקופת ההמתנה, לפני פרוץ מלחמת ששת הימים, תוכנן שחטיבת הצנחנים 55 תוצנח בעורף הצבא המצרי, באזור אל-עריש. אולם התקדמותם המהירה של כוחות צה\"ל בפתחת רפיח והצלחת המהלומה האווירית שהונחתה על חיל האוויר המצרי הפכו את משימת החטיבה למיותרת. לאור המצב, הוחלט על סיפוח החטיבה לפיקוד מרכז. אלוף פיקוד המרכז, עוזי נרקיס הציג בפני מפקד החטיבה וקציניה את משימתם החדשה: פריצת הקו העירוני ויצירת מעבר קרקעי מאובטח בין העיר העברית למובלעת הר הצופים. מובלעת זו הייתה אחת מנקודות התורפה המרכזיות של הגזרה, והפיקוד חשש מאד מכיבושה של המובלעת המנותקת לפני שיוכלו כוחות צה\"ל לחבור אליה._NEWLINE_חטיבת הצנחנים מנתה שלושה גדודי צנחנים, פלוגת סיור, פלוגת הנדסה, ויחידת סיוע חטיבתית, שבה 4 סוללות תותחים ומרגמות. הפיקוד סיפח לחטיבה עשרה טנקי סופר שרמן מפלוגת הטנקים הירושלמית.\\n_START_SECTION_\\nהכוח הירדני והמוצב\\n_START_PARAGRAPH_\\nמתחם גבעת התחמושת כלל את בית הספר לשוטרים ואת המוצב הגדול בגבעת התחמושת. במתחם החזיקה פלוגת חי\"ר מס\\' 2 מגדוד 2 של הלגיון הירדני, בפיקוד ראיד (רב-סרן) סלימאן סאלם אל-סלאיטה. שתי מחלקות היו בגבעת התחמושת, מחלקה אחת בבית הספר לשוטרים ומחלקה רביעית במוצב סמוך שנקרא מוצב המטלית הצהובה, מדרום לבית הספר לשוטרים. בסך הכל היו במתחם בעת פרוץ המלחמה 120 לוחמים ירדניים._NEWLINE_מוצב גבעת התחמושת הוקם על ידי הירדנים כדי לשלוט על הכביש להר הצופים. מצפון-מזרח למוצב גבעת התחמושת הוקמה עמדה שלטת על גבעת המבתר. כלומר, מעמדת גבעת המבתר יכלו צלפים ירדנים לירות בכוחות ישראלים שהחלו לחדור אל מוצב גבעת התחמושת. שמה של גבעת המבתר בפי הירדנים היה תל אל מודורה, כלומר הגבעה העגולה._NEWLINE_מוצב גבעת התחמושת היה מוקף גדרות תיל ושדות מוקשים. מעבר לאלה, הייתה גבעת התחמושת מוקפת משלושה צדדים בתעלות קשר עמוקות, מבוטנות ומדופנות באבן ירושלמית, שלאורכן בונקרים רבים ועמדות מוגנות היטב מבטון. התעלה המערבית, שפנתה לעבר ירושלים היהודית הייתה מבוצרת במיוחד. בקצה הצפוני של התעלה המערבית התעקלה זו מזרחה ודרומה, והמשיכה לתעלה היקפית מזרחית. שתי התעלות יצרו מעין צורת פעמון שהקיף את המוצב מדרום מערב לדרום מזרח. באמצע המוצב הייתה תעלה מרכזית, שהובילה אל מבנים שונים שהיו במרכז הגבעה - שני מבני מגורים בעלי גג מעוגל מפח, מגדל מים, פילבוקס, ועוד. בסמוך לקצה הצפוני של התעלה המרכזית נמצא בונקר הפיקוד התת-קרקעי של המוצב. בסמוך לבונקר הפיקוד, ממזרח לגבעה ומחוץ לגדרות שסביבה, עמד מבנה נוסף (כנראה בית מגורים) שכונה על ידי הצנחנים \"בית הטלוויזיה\"._NEWLINE_בצידו הדרומי של המוצב הייתה תעלה רדודה, שחיברה בין התעלה המרכזית לתעלה המערבית, וכן הייתה לה שלוחה לכיוון בית הספר לשוטרים. תעלה זו הייתה מוגנת פחות משאר התעלות. על צד זה חלשו כמה בונקרים, שיצרו \"שטח הריגה\" לכוחות שינסו להסתער על המוצב מכיוון זה. בצד זה היו שלושה בתים, שהיו חלק ממחסני אונר\"א, ולאורכם, מדרום להם, עברה גדר אבן. במקביל לגדר היה בית אבן מוארך.\\n_START_SECTION_\\nתכנון\\n_START_PARAGRAPH_\\nפיקוד המרכז הועיד את חטיבה 55 למטרת יצירת הקשר עם הר הצופים, דרך שכונת שייח\\' ג\\'ראח, וכיבוש השטח מדרום לשכונה, עד מוזיאון רוקפלר. מח\"ט 55, אלוף-משנה מרדכי גור, חילק את המשימה בין גדודי החטיבה. על גדוד 66, בפיקודו של יוסי יפה, הוטל לפרוץ את הקו העירוני באזור שכונת סנהדריה, לכבוש את בית-הספר לשוטרים וגבעת התחמושת, ולהמשיך מהם לשכונת שייח\\' ג\\'ראח, שם יועד להתמקם במלון אמבסדור שמרכזה._NEWLINE_מפקד הגדוד החליט לפרוץ פרצה אחת בגדרות בית-הספר לשוטרים, שדרכה תוכנן לעבור הגדוד כולו. על פי התוכנית הייתה אמורה פלוגה ד\\' של הגדוד, בפיקוד גיורא אשכנזי, לפרוץ את הגדרות מול בית-הספר לשוטרים. פלוגה ב\\', בפיקוד דוד (דודיק) רוטנברג הייתה אמורה לחדור ראשונה דרך הפרצה, לכבוש את תעלת הקשר לאורך בית-הספר לשוטרים בואכה גבעת התחמושת, ולאחר מכן להמתין בפאתי הגבעה כעתודה, בכוננות לסיוע בכיבוש הגבעה. פלוגה ג\\', בפיקוד עובד (דדי) יעקובי הייתה אמורה לנוע בעקבות פלוגה ב\\' דרך השטח הכבוש, לתקוף את גבעת התחמושת מעורפה - אגפה הדרום מזרחי - ולכבוש אותה. פלוגה א\\', בפיקוד גבי מגל, תוכננה לנוע דרך השטח שנכבש על ידי הפלוגות שקדמו לה, ולנוע אל תוך שכונת שייח\\' ג\\'ראח. כוח נוסף תוכנן לכבוש את מוצב המטלית הצהובה, מדרום. ששת הטנקים מגדוד 182 שצורפו לכוח נועדו לשמש לסיוע ועתודה.\\n_START_SECTION_\\nתחילת הקרב וכיבוש בית הספר לשוטרים\\n_START_PARAGRAPH_\\nאור ליום שלישי, 6 ביוני 1967, בשעה שתיים ועשרים אחר חצות, החל ריכוך ארטילרי של מטרות ההתקפה. יחידות הסיוע תקפו באש מרגמות ותותחים את מתחם גבעת התחמושת. תוך כדי חילופי האש הארטילרית בין שני הצדדים החלו הצנחנים להבקיע את המערך הירדני בקו העירוני תוך שהם נתקלים באש המגינים ובגדרות התיל. בסופו של דבר נפרצו ארבע הגדרות, והצנחנים החלו להיכנס לתעלת הקשר המובילה לבית הספר לשוטרים. כבר בשלב זה התברר כי תעלות הקשר צרות מאד: לא רק שלא היה בהן מקום לשני לוחמים זה בצד זה, עובדה שהקשתה מאד על החייל המוביל להתחלף לאחר שאזלה תחמושתו, אלא שגם לוחם בודד עם חגור מלא ציוד ותחמושת התקשה להתקדם._NEWLINE_הצנחנים הצליחו לחדור לתוך מבנה בית הספר לשוטרים ללא קשיים מיוחדים, מאחר שחיילי הלגיון הערבי נטשו את המבנה מיד עם תחילת ההרעשה ונסוגו אל תוך מוצב גבעת התחמושת. מוצב המטלית הצהובה נכבש אף הוא ללא התנגדות מיד לאחר מכן, כשכל החיילים בו נסוגים אף הם לתוך הגבעה. פלוגה ב\\' המשיכה בכיבוש התעלות המובילות למוצב גבעת התחמושת, תוך השתלטות על הבונקרים שבדרכה. הפלוגה מילאה את משימתה, והמ\"פ הודיע למג\"ד שהוא שולט על הכניסה המערבית למוצב._NEWLINE_כוח הטנקים חדר אל חצר בית הספר לשוטרים, משם פנו שני טנקים דרומה, לעבר דרך שכם, בעוד ששאר הטנקים עוכבו על מנת לשמש חיפוי לכוח הפורץ לגבעה לכיוון מוצב גבעת המבתר.\\n_START_SECTION_\\nהלחימה על גבעת התחמושת: השלב הראשון\\n_START_PARAGRAPH_\\nמפקד פלוגה ג\\', דדי, חילק את חייליו לשלושה כוחות. כוח אחד, בפיקוד המ\"מ דני יצחקי אמור היה להיכנס לתעלה המזרחית. כוח שני, בפיקוד המ\"מ יורם אלישיב אמור היה להיכנס אל התעלה המערבית ולכבוש אותה. כוח שלישי, בפיקוד דדי עצמו והמ\"מ מילר, יועד להיכנס אל התעלה המרכזית. אולם בשל המהומה הגדולה וההפגזה הארטילרית שנחתה סמוך מאד לצנחנים השתבשו העניינים: המחלקה בפיקודו של יורם טעתה ונכנסה אל התעלה המרכזית, שהוליכה אל מרכז הגבעה; דדי, יחד עם מחלקתו של מילר טעו ונכנסו לתעלה המזרחית; ואילו דני המשיך בעקבותיו של יורם אל מרכז הגבעה. כך לא נכנס כוח כלשהו לחלק המערבי של הגבעה. חוליה ממחלקתו של יורם, בפיקודו של מאיר מלמודי הסתערה אל תוך שטח ההריגה שכונה \"משולש האש\", וארבעה מאנשיה נהרגו והשאר נפצעו. יורם לא ידע על כך והמשיך להתקדם בתעלה המרכזית, הצליח להגיע אל מרכז הגבעה ולכבוש את המבנים שעמדו במרכזה._NEWLINE_דדי עצמו התקדם יחד עם מחלקה נוספת בתעלה המזרחית. בתחילה הייתה ההתקדמות קלה, והדיווחים מהמחלקה בתעלה המרכזית יצרו אצל מפקד הפלוגה את הרושם המוטעה שההשתלטות על הגבעה תהיה קלה ולא יהיה צורך בסיוע מפלוגה אחרת. תמונת הקרב הזו הייתה מוטעית; דדי לא הבין שטרם התרחשה ההיתקלות עם הכוח העיקרי במוצב, ולפיכך דיווח למג\"ד כי די בכוחו שלו בלבד להכרעה מהירה של מוצב גבעת התחמושת, והמג\"ד הסיק מדיווח זה כי ניתן להמשיך במשימה הבאה - היאחזות במלון אמבסדור. הוא הורה לפלוגות א\\' ו-ד\\' להמשיך לכיוון המלון ואלו אכן כבשו את הצומת האסטרטגי של שייח\\' ג\\'ראח._NEWLINE_ואולם כאשר הגיעו הצנחנים לחלקה הצפוני של הגבעה החלה תמונת הקרב להשתנות. הכוח שלחם בתעלה המרכזית נתקל בהתנגדות קשה מול בית הטלוויזיה, ונחשף לאש הלוחמים הירדנים שבגבעת המבתר ובחלקה הצפוני של הגבעה עצמה. הכוח ספג אבדות רבות, ובשלב מסוים נותר מפקד הפלוגה עם שמונה לוחמים בלבד. הכוח השלישי שנותר, כאמור, בשלושת הבתים הצטרף ללחימה והחל לנוע בתעלה המרכזית, משם עבר לתעלה קצרה שהייתה בין התעלה ההיקפית המזרחית לתעלה המרכזית. כוח זה, בפיקודו של איילון, כבש את בונקר הפיקוד._NEWLINE_מחלקתו של יורם שהגיעה למרכז הגבעה כבשה את מבני המגורים שעמדו שם, והמשיכה צפונה, שם ניהלה קרב עם עמדת תותח ללא רתע (תול\"ר), בו נהרג המ\"מ יורם. מ\"מ נוסף שהיה במקום, צביקה מגן, נטל את הפיקוד והצליח להשמיד את התול\"ר, ומשם להגיע אל הקטע הצפוני של התעלה ההיקפית. כך היו שלוש המחלקות של פלוגה ג\\' כל אחת על ציר נפרד: יחידה אחת ליד בית הטלוויזיה, יחידה שנייה ליד צריפי המגורים ויחידה שלישית בקרב מר לכיבוש תעלות הקשר המרכזיות. דדי הבין שמצב פלוגתו חמור, ושללא תגבורת לא יצליח להשתלט על הגבעה; הוא החליט להזעיק את פלוגה ב\\', בפיקוד דודיק, לעזרה.\\n_START_SECTION_\\nשלב שני: כניסת פלוגה ב\\' לקרב\\n_START_PARAGRAPH_\\nהלחימה בגבעת התחמושת נכנסה לשלב האינטנסיבי ביותר. תגבורת ראשונה שנשלחה לגבעה הייתה מחלקה מפלוגה א\\', שכזכור המשיכה הלאה לכיוון שייח\\' ג\\'ראח. מחלקה אחת, בפיקודו של עופר פניגר, שהייתה עסוקה בכיבוש בתים ממזרח לגבעה, נשלחה לסיוע לפלוגה ג\\'. המחלקה נכנסה לתעלה המזרחית והמשיכה בעקבות מחלקתו של מילר, שנבלמה כאמור מול \"בית הטלוויזיה\". מחלקה זו הצליחה להמשיך צפונה ולחמה באיטיות בחלקה הצפוני של התעלה ההיקפית, מול התנגדות עזה. הכוח בפיקודו של צביקה חבר אליה מאוחר יותר, וכך גם הכוח בפיקוד של אילון שכבש את בונקר הפיקוד; כוחות אלה התקדמו בתעלות לכוון התעלה המערבית והבונקר הגדול שהיה במרכזה. בשלב זה עלה השחר, ולאורו נחשפו הצנחנים לעיני החיילים הירדנים הן בבונקרים של הגבעה והן לחיילים ירדניים שישבו במוצב גבעת המבתר. רוב הלוחמים נפגעו וממחלקות שלמות נותרו רק קומץ של לוחמים._NEWLINE_משנודע למג\"ד על המצב הקשה בגבעת התחמושת הוא הורה למפקד פלוגה ב\\' להיכנס לגבעה. מפקד הפלוגה, דודיק, הורה לסגנו ניר ניצן לנוע בתעלה המערבית, והוא המשיך לכיוון התעלה המרכזית. עם תנועתה אל תעלות הקשר נכנסה הפלוגה לשטח ההריגה שנקרא, כאמור, משולש האש; מיד נפגעו מפקדים ולוחמים נוספים. דודיק הצליח להמשיך בהתקדמותו אל התעלה המרכזית ובמהלך ההתקדמות וההשתלטות על התעלה פגש ליד בתי המגורים את דדי, מ\"פ ג\\', שדווח לו על מצבה הקשה של פלוגתו. דדי סיפר שיש לו נפגעים רבים וכי הלוחמים המעטים מפוזרים ברחבי המוצב. הלחימה בתעלות הקשר נמשכה, מספר הפצועים וההרוגים המשיך לעלות ולא הושגה הכרעה._NEWLINE_פלוגה ב\\' הייתה מפוצלת בשלב זה לשני כוחות: כוח אחד, בפיקוד המ\"פ דודיק, שלחם בתעלה המרכזית, וכוח שני, בפיקוד הסמ\"פ ניר, שלחם בתעלה מערבית. בין שני הכוחות פעלה מחלקתו של המ\"מ יואב צורי, שהסתערה על משולש האש. בשלב זה הגיעו שני טנקים לגבעה. היו אלה הטנקים אשר ריתקו קודם לכן באש את מוצב גבעת המבתר והופנו עתה על ידי הסמג\"ד דורון לגבעת התחמושת. יואב ביקש את חיפוי הטנקים, ובעזרתם הצליחו למוטט את הבונקרים הירדניים בשטח ההריגה ולכבוש את התעלה הרוחבית. יואב עצמו נהרג במהלך ההסתערות האחרונה על הבונקרים. הלוחמים הנותרים המשיכו בהתקדמותם עד אשר הגיעו למקום בו התחברה התעלה הרוחבית עם הקטע המערבי של התעלה ההיקפית, מקום אשר כונה מאוחר יותר \"צומת הפצועים\"._NEWLINE_במקביל המשיך הסמ\"פ ניר ניצן, להתקדם, בראש עשרה לוחמים, בתעלה ההיקפית המערבית. ההתקדמות הייתה איטית וקשה. כל שלושה מטרים ניצב בונקר, שהיה מאויש בלגיונרים שירו אש כבדה והשליכו רימוני יד. מעבר לבונקרים, היו מחוץ לתעלה ליגיונרים רבים, שנמלטו מעמדותיהם וירו מבחוץ אל החיילים הישראליים שהיו בתוך התעלות. ניר חשש שהכוח בפיקודו, שהיה בעמדת נחיתות בתוך התעלה, ייפגע מאש זו. הוא החליט לשלוח מקלענים אל מחוץ לתעלה כדי לחפות על הכוח. ראשון יצא המקלען ישראל צוריאל, שחיפה מבחוץ עד שאזלה תחמושתו והוא חזר פנימה. אחריו יצא נפתלי כהן, שחיפה מבחוץ ברובהו עד שנפצע וחילץ את עצמו לאחור. ניר פקד כעת על המקלען איתן נאוה לצאת מחוץ לתעלה. מעשה הגבורה של איתן הונצח בשירו של יורם טהרלב, גבעת התחמושת, במילים שצוטטו מפיו של הסמ\"פ, ניר ניצן:_NEWLINE_לא היה לי זמן לשאול מי מתנדב, שלחתי את איתן._NEWLINE_איתן לא היסס לרגע, עלה למעלה והתחיל להפעיל את המקלעון._NEWLINE_לפעמים היה עובר אותי והייתי צריך לצעוק לו שיישאר בקו שלי._NEWLINE_ככה עברנו איזה שלושים מטר._NEWLINE_איתן היה מחפה מלמעלה ואנחנו טיהרנו את הבונקרים מבפנים,_NEWLINE_עד שנפגע בראשו ונפל פנימה._NEWLINE_עם נפילתו של איתן עלה שוב המקלען צוריאל במקומו; הצנחנים המשיכו הלאה בחיפוי שלו מבחוץ, והשאר כבשו את התעלות עד שהגיעו בסמוך לבונקר הגדול. עתה ידוע כי בשלב זה נותרו בצפון הגבעה 11 לוחמים ירדניים, שלושה מהם בבונקר הגדול. השאר הצליחו לברוח ולסגת לעבר תל שועפט.\\n_START_SECTION_\\nשלב שלישי: הבונקר הגדול וסיום הקרב\\n_START_PARAGRAPH_\\nהבונקר הגדול היה עמדת מקלע כבד, יצוקה בטון מזוין בעובי 40 ס\"מ ומחופרת היטב במרכז התעלה ההיקפית המערבית. הבונקר היה בנוי חדר בתוך חדר. החיילים הירדניים שהיו מבוצרים בעמדה, ישבו למעשה בתוך בונקר הנמצא בתוך בונקר, ולהם תחמושת ונשק לרוב. הבונקר נמצא בנקודה קריטית: כל הכוחות שפעלו בגבעה התכנסו לכיוונו, מן התעלה ההיקפית מצפון (מחלקת עופר והכוח של צביקה) מן התעלה ההיקפית מדרום (הכוח של ניר) וממרכז הגבעה (הכוחות של פלוגה ג\\'). יקי חץ (חיימוביץ) שהפך להיות חיל החוד של הכוח מפלוגה ב\\' שהגיע מצפון, מצא עצמו מול פתח הבונקר והשליך פנימה רימון. לאחר קרב יריות ורימונים החליט יקי להיכנס בזהירות לתוך הבונקר, נתקל בחייל ירדני והצליח לפגוע בו._NEWLINE_הצנחנים, שהבינו את מבנהו של הבונקר, ניסו לשתק אותו בעזרת מטול רקטות נגד טנקים, \"בזוקה\". ואולם, שתי פצצות שנורו גרמו נזק מועט בלבד לבונקר המבוצר היטב. לאחר שגם ניסיון זה לא הצליח לשתק את העמדה נעשה ניסיון לפגוע בעמדה בעזרת אחד משני הטנקים שפעלו על הגבעה, אך הטנק לא יכול היה להנמיך די הצורך את תותחו כדי לפגוע בבונקר. יהודה קנדל יצא מהתעלה, זחל לכיוון הבונקר, טיפס על גג העמדה, התכופף ושילשל לתוכה רימון. בכך איפשר לחיילים אחרים להגיע לבונקר ולהניח מטעני חומר נפץ. כוח נוסף שהגיע הביא חומר נפץ שהונח על הבונקר; 16 ק\"ג של חומר-נפץ הופעלו והבונקר הושמד בבוקר ה-6 ביוני. זה היה סיומו של הקרב._NEWLINE_הלוחמים שנותרו בחיים התארגנו לפינוי הנפגעים, וחוליות נוספות סרקו את התעלות וריכזו את הפצועים. הכוחות עדיין היו תחת אש מגבעת המבתר, קרב יריות שנמשך עד לשעה 12:00, בה כבשו כוחות מחטיבה 10 את מוצב גבעת המבתר._NEWLINE_יום למחרת הקרב, נכנס כוח מפלוגה ב\\' של משמר הגבול, בפיקוד מפקד הפלוגה צבי בן אליהו, והשתלט בפעם השנייה על גבעת התחמושת. ההשתלטות ארכה כ-8 שעות ובמהלכה נאספו כ-60 שבויים ירדניים._NEWLINE_36 צנחנים נהרגו בקרב זה - 21 בגבעת התחמושת ו- 12 בתעלות הקשר המתחברות אל הגבעה. כ-70 חיילים ירדניים נהרגו בהגנה על גבעת התחמושת - כמחצית הכוח שהיה במוצב.\\n_START_SECTION_\\nביקורת ומחלוקת\\n_START_PARAGRAPH_\\nבמהלך השנים נשמעו ביקורות על הקרב ודרך ניהולו, בקרב לוחמי החטיבה ובין היסטוריונים צבאיים, כמו אורי מילשטיין. מספר הנופלים הגבוה, וההרגשה כי הקרב עצמו לא היה הכרחי, הלהיטו את הוויכוח. הטענות נגעו לנשק שאותו נשאו הלוחמים - נשק ארוך קנה (רובים מדגם FN), שלא התאים ללחימת תעלות; למידע ולאימון - הצנחנים לא ידעו דבר כמעט על גבעת התחמושת; וכן לעצם נחיצות הקרב - גבעת התחמושת על מגיניה הירדניים המחופרים בבונקרים לא היוותה איום על כיבוש ושליטה מלאים בירושלים, וניתן היה - על פי המבקרים - לכתר את הגבעה מבלי לכבשה ולהניח ללוחמים הירדניים להיכנע עם ניתוקם מהצבא הירדני._NEWLINE_יודגש כי הביקורת לא הייתה על הלוחמים שלחמו על הגבעה, אלא התמקדה בפיקוד החטיבה ופיקוד מרכז.\\n_START_SECTION_\\nמודיעין\\n_START_PARAGRAPH_\\nלפני מלחמת ששת הימים, נאסף מידע מודיעיני רב על המערך הירדני, אך לקראת ההתקפה קיבלו קציני המודיעין של הצנחנים שני תצלומים של היעד בלבד. בכתבתו הקרב על הקרב על ירושלים שפורסמה בעיתון הארץ מצטט הכתב אנשיל פפר את מיכה עשת קמב\"ץ גדוד 66, שסיפר: \"...אפס חומר מודיעיני. מוטה לקח את המג\"דים לסיור אבל כל מה שהיה זה תצפית שראו בעין, לחלקנו הייתה היכרות עם הקו העירוני מקורס מ\"פים אבל שום תוכניות להתקפה. לפני הקרב ראינו בקושי את הצללית של בית הספר לשוטרים\". כן מצוטט בכתבה עמוס נאמן קצין האג\"מ בחטיבה הירושלמית שמספר כי הוא העביר לקציני הצנחנים את כל המפות ותצלומי אוויר שהיו על קירות המפקדה.\\n_START_SECTION_\\nאתר הנצחה\\n_START_PARAGRAPH_\\nהקמתו של אתר ההנצחה בגבעת התחמושת הייתה ביוזמת ההורים השכולים. לאחר מלחמת ששת הימים, כאשר החלו עבודות להכשרת הגבעות החשופות שבצפון-מזרח ירושלים שעד המלחמה היו שטח הפקר להקמת שכונות מגורים, פנה מג\"ד 66, יוסי יפה, ליצחק פניגר, אביו של עופר פניגר, אחרון הנופלים בקרב, וזה האחרון כינס כמה הורים שכולים שביקשו לשמר את מוצב גבעת התחמושת ולהקים בו אתר הנצחה מרכזי לנופלים._NEWLINE_בשנת 1968 הוחל בתכנון אתר ההנצחה במקום. את התכנון ביצע משרד האדריכלים בנימין אידלסון – גרשון צפור כשעל עיצוב הפנים והתצוגה הופקד רפי בלומנפלד._NEWLINE_בשנת 1975 נחנך בגבעת התחמושת אתר ההנצחה, שבו נשמר חלק מהמוצב שהיה במקום, והוקם בו מוזיאון להנצחת הלוחמים. על קיר במוזיאון חקוקים שמות 182 הנופלים במערכה על ירושלים במלחמת ששת הימים. במקום ניטעו 182 עצי זית לזכר הנופלים בקרב על ירושלים. חלק מן המוצב נהרס, אך חלקיו המרכזיים והמערביים נותרו כשהיו בעת הלחימה. בשנת 1990 הוכר האתר כאתר הנצחה ממלכתי._NEWLINE_כך מתאר האדריכל גרשון צפור את אתר ההנצחה:_NEWLINE_באתר ההנצחה שני חלקים עיקריים: המבנים מתחת לקרקע, שהינם \\'בונקרים\\' ו\\'תעלות\\'. אלה מהווים רקע לקרבות המרים, שנערכו בגבעת התחמושת; והמבנים שמעל הקרקע, המשמשים ללימוד הלחימה של יחידות צה\"ל בקרב על ירושלים ומוקד לזיכרון ולהתייחדות._NEWLINE_הגישה אל האתר היא ממפלס נמוך, הדומה לכניסות אל הבונקרים. מחלל הכניסה יש מעבר למסדרון מקורה, הבנוי בחלקו בטון חשוף ובחלקו אבן גסה, המשרה תחושה של מעבר בתעלות קשר בין הבונקרים לנקודות האיסוף. לאורך קירות המסדרון נחצבו גומחות, ובהן מוצגים כלי נשק שהונחו באופן פיסולי, אות ועדות לכלי המלחמה, שבהם השתמשו הלוחמים. באולמות ובמבואות נפרצו פתחי התבוננות ותצפית אל ירושלים, ומהם ניתן לראות את נופי העיר. קיימים גם פתחים ודלתות אל תעלות הקשר, הנמצאות מחוץ למבנה והמקיפות את הגבעה כולה._NEWLINE_המראה, שנצפה על גבעת התחמושת לפני מלחמת ששת הימים, היה של עצי אורן וביניהם היו פזורים מבנים עם קשתות וגגות פח עגולים. לכן תוכנן המבנה שמעל הקרקע באופן דומה. הוא עשוי מבטון בהיר ומקשתות המשתזרות אחת בשנייה. בדרך זו מתהווים חללים המשתלבים והמתאחדים מדי פעם, כאשר עוצמת התאורה והמבטים משתנים ומפתיעים. הקשתות מחודדות מעט וגבוהות, והן יוצרות ניגוד לבונקרים ולתעלות. כך מודגש המעבר מחשיכה אל האור, מהלחימה לניצחון._NEWLINE_על קיר מרוקע זהב באולם הזיכרון וההתייחדות חקוקים שמות 183 החללים, סדורים לפי חטיבותיהם: צנחנים, ירושלים, שריון וחיל אוויר. קיר זה הנו מוקד חדר ההתייחדות. נר תמיד מהבהב בפינה. מאולם זה ישנה יציאה אל משטח המסדרים ואל הנוף של ירושלים וסביבתה. הגבעה עצמה נשארה כפי שהייתה לפני הקמת האתר, ולא נעשו בה שינויים חזותיים ניכרים_NEWLINE_מקומו המרכזי של שדה הקרב גבעת התחמושת במיתוס הגבורה בישראל, והקשר הישיר שיש לו למערכה על ירושלים, הפכו את אתר ההנצחה לנקודת מוקד טקסית בעלת משמעות לאומית. במקום נערך מדי שנה ביום ירושלים טקס הזיכרון הממלכתי המרכזי._NEWLINE_באתר ההנצחה מוצבים זחל\"מ הפיקוד של מוטה גור, טנק השרמן של הפלוגה הירושלמית וג\\'יפ התול\"ר ומשוריין של מגיני הר הצופים._NEWLINE_ב 29 נובמבר 2017 עמותת גבעת התחמושת שהפעילה את המקום כל השנים, הפכה להיות תאגיד ממשלתי, המדינה לקחה אחריות על המקום ומממנת את התפעול השוטף של האתר. בחמש השנים האחרונות הושקעו באתר כ 25 מיליון ש״ח ,במסגרת פרויקט מורשת שסייעו להקים במקום מוזיאון חדש וייחודי ,היכל הנצחה חדש גם הוא ייחודי,אתר הקרב עבר תהליך שימור והוקם מסביב האתר גן ובתוכו שביל תוכן המתאר את תולדות ירושלים במלל ותמונות,מתקופת השלטון העות\\'מאני ועד מלחמת ששת הימים._NEWLINE_בנוסף עמותת גבעת התחמושת יזמה מחקר מקיף המתאר את כל הקרבות בירושלים,של כל החטיבות והיחידות שלחמו בעיר במלחמת ששת הימים,המחקר יצא כספר לאחרונה ושמו-67, ירושלים מלחמה.\\n_START_SECTION_\\nהנצחת הלוחם היהודי\\n_START_PARAGRAPH_\\nארגון הווטרנים היהודים העולמי: International Council of jewish war veterans החליט להקים את מרכזו בירושלים בגבעת התחמושת._NEWLINE_אתר ההנצחה בסיוע קרן קיימת לישראל – J.N.F ארצות הברית קיבלו על עצמם לממש את פרויקט: \"כבוד ותהילה ללוחמים היהודים\". במטרה לרכז מידע ונתונים על פועלם של הלוחמים היהודים לדורותיהם, שמותיהם ופרטים אישיים, וכן להנציחם על \"קיר הנצחה ללוחם היהודי\"._NEWLINE_שרותם, של הלוחמים היהודים בצבאות ומסגרות אחרות (מחתרות, פרטיזנים, משטרה וכל שרות ביטחוני אחר) בארצותיהם במלחמות העולם, ביניהן, לפניהן ואחריהן כולל למעלה מ-3 מיליון לוחמים יהודים.\\'',\n",
       " 'version_id': \"b'7275818517328639782'\",\n",
       " 'anchor_text': '[TASK_TITLE_DOC] [QUERY] הקרב על גבעת התחמושת תחילת המלחמה בירושלים',\n",
       " 'positive_text': '[DOCUMENT] בבוקר יום 5 ביוני 1967 פתחו כוחות הלגיון הירדני בהרעשה ארטילרית על ירושלים המערבית, וחיילי הלגיון הירדני השתלטו על מטה משקיפי האו\"ם ששכן בארמון הנציב. מלך ירדן, חוסיין, שעד אותו בוקר היסס בשאלה האם להצטרף למלחמה נגד ישראל, הכריע בעד שיתוף הפעולה עם מצרים וסוריה, והחליט להעביר את צבאו לפיקוד מצרי. המלך הסתמך על ידיעות מצריות מוטעות, והשתכנע כי הקרב בסיני הוכרע לטובת הכוחות המצריים, ומכאן שיהיה זה לטובת האינטרסים הירדניים להיכנס למלחמה. יחד עם זאת, הוצבו לצבא הירדני מטרות מוגבלות בלבד, מתוך כוונה לבצע מחטפים מוצלחים בנקודות בעלות חשיבות אסטרטגית לאורך הקו העירוני.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032da621",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_dataset(dataset_name, splits=['train', 'validation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
