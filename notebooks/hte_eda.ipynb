{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2584a5c5",
   "metadata": {},
   "source": [
    "# H T E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd4a3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep  5 16:13:17 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   33C    P0              26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ee578c",
   "metadata": {},
   "source": [
    "## Setup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1335cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U torch transformers bitsandbytes datasets huggingface_hub accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6594aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "import sys\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa75b708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d042c2502ca04d17a4797845c4d0f508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2734386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /home/ec2-user/SageMaker/sandbox/hte\n",
      "PYTHONPATH updated with: /home/ec2-user/SageMaker/sandbox/hte\n"
     ]
    }
   ],
   "source": [
    "project_dir = '/home/ec2-user/SageMaker/sandbox/hte'\n",
    "\n",
    "os.chdir(project_dir)\n",
    "print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "    \n",
    "    \n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)  # Add it to the front of PYTHONPATH\n",
    "    print(f\"PYTHONPATH updated with: {project_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d06bcf",
   "metadata": {},
   "source": [
    "## Load the data and prepare it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e4a61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/importlib/__init__.py:126: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 165359\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9231\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['wikidata_id', 'text', 'version_id'],\n",
       "        num_rows: 9344\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wiki40b\", \"he\")  # Specific version and Hebrew language code\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d8da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "\n",
    "def decode_text(text):\n",
    "    decoded_text = bytes(text, \"utf-8\").decode(\"unicode_escape\").encode(\"latin1\").decode(\"utf-8\")\n",
    "    return decoded_text\n",
    "\n",
    "# Apply the decoding function to the dataset\n",
    "decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5773d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wiki_article(text):\n",
    "    lines = text.strip().split('\\n')\n",
    "\n",
    "    PARAGRAPH_DIVIDER = '_NEWLINE_'\n",
    "\n",
    "    # Initialize variables\n",
    "    article_dict = {'title': '', 'abstract': '', 'sections': []}\n",
    "    current_section = None\n",
    "    abstract_parsed = False\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        if line == \"_START_ARTICLE_\":\n",
    "            # The next line is the title\n",
    "            article_dict['title'] = lines[i + 1].strip()\n",
    "            i += 2  # Move to the next relevant line\n",
    "        elif line == \"_START_PARAGRAPH_\":\n",
    "            # If the abstract has not been parsed and the current section is None, this is the abstract\n",
    "            paragraph = lines[i + 1].strip()\n",
    "            if not abstract_parsed and not current_section:\n",
    "                article_dict['abstract'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "                abstract_parsed = True\n",
    "            elif current_section:\n",
    "                current_section['paragraphs'] = paragraph.split(PARAGRAPH_DIVIDER)\n",
    "            i += 2\n",
    "        elif line == \"_START_SECTION_\":\n",
    "            # The next line is the section name\n",
    "            section_name = lines[i + 1].strip()\n",
    "            current_section = {'section': section_name, 'paragraphs': ''}\n",
    "            article_dict['sections'].append(current_section)\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1  # Move to the next line if none of the cases match\n",
    "\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a522de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'הקמרילה של קרול השני, מלך רומניה',\n",
       " 'abstract': ['הקמרילה של קרול השני, מלך רומניה (ברומנית: Camarila lui Carol al II lea) הוא השם המקובל של החוג הפנימי של קרול השני, מלך רומניה. הקמרילה השפיעה על הכלכלה והפוליטיקה הרומנית ותפסה חלק ניכר מהכיסוי התקשורתי של התקופה.',\n",
       "  'הקמרילה עמדה במרכז התקפות האופוזיציה על ממשלו של המלך קרול השני ועם הדחתו הגיעה לקץ השפעתה וחלק מחבריה עזבו את רומניה יחד עם המלך.'],\n",
       " 'sections': [{'section': 'חברי הקמרילה',\n",
       "   'paragraphs': ['במרכז הקמרילה עמדו בעלי תפקידים בארמון, פילגש המלך, אלנה לופסקו, תעשיינים ובנקאים גדולים ותופסי עמדות מפתח בממשל. בין חברי הקמרילה היו אנטישמיים ויהודים, חלק פעלו בשיתוף פעולה וחלקם זממו האחד נגד חברו. היו חברי קמרילה שהודרו והיו אחרים שצורפו.',\n",
       "    'פויו (קונסטנטין) דומיטרסקו (Puiu (Constantin) Dumitrescu), בנו של קולונל רומני, היה סטודנט בפריז בתקופה בה שהה שם הזוג קרול קאראימאן (השם של קרול השני בתקופה בה ויתר על ירושת המלוכה וגלה מארצו) ואלנה לופסקו. הוא ביצע עבור קרול שירותים אישיים וליווה אותו לבתי הימורים. עם שובו של קרול לרומניה והכתרתו ב-1930, ליווה אותו דומיטרסקו והפך למזכיר האישי של המלך ועמוד ראשי בקמרילה שלו. הוא זכור גם בגלל \"פרשת המטפחות\", כאשר אביו של דומיטרסקו (שקודם לדרגת גנרל על ידי המלך ומונה למפקד הז\\'נדרמריה) ובן דוד של אלנה לופסקו הפיקו מטפחות עם דיוקנו של המלך קרול וכל אנשי הצבא והמשטרה חויבו לרכוש מטפחת כזאת. בינואר 1934, לאחר הרצח של יון ג. דוקה, אולץ דומיטרסקו לעזוב את רומניה ומאוחר יותר, ב-1935, האב הועמד למשפט באשמת שחיתות ונידון לחמש שנות מאסר ומת בכלא - היו שמועות על הרעלה, אך לא נערכה בדיקה. באותה תקופה צונזרה התקשורת בכל הנוגע לדומיטרסקו, לכן לא ברור מה הייתה הסיבה להרחקתו, יש הקושרים זאת לעימות בינו ובין אלנה לופסקו ויש הקושרים זאת לתביעות של פוליטיקאים, שבאו במקום דוקה.',\n",
       "    'ארנסט אורדראנו, שהתמנה במקומו של דומיטרסקו, היה שחקן מרכזי בקמרילה ושיתף פעולה היטב עם אלנה לופסקו. הוא היה נאמן למלך וליווה אותו לאחר יציאתו לגלות ועד מותו.',\n",
       "    'הבנקאי היהודי אריסטידה בלנק סייע בסכומי כסף גדולים למלך קרול לפני שהיה מלך, בתקופה בה הודח מזכויותיו כבן משפחת המלוכה וחי בצרפת ללא שום הכנסה. הסיוע של בלנק, שניתן ללא התניה כלשהי, הפך את בלנק לאחד המקורבים ביותר למלך לאחר המלכתו.',\n",
       "    'פרופסור נאה יונסקו היה פילוסוף ותאולוג נוצרי אורתודוקסי, אוהד התנועה הלגיונרית והמשטרים הפשיסטיים. הוא היה בעל קשרים טובים בגרמניה הנאצית ועזר לניקולאה מאלאקסה להשיג חוזים משתלמים איתה וקיבל ממנו תמורה יפה.',\n",
       "    \"גבריאל מרינסקו היה במשך שנים מפקד משטרת בוקרשט ובהמשך גם תת-שר הפנים, שר הפנים ושר הממונה על הסדר הציבורי. נרצח בטבח ז'ילבה.\",\n",
       "    'מקס אאושניט היה תעשיין יהודי ברומניה, מתחרה של ניקולאה מאלאקסה וחבר נכבד בקמרילה המלכותית עד שהמלך שינה את טעמו ודרש ממנו להעביר לו את הבעלות על עסקיו וכשסירב שלח אותו לבית סוהר.',\n",
       "    'ניקולאה מאלאקסה היה תעשיין רומני ממוצא יווני וחבר נכבד בקמרילה המלכותית, שהתחרה בהתמדה במקס אאושניט. כשאאושניט נתן למלך במתנה סוס מרוצים ערבי טהור גזע, נתן מאלאקסה למלך אורווה מודרנית מצוידת היטב.',\n",
       "    \"מיכאיל מורוזוב היה ראש הסיגורנצה ועשה שירותים שונים למען המלך ופילגשו. נרצח בטבח ז'ילבה.\",\n",
       "    \"גאורגה טטרסקו, בן אצולה רומני בעל הכשרה של משפטן, פנה לפוליטיקה וקרבתו למלך הביאה אותו פעמיים לכס ראשות הממשלה.'\"]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "text = decoded_dataset['train'][12]['text']\n",
    "parsed_article = parse_wiki_article(text)\n",
    "parsed_article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4a28a",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac91230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import logging\n",
    "import os\n",
    "from datasets import DatasetDict, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562a2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset_wiki40b(tokenizer):\n",
    "    dataset = load_dataset(\"wiki40b\", \"he\")\n",
    "    decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})\n",
    "\n",
    "    def transform_entry(entry):\n",
    "        # Process the 'text' using parse_wiki_article\n",
    "        article = parse_wiki_article(entry['text'])\n",
    "\n",
    "        # Extract anchor_text and positive_text based on the parsed output\n",
    "        anchor_text = article['title']\n",
    "        if 'sections' in article and len(article['sections']) > 0:\n",
    "            anchor_text += \" \" + article['sections'][0]['section']\n",
    "            positive_text = article['sections'][0]['paragraphs'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "        else:\n",
    "            positive_text = article['abstract'][0]\n",
    "            positive_text += tokenizer.eos_token\n",
    "\n",
    "        # Return the transformed data\n",
    "        return {\n",
    "            'anchor_text': anchor_text,\n",
    "            'positive_text': positive_text\n",
    "        }\n",
    "\n",
    "    # Apply the transformation to the train, validation, and test subsets\n",
    "    transformed_dataset = {}\n",
    "    for subset in ['train', 'validation', 'test']:\n",
    "        # Transform each subset of the dataset using map (this processes each 'text' entry)\n",
    "        transformed_subset = decoded_dataset[subset].map(transform_entry)\n",
    "        transformed_dataset[subset] = transformed_subset\n",
    "\n",
    "    # Return the transformed dataset as a DatasetDict\n",
    "    return DatasetDict(transformed_dataset)\n",
    "\n",
    "\n",
    "def transform_dataset(dataset_name, **kwargs):\n",
    "    if dataset_name == 'wiki40b':\n",
    "        return transform_dataset_wiki40b(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253b117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the logger\n",
    "def setup_logger(log_file):\n",
    "    log_dir = os.path.dirname(log_file)\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    \n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Create handlers for both console and file output\n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(log_file)\n",
    "\n",
    "    # Set up the format for logging\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add the handlers to the logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b041f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCELoss(torch.nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - temperature: Scaling factor applied to the logits before applying the softmax function.\n",
    "        \"\"\"\n",
    "        super(InfoNCELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, anchor, positive, negatives):\n",
    "        \"\"\"\n",
    "        Compute the InfoNCE loss.\n",
    "\n",
    "        Parameters:\n",
    "        - anchor: Tensor of shape (batch_size, embedding_dim) - anchor samples\n",
    "        - positive: Tensor of shape (batch_size, embedding_dim) - positive samples corresponding to each anchor\n",
    "        - negatives: Tensor of shape (batch_size, num_negatives, embedding_dim) - negative samples\n",
    "\n",
    "        Returns:\n",
    "        - loss: Computed InfoNCE loss\n",
    "        \"\"\"\n",
    "        batch_size = anchor.size(0)\n",
    "        num_negatives = negatives.size(1)\n",
    "\n",
    "        # Normalize embeddings to unit vectors\n",
    "        anchor = F.normalize(anchor, dim=-1)\n",
    "        positive = F.normalize(positive, dim=-1)\n",
    "        negatives = F.normalize(negatives, dim=-1)\n",
    "\n",
    "        # Calculate the positive logits (similarity between anchor and positive)\n",
    "        positive_logits = torch.sum(anchor * positive, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "\n",
    "        # Calculate the negative logits (similarity between anchor and negatives)\n",
    "        negative_logits = torch.bmm(negatives, anchor.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, num_negatives)\n",
    "\n",
    "        # Concatenate positive and negative logits\n",
    "        logits = torch.cat([positive_logits, negative_logits], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        logits = logits / self.temperature\n",
    "\n",
    "        # Create labels - 0 for the positive samples, as it is the first in the concatenated logits\n",
    "        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)\n",
    "\n",
    "        # Compute the InfoNCE loss using cross-entropy\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bad7c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, checkpoint_dir):\n",
    "    latest_checkpoint = None\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n",
    "        if checkpoint_files:\n",
    "            latest_checkpoint = sorted(checkpoint_files)[-1]  # Get the latest checkpoint\n",
    "\n",
    "    if latest_checkpoint:\n",
    "        logger.info(f\"Loading checkpoint {latest_checkpoint}\")\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        return checkpoint['epoch']  # return the epoch to resume from\n",
    "\n",
    "    logger.info(\"No checkpoint found. Starting from scratch.\")\n",
    "    return 0  # Start from the first epoch if no checkpoint found\n",
    "\n",
    "\n",
    "# Save model and optimizer state\n",
    "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
    "    create_directory(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09d3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            anchor_ids, positive_ids, *negative_ids = batch\n",
    "            \n",
    "            # Forward pass to get embeddings for validation\n",
    "            anchor_embeds = model(anchor_ids).last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "            positive_embeds = model(positive_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "            # Process multiple negatives\n",
    "            negatives_embeds = torch.stack([\n",
    "                model(negative_id_batch).last_hidden_state[:, 0, :] for negative_id_batch in negative_ids\n",
    "            ], dim=1)\n",
    "\n",
    "            # Compute the validation loss\n",
    "            val_loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "\n",
    "def train(\n",
    "    model, \n",
    "    optimizer,\n",
    "    criterion, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    epochs, \n",
    "    start_epoch=0, \n",
    "    checkpoint_dir='checkpoints', \n",
    "):\n",
    "    model.train()\n",
    "    best_val_loss = float('inf')  # Initialize best validation loss to infinity\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            anchor_ids, anchor_mask, positive_ids, positive_mask = batch\n",
    "\n",
    "            # Forward pass to get the embeddings\n",
    "            anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n",
    "            anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n",
    "            positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "\n",
    "            # Set negatives as the other positives in the batch\n",
    "            # Create a matrix where the negatives are shifted versions of positives\n",
    "            batch_size = positive_embeds.size(0)\n",
    "#             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n",
    "            # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n",
    "            negatives_embeds_list = []\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # Exclude the current index `i` using slicing\n",
    "                negatives_embeds = torch.cat([positive_embeds[:i], positive_embeds[i+1:]], dim=0)\n",
    "\n",
    "                # Append the result to the list\n",
    "                negatives_embeds_list.append(negatives_embeds)\n",
    "\n",
    "            # Stack the negatives for each sample in the batch\n",
    "            # Each entry in the batch now has (batch_size - 1) negative embeddings\n",
    "            negatives_embeds = torch.stack(negatives_embeds_list)\n",
    "\n",
    "            # Compute the InfoNCE loss\n",
    "            loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "        # Compute validation loss after each epoch\n",
    "        avg_val_loss = validate(model, val_dataloader, criterion)\n",
    "        logger.info(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        save_checkpoint(model, optimizer, epoch, \"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90891a",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "183ab254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720181eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'intfloat/multilingual-e5-base'\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "INFONCE_TEMPERATURE = 0.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e252173",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "logger = setup_logger(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-05 16:13:39,350 - INFO - Start train base model: intfloat/multilingual-e5-base\n",
      "2024-09-05 16:13:40,397 - INFO - Switching to new dataset: wiki40b\n",
      "2024-09-05 16:14:50,239 - INFO - No checkpoint found. Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Log file setup\n",
    "model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n",
    "log_file = f\"logs/hte_training_{model_name_slug}.log\"\n",
    "\n",
    "\n",
    "# Define model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "logger.info(f\"Start train base model: {MODEL_NAME}\")\n",
    "\n",
    "# Initialize the InfoNCE loss and the optimizer\n",
    "criterion = InfoNCELoss(temperature=INFONCE_TEMPERATURE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Datasets to train on\n",
    "dataset_names = ['wiki40b']\n",
    "\n",
    "# Iterate over datasets and train\n",
    "for dataset_name in dataset_names:\n",
    "    start_datetime = datetime.now()\n",
    "    \n",
    "    logger.info(f\"Switching to new dataset: {dataset_name}\")\n",
    "    dataset = transform_dataset(dataset_name, tokenizer=tokenizer)\n",
    "    \n",
    "    # Tokenize the train dataset\n",
    "    anchor_inputs_train = tokenizer(dataset['train']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_train = tokenizer(dataset['train']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Create DataLoader for training\n",
    "    train_dataset = TensorDataset(anchor_inputs_train['input_ids'], anchor_inputs_train['attention_mask'], \n",
    "                                  positive_inputs_train['input_ids'], positive_inputs_train['attention_mask'])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # Tokenize the validation dataset\n",
    "    anchor_inputs_val = tokenizer(dataset['validation']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    positive_inputs_val = tokenizer(dataset['validation']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    # Create DataLoader for validation\n",
    "    val_dataset = TensorDataset(anchor_inputs_val['input_ids'], anchor_inputs_val['attention_mask'], \n",
    "                                positive_inputs_val['input_ids'], positive_inputs_val['attention_mask'])\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Load the latest checkpoint if available and resume training\n",
    "    checkpoint_dir = \"checkpoints\"\n",
    "    start_epoch = load_checkpoint(model, optimizer, checkpoint_dir)\n",
    "\n",
    "    # Train the model for this dataset\n",
    "    train(\n",
    "        model=model, \n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_dataloader=train_dataloader, \n",
    "        val_dataloader=val_dataloader, \n",
    "        epochs=3, \n",
    "        start_epoch=start_epoch,\n",
    "    )\n",
    "    \n",
    "    end_datetime = datetime.now()\n",
    "    logger.info(f\"Total training on {dataset_name} elapsed time is {(end_datetime - start_datetime).total_seconds()} seconds\")\n",
    "    \n",
    "logger.info(f\"End train base model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f18ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03137d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../data/synthetic_data_20240906_0018.pkl ./data.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "449c6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ed50ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1902"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ce3d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_query': 'דוחות בדיקה ספרותיים לתקופה הנצרות',\n",
       " 'positive_document': \"הנצרות המוקדמת התפתחה בתוך הקהילה היהודית המתרחבת של אותה תקופה. בעוד שהיהדות המקורית התבססה על כתבי הקודש העבריים, הנצרות אימצה גם את הברית החדשה, הכוללת את הבשורות על חייו של ישו והאיגרות של שליחיו. עם זאת, דוחות מפורטים על חייו של ישו ועל התפתחות הנצרות המוקדמת בתקופה זו הם מועטים ביותר. מרבית הידע שלנו על התקופה נשען על האמונות והמסורות שהתפתחו מאוחר יותר בכנסייה הנוצרית. דוחות ספרותיים מאותה תקופה מתארים במקרים רבים את היחסים המורכבים בין הנצרים לבין הקהילות היהודיות והרומיות ששלטו בארץ ישראל. כמה דוחות משקפים גם את המאבקים הפנימיים בין קבוצות שונות של נוצרים על פרשנות התנ'ך החדש והאמונות האמיתיות של ישו.\",\n",
       " 'hard_negative_document': \"פילוסופיה יהודית היא תחום עיון עתיק יומין, הנוגע בהיבטים המטאפיזיים והאתיים של המחשבה היהודית. תורת הקבלה, למשל, חוקרת את הטבע האלוהי ואת הקשרים המיסטיים בין האל לבריאה. בנוסף, חכמת המוסר העברית דנה ברעיונות מוסריים כמו חמלה, צדקה וחסד. עם זאת, בניגוד לפילוסופיה המערבית, המחשבה היהודית מסורתית אינה מחולקת לתחומים מובחנים כמו מטאפיזיקה, אפיסטמולוגיה ואתיקה. במקום זאת, היא נטועה בתוך עולם התורה והמצוות, כשהיא מחפשת תובנות בתוך המקורות הקדושים של העם היהודי. מקורות אלה כוללים את התנ'ך, התלמוד, כתבי קבלה וספרי מוסר יהודיים מזמנים שונים. לכן, הפילוסופיה היהודית אינה תחום עצמאי, אלא חלק בלתי נפרד מהמסורת הרוחנית והתרבותית של היהדות.\",\n",
       " 'timestamp': datetime.datetime(2024, 9, 6, 5, 10, 58, 915768),\n",
       " 'random_seed': 1042,\n",
       " 'task': 'Find relevant news articles discussing a current event or trending topic (e.g., \"climate change impact on coastal cities\").',\n",
       " 'query_type': 'extremely long-tail',\n",
       " 'query_length': 'less than 5 words',\n",
       " 'difficulty': 'college',\n",
       " 'clarity': 'clear',\n",
       " 'num_words': '200',\n",
       " 'language': 'Hebrew',\n",
       " 'input_tokens': 348,\n",
       " 'output_tokens': 887,\n",
       " 'success': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e432e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4cd492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32af3245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial allocated memory: 1.953125 MB\n",
      "Initial reserved memory: 20.0 MB\n",
      "Final allocated memory: 1956.8583984375 MB\n",
      "Final reserved memory: 1974.0 MB\n",
      "Memory increase from initial to final (allocated): 1954.9052734375 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the batch size and embedding dimension\n",
    "batch_size = 1000\n",
    "embed_dim = 512\n",
    "\n",
    "# Create dummy positive embeddings (batch_size, embed_dim)\n",
    "positive_embeds = torch.randn(batch_size, embed_dim, device=device)\n",
    "\n",
    "# Measure initial memory usage\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "initial_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Initial allocated memory: {initial_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Initial reserved memory: {initial_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Pre-allocate a tensor for negatives (shape: batch_size, batch_size - 1, embed_dim)\n",
    "negatives_embeds = torch.zeros(batch_size, batch_size - 1, embed_dim, device=device)\n",
    "\n",
    "# Create an identity mask to exclude diagonal elements (positives)\n",
    "negatives_mask = torch.eye(batch_size, dtype=torch.bool).to(device)\n",
    "\n",
    "# Fill the negatives_embeds in place, row by row\n",
    "for i in range(batch_size):\n",
    "    # Select all positive embeddings except the current index `i`\n",
    "    negatives_i = positive_embeds[~negatives_mask[i]]  # Exclude diagonal\n",
    "    negatives_embeds[i] = negatives_i  # In-place assignment\n",
    "\n",
    "# Measure memory usage after in-place assignments\n",
    "final_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "final_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Final allocated memory: {final_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Final reserved memory: {final_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Compare memory usage before and after assignments\n",
    "print(f\"Memory increase from initial to final (allocated): {(final_allocated_memory - initial_allocated_memory) / (1024 ** 2)} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da094db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial allocated memory: 1.953125 MB\n",
      "Initial reserved memory: 20.0 MB\n",
      "Final allocated memory: 1954.9072265625 MB\n",
      "Final reserved memory: 1974.0 MB\n",
      "Memory increase from initial to final (allocated): 1952.9541015625 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set the batch size and embedding dimension\n",
    "batch_size = 1000\n",
    "embed_dim = 512\n",
    "\n",
    "# Create dummy positive embeddings (batch_size, embed_dim)\n",
    "positive_embeds = torch.randn(batch_size, embed_dim, device=device)\n",
    "\n",
    "# Measure initial memory usage\n",
    "initial_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "initial_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Initial allocated memory: {initial_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Initial reserved memory: {initial_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "batch_size = positive_embeds.size(0)\n",
    "negatives_mask = torch.eye(batch_size, dtype=torch.bool).to(device)  # Identity matrix to mask out positives\n",
    "positive_embeds_reshaped = positive_embeds.unsqueeze(0)  # Shape: (1, batch_size, embed_dim)\n",
    "\n",
    "# Use the mask to select negatives (all non-diagonal elements are negatives)\n",
    "# negatives_embeds = positive_embeds_reshaped.masked_select(~negatives_mask.unsqueeze(-1))#.view(batch_size, batch_size - 1, -1)\n",
    "# Pre-allocate a tensor for negatives (shape: batch_size, batch_size - 1, embed_dim)\n",
    "negatives_embeds = torch.zeros(batch_size, batch_size - 1, embed_dim, device=device)\n",
    "\n",
    "# Measure memory usage after in-place assignments\n",
    "final_allocated_memory = torch.cuda.memory_allocated(device)\n",
    "final_reserved_memory = torch.cuda.memory_reserved(device)\n",
    "\n",
    "print(f\"Final allocated memory: {final_allocated_memory / (1024 ** 2)} MB\")\n",
    "print(f\"Final reserved memory: {final_reserved_memory / (1024 ** 2)} MB\")\n",
    "\n",
    "# Compare memory usage before and after assignments\n",
    "print(f\"Memory increase from initial to final (allocated): {(final_allocated_memory - initial_allocated_memory) / (1024 ** 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e76af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry 1, Paragraph 1: לאחר שהטיוטות הראשוניות הובאו למנהלת העם וספגו ביקורת חריפה, הוקמה ועדת חמישה בראשות משה שרת שניסחה טיוטה רביעית. ב-13 במאי לפנות ערב הוגשה הצעתה למנהלת העם. שרת עשה את רוב המלאכה, תוך התייעצות עם משפטנים מומחים. נוסח ההכרזה הושאל מכתב המנדט וממסמכים משפטיים רבים אחרים, וכל סעיף שלו התחיל במילים \"הואיל ו...\". בהצעה זו הוזכרה גם תוכנית החלוקה של האו\"ם. מזכיר מנהלת העם כתב כי \"בן-גוריון התנגד ל'הואיל' כי אינו עברי\" וכן \"התנגד למילים 'ישוב רב־איל', 'עוז וגבורה' וכיוצא באלה\". כמו כן, התנגד להזכרה מפורשת של תוכנית החלוקה.\n",
      "Entry 1, Paragraph 2: יש הטוענים ששמו של חיים ויצמן, נשיאה הראשון של מדינת ישראל, נעדר מרשימת חותמי המגילה. ניתן להבחין בטור הראשון של החותמים כי המקום בין אליהו דובקין ומאיר וילנר, בו אמור היה ויצמן לחתום לפי סדר אלפביתי, נותר ריק. לתעלומה זו שני הסברים מתחרים. הסבר אחד מצביע על ראשון חותמי המגילה, דוד בן-גוריון, שחתימתו חורגת מהסדר האלפביתי. לפי גרסה זו מחה ויצמן על החריגה מסדר החתימה ולא הסכים לחתום. לשיטתו לא צריכה הייתה להיות חריגה שכזו, אך אם נעשתה, הרי שכנשיא המדינה שמו שלו צריך היה להיות בראש החותמים. גרסה שנייה טוענת שבן-גוריון, יריבו הפוליטי של ויצמן, מנע ממנו לחתום על המגילה, שחתימתה נעשתה בזמן בו שהה ויצמן בארצות הברית. בן-גוריון לא אפשר לו לחתום בנימוק שרק חברי מועצת העם חתמו עליה. מרדכי נאור ומיכאל לוין טוענים שוויצמן כלל לא היה אמור לחתום על המגילה, שכן מלבד העובדה שהוא לא היה חבר מועצת העם, הוא גם לא היה נשיא בשעת ההכרזה והרווח נותר בשביל החברים תושבי ירושלים הנצורה באותה עת.\n",
      "Entry 2, Paragraph 1: במהלך ההיסטוריה, ואף כיום, מתקיימות חברות רבות ללא מדינה. המדינות הראשונות בהיסטוריה האנושית התקיימו לפני כ-6000 שנה באזור המזרח התיכון, במסופוטמיה. בתקופה שלאחר נפילתה של האימפריה הרומית במאה החמישית לספירה, אירופה החלה תהליך של התחלקות לנסיכויות רבות, שריבונותיהן וגבולותיהן השתנו בתכיפות. ממצב זה של אי סדר פוליטי, התפתחה בתהליך הדרגתי שנמשך לאורך מאות שנים, מדינת הלאום המודרנית, המורכבת מאנשים בעלי אזרחות דומה החיים בטריטוריה מוגדרת. סוג השלטון השתנה, בהתחלה קמו שלטונות מונרכיים לחלוטין ולאחר מכן קמו מונרכיות קונסטיטוציוניות (חוקתיות) או רפובליקות, כמה מהן פדרציות, כלומר איחוד של מדינות במטרה לתת לשלטון המרכזי סמכויות בתחום הביטחון, מדיניות החוץ, האוצר והחוקה. בנוסף, קמו איחודים של מדינות חצי עצמאיות.\n",
      "Entry 3, Paragraph 1: הממשלה הזמנית השיגה את הסכמת רוב \"מועצות החיילים והפועלים\" לקיים בחירות דמוקרטיות כדי לכנס אספה לאומית. במקביל דיכא אברט באמצעות חיילי הצבא ומיליציות הפרייקור את \"מרד הספרטקיסטים\" וכן מרידות שמאל בכל גרמניה, ובמיוחד במינכן, שם הוכרזה רפובליקה סוציאליסטית עוד בטרם התפטר הקייזר. דיכוי מרידות השמאל היה אכזרי, וגרם למאות קורבנות בנפש. בין היתר נרצחו מנהיגי השמאל רוזה לוקסמבורג וקרל ליבקנכט. היה זה שיתוף פעולה יחיד במינו בין הצבא ויסודות הימין הקיצוני האנטי-דמוקרטי, ובין הממשלה הסוציאל-דמוקרטית. מאורעות העתיד יראו כי שיתוף פעולה זה נמשך רק כל עוד הדם שנשפך הוא דמם של תומכי השמאל. רבים טוענים כי בהסכימו לדיכוי המרידות באופן בו בוצע, חנק אברט את הדמוקרטיה הצעירה כבר בראשיתה.\n",
      "Entry 4, Paragraph 1: ב-1 באפריל 2012 חנכה ההתאחדות לכדורגל קומפלקס אימונים חדש בשם \"בית הנבחרות\" בקיבוץ שפיים שנבנה בהשקעה של 22 מיליון ש\"ח על שטח של 30 דונם ליד מלון קיבוץ שפיים. המתחם כולל שני מגרשי אימונים (אחד דשא טבעי ובשני דשא סינתטי) עם 300 מקומות ישיבה ומבנה מרכזי הכולל 4 חדרי הלבשה ומלתחות, 2 חדרי טיפולים, מרפאה, חדרי ציוד, אודיטוריום קטן להרצאות ומסיבות עיתונאים, לובי וקפיטריה. המתחם משמש את נבחרת ישראל לצד נבחרות הנוער והנשים ואף אירח משחקים שאינם דורשים נוכחות קהל כגון משחקי אימון ומשחקי ידידות של נבחרות נוער.\n",
      "Entry 5, Paragraph 1: מבקרי הליברטריאניזם משמאל ומימין כאחד טוענים שהרעיונות הליברטריאניים אודות חירותו החברתית והכלכלית של היחיד מתאפיינים בסתירות פנימיות, בלתי-עמידים לביקורת ולא ראויים. המבקרים משמאל, מסבירים כי הליברטריאניזם מתעלם ממהותו החברתית של האדם ומאופיים החברתי של אמונותיו, צרכיו, שאיפותיו וכישוריו ומתעלם מכוח הכפייה האדיר המגולם בהון, ומן התלות של ההון במנגנון המדינה המגן עליו ונותן לו תוקף (אם צריך – גם באמצעים אלימים). בנוסף, הם טוענים ששווקים חופשיים ללא מגבלות (או קפיטליזם בסגנון לסה פר) פוגעים בחירותם האישית של רבים על ידי יצירת אי-שוויון חברתי ועוני, וגם בגלל שחרורם של בעלי העוצמה מאחריות כלפי הזולת. מבקרי הליברטריאנים מימין נוטים להתמקד בסוגיות המסורת והמוסר האישי וטוענים שהחרויות הרבות שמקודמות על ידי הליברטריאנים מעודדות התנהגות בלתי-בריאה או לא מוסרית וחותרות תחת הדת. ליברטריאנים הנותנים דעתם על ביקורות אלה טוענים, שאחריות אישית, צדקה פרטית וחליפין-מרצון של סחורות ורעיונות הם הביטוים החברתיים שעולים בקנה אחד עם הגישה האינדיבידואליסטית לחירות ומספקים את האמצעים היעילים והמוסריים ביותר להשגת שלום חברתי. הם גם חושבים, שבחברה קפיטליסטית אמיתית אפילו לעניים ביותר ייטב בגלל צמיחה כללית מהירה יותר של הכלכלה, שתושג על ידי הורדת מיסים ורגולציה מופחתת.\n",
      "Entry 5, Paragraph 2: מנגד, חלק מהליברטריאנים רואים את האובייקטיביזם כמשנה דוגמטית, לא-מציאותית ובלתי-מתפשרת. לדברי עורכו של המגזין הליברטריאני \"Reason\" בגיליון מרץ 2005 שהוקדש להשפעת האובייקטיביזם, איין ראנד \"היא אחת מן הדמויות החשובות בתנועה הליברטריאנית... ראנד עודנה אחת מ[המחברים] הנמכרים ביותר ומהדמויות המשפיעות ביותר במחשבה ובתרבות האמריקאית\" בכלל ובליברטריאניזם בפרט. בכל זאת, הוא מודה שהוא נבוך מקישור המגזין שלו עם רעיונותיה. באותו גיליון קטי יאנג כותבת ש\"הליברטריאניזם, התנועה הקרובה ביותר לרעיונותיה של ראנד, היא פחות צאצאית שלהם ויותר בת חורגת מרדנית\". אף כי הם דוחים את מה שהם מגדירים כדוגמות של ראנד, ליברטריאנים כמו יאנג עדיין סבורים ש\"מסר החירות והתבונה של ראנד... יכול להיות נקודת אחיזה\" עבור הליברטריאנים.\n",
      "Entry 6, Paragraph 1: החל משלהי המאה ה-15 ניתן למצוא תיאורים שליליים של בני רומה מפי יושבי הקבע, והם תוארו לעיתים כגנבים, רמאים, בורים, בטלנים, עובדי אלילים, מזוהמים, מגלי עריות, גנבי ילדים ועוד. גילויי העוינות כלפי בני רומה מצד האוכלוסייה המקומית ומצד השלטונות הלכו וגברו, עד כדי מתן צווי גירוש. צו הגירוש הראשון הוצא על ידי הרייכסטאג הקיסרי הגרמני בשנת 1497. שנים ספורות לאחר מכן ניתנו צווים דומים בספרד, בפורטוגל, בערי המדינה של איטליה ובצרפת. באנגליה צו הגירוש הראשון הוצא ב-1530, ובסביבות השנים האלה הוצאו צווים דומים באירלנד, בהולנד, ובארצות סקנדינביה. העונשים שניתנו למפירי צווי הגירוש, גברים ונשים כאחד, כללו גילוח שיער הראש, מלקות, כריתת אוזן אחת או שתיים, מאסר עם עבודת פרך וכיוצא בזאת. בני רומה שנתפסו הוגלו לעיתים לקולוניות.\n",
      "Entry 6, Paragraph 2: פעמים רבות, התיאורים באומנות של הצוענים, עושים רומנטיזציה לנראטיב הצועני לגבי יכולתם לנבא עתידות ואופיים העצבני והתשוקתי יחד עם אהבתם לחופש שלא ניתנת לביות והרגליהם הקרימינליים. יצירות ידועות שמייצגות גישה זו הן \"כרמן\" בנובלה של פרוספר מרימה ועיבוד היצירה באופרה של ג'ורג' ביזה. \"הגיבן מנוטרדאם\" של ויקטור הוגו וכן La Gitanilla של מיגל דה סרוונטס מייצגים אף הם גישה זו. ביצירתו של הוגו בא לידי ביטוי גם החשש ממעשי הכישוף אשר יכלו כביכול לעשות הצוענים. רומנטיזציה משמעותית עברו הצוענים גם באמנות בברית המועצות. דוגמה קלאסית לכך היא הסרט \"מלכת הצוענים\" משנת 1975. תיאור אקטואלי וריאליסטי יותר של צוענים מהבלקן, ניתן לראות בסרטיו של אמיר קוסטריצה כאמור, \"חתול שחור חתול לבן\" וכן בסרטו \"שעת הצוענים\". בסרטים אלו נשמר הדיאלקט המקורי של הצוענים אך חזרו לקלישאות הצועניות הידועות של צוענים אשר עוסקים בכישוף ופשע. תיאור אותנטי נוסף של הצוענים ביוגוסלביה ניתן לראות בסרט משנת 1967 \"פגשתי צוענים מאושרים\". בשנת 2007, עם עליית הטלנובלה \"כמעט מלאכים\" מאת כריס מורנה, הוצגה לראשונה דמותה של חסמין רומרו, בגילומה של אאוחניה סוארס, נערה צעירה בעלת שורשים צועניים המתבטאים בכישרונה בריקוד הפלמנקו וריקודים צועניים אחרים וכן בחיזיונות על העתיד. במסגרת פס-קול העונה הראשונה של הסדרה, קיבלה חסמין שיר סולו בשם \"מלכה צוענייה\" שכולו בסגנון פלמנקו.\n",
      "Entry 7, Paragraph 1: בראש ההיררכיה האינטרנטית נמצאים שרתים המכונים \"רמת השורש\" (root servers). שרתים אלה מסוגלים להפנות את המבקש לשרתים הרלוונטיים עבור כל כתובת. השרתים ברמה הבאה אחראיים על שמות תחום מהרמה העליונה (Top level domains), כלומר על כל שמות התחום אשר משתמשים בסיומת אינטרנט מסוימת. למשל, שאילתה לשרת מרמת השורש לגבי www.mywebsite.co.il תופנה לשרת אשר אחראי על כלל שמות התחום הישראליים. שרת זה יוכל להפנות לשרת ברמה השנייה – www.mywebsite.co.il, כלומר השרת שאחראי על כתובות של אתרים מסחריים בישראל. שרת זה יטפל בחלק השלישי של הכתובת – www.mywebsite.co.il ויפנה את הבקשה לשרת המארח (Host) של mywebsite, שיבדוק וימצא שהמשתמש מעוניין בכתובת ה-IP של שרת האינטרנט (www.mywebsite.co.il) ויספק את הכתובת המבוקשת.\n",
      "Entry 7, Paragraph 2: DNS היא שיטת רישום שמות היררכית. לכל שרת יש רשומות משאבים (resource records) שמאפשרות לו לטפל בבקשות לגבי תחום שמות מסוים. רשומות אלה מחלקות לאזורים (zones) את התחום עליו אחראי השרת. השרת יודע להפנות את הפונים אליו לשרתים האחראיים על אזורים אלו.\n",
      "תהליך הבירור לגבי שם תחום מסוים נעשה על ידי פנייה לשרת וקבלת הפנייה לשרת אשר ככל הנראה יוכל לספק את התשובה, או לפחות לקרב אותנו לשרת אשר ידע להשיב ובו התהליך יחזור על עצמו. התהליך מסתיים כאשר מתקבלת תשובה מדויקת משרת אשר אחראי לשם התחום המבוקש, או משרת אשר אינו אחראי אבל זיכרון המטמון שלו מכיל את התשובה. תהליך בירור זה הוא תהליך איטרטיבי, ומתבצע בעיקר על ידי שרתים ששייכים לספקי אינטרנט. מחשבים אישיים בדרך כלל לא ידעו לבצע אותו, אלא רק לפנות לשרת של ספק האינטרנט על מנת שיבצע אותו עבורם. פנייה כזו נקראת פנייה רקורסיבית.\n",
      "Entry 8, Paragraph 1: המלט מגיע לדנמרק בדיוק לבית הקברות של אלסינור, בו שני חופרי קברים, מכינים את הקבר של אופליה. הם מתווכחים על אופי מותה של הנקברת-למרות שהיא מקבלת קבורה נוצרית, הם חושבים שהיא התאבדה, וקיבלה קבורה נוצרית, כיוון שהיא קרובה למשפחת המלוכה. כשמתחילה הלוויה המלט והורציו מתחבאים, ורואים את תהלוכת הלוויה של אופליה, שמובלת על ידי אחיה האבל לארטס. המלט מבין שזו הלוויה של אופליה, וקופץ ממקום מחבואו אל הקבר הפתוח, ומתוודה על אהבתו לאופליה תוך בכי. לארטס קופץ לתוך הקבר אחריו, ומאשים את המלט כאחראי למותה. המלט טוען לאהבתו ואבלו על אופליה. הוא ולארטס מתחילים לריב, ומופרדים על ידי קלאודיוס וגרטרוד. קלאודיוס מזכיר ללארטס על התוכנית לתחרות סיף. מאוחר יותר באותו יום, המלט מספר להורציו איך הוא ברח ושרוזנקרנץ וגילדנשטרן נשלחו למותם. החצרן אוסריק מפריע כדי להזמין את המלט להתחרות בסיף עם לארטס. למרות אזהרות מהורציו, המלט מסכים והקרב מתחיל. אחרי מס' סיבובים גרטרוד מרימה כוסית לחיי המלט. המלט לא רוצה לשתות, וקלאודיוס מזהיר את גרטרוד לא לשתות, כיוון שזו הכוס עם היין המורעל שהוא הכין עבור המלט. גרטרוד שותה יין מהכוס עם הרעל. בקרב לארטס תוקף ודוקר את המלט עם הלהב המורעל. במהלך הקרב המלט משיג את חרבו המורעלת של לארטס ומשתמש בה נגדו, ופוצע את לארטס. פתאום גרטרוד נופלת ובעודה גוססת היא מודיעה שהיא הורעלה.\n",
      "Entry 9, Paragraph 1: בארצות הברית שכיחות ההפרעה עומדת על כ-7%. השיעור החציוני במדינות אירופה עומד על 2.3%. השכיחות בילדים ומתבגרים דומה לזו של מבוגרים. השכיחות של חרדה חברתית בקרב זקנים נעה בין 2% ל-5% בקרב אזרחי ארצות הברית. הבדלים בין המינים באים לידי ביטוי בעיקר בילדים ובני נוער, כאשר בנים נוטים יותר מבנות לפתח את ההפרעה. בארצות הברית שיעור החרדה החברתית גבוה יותר בקרב ילידים אינדיאניים ונמוך יותר בקרב יוצאי אסיה, אמריקה הלטינית, אפרו-אמריקנים, ויוצאי הקריביים. בשל מגפת הקורונה חלה עלייה משמעותית באחוזי הסובלים מחרדה זו בקרב בני הנוער.\n",
      "Entry 10, Paragraph 1: מופע הגמר של העונה נערך ב-7 בספטמבר, בפארק הירקון בתל אביב, כחלק ממופע \"ווליום תל אביב\", בהשתתפות ג'קו אייזנברג, מאיה רוטמן ורפאל מירילא. אייזנברג ניצח, עם כ-46% מהקולות, במקום השני זכתה רוטמן עם כ-28% מהקולות ובמקום השלישי זכה מירילא עם כ-26% מהקולות. בריאיון שנתן אייזנברג זמן קצר לאחר זכייתו, הוא עורר סערה תקשורתית, לאחר שסיפר, בין השאר, שהוא לא התגייס לצה\"ל ולא מתחרט על כך. הראיון הוביל לתגובות חריפות בציבור, אשר הובילו את אייזנברג להתנצל בפומבי, ולפסק זמן בן מספר חודשים מהופעות בתקשורת, ובשנת 2010 הוא עבר להתגורר באמסטרדם.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL of the train JSON file\n",
    "train_url = \"https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/main/data/train.json\"\n",
    "\n",
    "# Function to load JSON data from the given URL\n",
    "def load_json_from_github(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return json.loads(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data from {url}\")\n",
    "        return None\n",
    "\n",
    "# Load train data\n",
    "train_data = load_json_from_github(train_url)\n",
    "\n",
    "for i, entry in enumerate(train_data['data'][:10]):\n",
    "    for j, paragraph in enumerate(entry['paragraphs']):\n",
    "        print(f\"Entry {i + 1}, Paragraph {j + 1}: {paragraph['context']}\")\n",
    "        questions = [question in question for paragraph['qas'] if \n",
    "\n",
    "# # Print the first entry to check the data\n",
    "# if train_data:\n",
    "#     print(\"First example in train data:\")\n",
    "#     print(json.dumps(train_data['data'][0], indent=4, ensure_ascii=False))\n",
    "# else:\n",
    "#     print(\"No data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd01f101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'איזו תוכנית הוכרזה כחלק מההצעה של הטיוטה?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.choice([q['question'] for q in train_data['data'][0]['paragraphs'][0]['qas'] if q['answers']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a50e4383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "לאחר שהטיוטות הראשוניות הובאו למנהלת העם וספגו ביקורת חריפה, הוקמה ועדת חמישה בראשות משה שרת שניסחה טיוטה רביעית. ב-13 במאי לפנות ערב הוגשה הצעתה למנהלת העם. שרת עשה את רוב המלאכה, תוך התייעצות עם משפטנים מומחים. נוסח ההכרזה הושאל מכתב המנדט וממסמכים משפטיים רבים אחרים, וכל סעיף שלו התחיל במילים \"הואיל ו...\". בהצעה זו הוזכרה גם תוכנית החלוקה של האו\"ם. מזכיר מנהלת העם כתב כי \"בן-גוריון התנגד ל'הואיל' כי אינו עברי\" וכן \"התנגד למילים 'ישוב רב־איל', 'עוז וגבורה' וכיוצא באלה\". כמו כן, התנגד להזכרה מפורשת של תוכנית החלוקה.\n"
     ]
    }
   ],
   "source": [
    "print(train_data['data'][0]['paragraphs'][0]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a91dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
