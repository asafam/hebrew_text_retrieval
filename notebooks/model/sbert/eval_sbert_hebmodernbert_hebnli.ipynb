{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d1c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 390\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import InputExample\n",
    "\n",
    "# Load both train and validation splits\n",
    "hebnli = load_dataset('HebArabNlpProject/HebNLI')\n",
    "\n",
    "ENTAILMENT = \"entailment\"\n",
    "CONTRADICTION = \"contradiction\"\n",
    "PREMISE = \"translation1\"\n",
    "HYPOTHESIS = \"translation2\"\n",
    "LABEL = \"original_label\"\n",
    "\n",
    "# Use only 'entailment' pairs (label==2) for MultipleNegativesRankingLoss\n",
    "def make_examples(dataset, include_negative=False):\n",
    "    examples = [\n",
    "        InputExample(texts=[item[PREMISE], item[HYPOTHESIS]], label=1.0)\n",
    "        for item in dataset\n",
    "        if item[LABEL] == ENTAILMENT and item[PREMISE] and item[HYPOTHESIS]\n",
    "    ]\n",
    "    if include_negative:\n",
    "        examples += [\n",
    "            InputExample(texts=[item[HYPOTHESIS], item[PREMISE]], label=0.0)\n",
    "            for item in dataset\n",
    "            if item[LABEL] == CONTRADICTION and item[PREMISE] and item[HYPOTHESIS]\n",
    "        ]\n",
    "    return examples\n",
    "\n",
    "test_examples = make_examples(hebnli['test'], include_negative=True)\n",
    "\n",
    "print(f\"Train: {len(test_examples)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e65194da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_path = \"/home/nlp/achimoa/workspace/hebrew_text_retrieval/outputs/models/sbert/sbert-hebmodernbert-hebnli/ckpt_20250522_1841_ep1-ba136000\"\n",
    "# model_path = \"/home/nlp/achimoa/workspace/hebrew_text_retrieval/outputs/models/sbert/sbert-hebmodernbert-hebnli/ckpt_20250603_1331_ep4-ba628000\"\n",
    "model = SentenceTransformer(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dda516b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1 = [ex.texts[0] for ex in test_examples]\n",
    "sents2 = [ex.texts[1] for ex in test_examples]\n",
    "labels = [ex.label for ex in test_examples]  # 1.0 for entailment, 0.0 for contradiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06eede69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77256efa3be54c23805b7e35a350e9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2f261a1e1c44d2bd6f4aa65cb84096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs1 = model.encode(sents1, batch_size=32, convert_to_numpy=True, show_progress_bar=True)\n",
    "embs2 = model.encode(sents2, batch_size=32, convert_to_numpy=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "086a9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "cos_scores = np.array([cosine_similarity([a], [b])[0][0] for a, b in zip(embs1, embs2)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3a30d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "pred_labels = (cos_scores > threshold).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f88fad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6512820512820513\n",
      "F1 Score: 0.7094017094017094\n",
      "Macro F1 Score: 0.6367521367521367\n",
      "ROC-AUC: 0.7545760572270146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.746     0.454     0.564       194\n",
      "         1.0      0.610     0.847     0.709       196\n",
      "\n",
      "    accuracy                          0.651       390\n",
      "   macro avg      0.678     0.650     0.637       390\n",
      "weighted avg      0.678     0.651     0.637       390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(labels, pred_labels))\n",
    "print(\"F1 Score:\", f1_score(labels, pred_labels))\n",
    "print(\"Macro F1 Score:\", f1_score(labels, pred_labels, average='macro'))\n",
    "print(\"ROC-AUC:\", roc_auc_score(labels, cos_scores))\n",
    "print(classification_report(labels, pred_labels, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef616bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 threshold: 0.54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "thresholds = np.arange(0.3, 0.8, 0.01)\n",
    "scores = [f1_score(labels, (cos_scores > t).astype(float)) for t in thresholds]\n",
    "best_thresh = thresholds[np.argmax(scores)]\n",
    "print(f\"Best F1 threshold: {best_thresh:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdda0841",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = (cos_scores > best_thresh).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c12c5ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6846153846153846\n",
      "F1 Score: [0.63501484 0.72234763]\n",
      "Macro F1 Score: 0.678681233296046\n",
      "ROC-AUC: 0.7545760572270146\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.748     0.552     0.635       194\n",
      "         1.0      0.648     0.816     0.722       196\n",
      "\n",
      "    accuracy                          0.685       390\n",
      "   macro avg      0.698     0.684     0.679       390\n",
      "weighted avg      0.698     0.685     0.679       390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(labels, pred_labels))\n",
    "print(\"F1 Score:\", f1_score(labels, pred_labels, average=None))\n",
    "print(\"Macro F1 Score:\", f1_score(labels, pred_labels, average='macro'))\n",
    "print(\"ROC-AUC:\", roc_auc_score(labels, cos_scores))\n",
    "print(classification_report(labels, pred_labels, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adc02d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[107  87]\n",
      " [ 36 160]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing 10 random validation examples:\n",
      "Premise:    המנהלים לא מכירים אף אחד אחר.\n",
      "Hypothesis: במסגרת הסדרים כאלה, מנהלים מחברות שונות שמכירים זה את זה ישבו בדירקטוריונים אחד של השני.\n",
      "Gold label: 0.0, Cosine: 0.375, Predicted: 0.0\n",
      "\n",
      "Premise:    יעדי אחריות רק עוברים ביקורת\n",
      "Hypothesis: כדי להבטיח שמקבלי ההחלטות יקבלו מידע שימושי, רלוונטי, אמין, ומתקבל בזמן הנכון, ארגוני מימון מובילים קובעים יעדי אחריות שמשתרעים הרבה מעבר לקבלת ביקורת בלתי מוגבלת.\n",
      "Gold label: 0.0, Cosine: 0.429, Predicted: 0.0\n",
      "\n",
      "Premise:    היה סוכר אבל לא היו עבדים.\n",
      "Hypothesis: סוכר ועבדים\n",
      "Gold label: 0.0, Cosine: 0.549, Predicted: 1.0\n",
      "\n",
      "Premise:    לא הייתה להקה שניגנה על הבמה לאחר שהכריחו את הלהקה האחרת לרדת מהבמה.\n",
      "Hypothesis: על הבמה, להקה אחרת התחילה לנגן.\n",
      "Gold label: 0.0, Cosine: 0.691, Predicted: 1.0\n",
      "\n",
      "Premise:    נתתי הרצאה על הכלכלה האמריקאית בתל אביב, ישראל, בחודש שעבר.\n",
      "Hypothesis: בחודש שעבר, בזמן שהייתי בישראל, דיברתי על הכלכלה האמריקאית.\n",
      "Gold label: 1.0, Cosine: 0.788, Predicted: 1.0\n",
      "\n",
      "Premise:    מבקרים אחרים מציבים את עבודתו של פייגלס באותו מעמד עם התרגומים הקלאסיים של פיצג'רלד ושל לאטימור.\n",
      "Hypothesis: מבקרים אחדים מציבים את עבודתו של פייגלס באותו מעמד של עבודותיהם של פיצג'רלד ולאטימור.\n",
      "Gold label: 1.0, Cosine: 0.806, Predicted: 1.0\n",
      "\n",
      "Premise:    אף אחת מהפלאים הגיאולוגיים של העולם אינן נמצאות בקרבת המנזר.\n",
      "Hypothesis: מערבית למנזר נמצא אחד מפלאי הטבע של העולם.\n",
      "Gold label: 0.0, Cosine: 0.617, Predicted: 1.0\n",
      "\n",
      "Premise:    הם לא התלהבו מהרכישה של המכונית הראשונה שלהם, מזראטי.\n",
      "Hypothesis: הייתי כל כך נרגשת, אמרה האלמנה והדיירת לשעבר בדופלקס על רכישת ביתה הראשון, מבנה מסגרת שוקע בסנודן.\n",
      "Gold label: 0.0, Cosine: 0.212, Predicted: 0.0\n",
      "\n",
      "Premise:    הניו יורק טיימס, לעומת זאת, ממשיך בקו הרביזיוניסטי.\n",
      "Hypothesis: הניו יורק טיימס ממשיך בנקיטת קו רביזיוניסטי.\n",
      "Gold label: 1.0, Cosine: 0.912, Predicted: 1.0\n",
      "\n",
      "Premise:    אין סדר בעבודה שנעשית בכוורת.\n",
      "Hypothesis: חלק מהפועלים שומרים על כניסת הכוורת, אחרים אוספים צוף, אחרים מייבשים צוף על ידי ניעור כנפיהם, אחרים מטפחים את המלכה, אחרים מייצרים משחה אנטיספטית שמצפה את הכוורת וכו' ...\n",
      "Gold label: 0.0, Cosine: 0.442, Predicted: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "indices = list(range(len(sents1)))\n",
    "random.shuffle(indices)\n",
    "sample_size = 10\n",
    "\n",
    "print(\"Showing 10 random validation examples:\")\n",
    "for i in indices[:sample_size]:\n",
    "    print(f\"Premise:    {sents1[i]}\")\n",
    "    print(f\"Hypothesis: {sents2[i]}\")\n",
    "    print(f\"Gold label: {labels[i]}, Cosine: {cos_scores[i]:.3f}, Predicted: {pred_labels[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faa20eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1.0: 196, 0.0: 194})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a87303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
