{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2f578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_path = \"/home/nlp/achimoa/workspace/hebrew_text_retrieval\"\n",
    "src_path = os.path.join(project_path, \"src\")\n",
    "\n",
    "os.chdir(project_path)\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9694e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, Trainer, TrainingArguments, AutoConfig, PreTrainedModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from src.data.heq.heq_data import HeQDatasetBuilder, HeQTaskName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce31c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/refs/heads/main/data/data%20v1.1/train%20v1.1.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c912756505914dd593aff3c991f559e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/4462 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://raw.githubusercontent.com/NNLP-IL/Hebrew-Question-Answering-Dataset/refs/heads/main/data/data%20v1.1/val%20v1.1.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c4038f77634d5f96ece9066ad7b04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'index', 'paragraph_index', 'question_id', 'question', 'answer', 'context'],\n",
       "        num_rows: 3198\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'index', 'paragraph_index', 'question_id', 'question', 'answer', 'context'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heq_dataset_builder = HeQDatasetBuilder(task=HeQTaskName.QUESTION_DOC, decorate_with_task_tokens=False)\n",
    "heq_dataset = heq_dataset_builder.build_dataset(filter_empty_answers=True, splits=['train', 'validation'])\n",
    "heq_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5a1cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor_text': 'מה אכלו הרשב\"י ובנו במערה?',\n",
       " 'positive_text': 'יהודה בן גרים סיפר על דבריו, ובעקבות זאת נדון ר\\' שמעון למיתה על ידי השלטון הרומאי, ונאלץ לרדת למחתרת. על פי המסורת, התחבאו רשב\"י ובנו רבי אלעזר 12 שנים במערה בפקיעין, וניזונו מעץ חרוב וממעין מים שנבראו להם בדרך נס. כל אותן 12 שנה היו שניהם לומדים תורה כאשר כל גופם מכוסה חול עד צווארם, ורק בזמן התפילה יצאו מהחול והתלבשו. לאחר 12 שנה הגיע אליהו הנביא למערה והודיע לרשב\"י כי קיסר רומא מת וגזרותיו בוטלו. אז יצאו רשב\"י ואלעזר בנו ממקום מחבואם, אך כשראה רשב\"י בעולם אנשים מתבטלים מתלמוד תורה ועוסקים בחרישה ובזריעה נתן בהם עיניו ונשרפו. אז יצאה בת קול מן השמים ואמרה \"להחריב עולמי יצאתם? חזרו למערתכם!\". חזרו רשב\"י ואלעזר בנו למערה לעוד 12 חודשים, שבסופם יצאו מהמערה ופגשו אדם המביא לכבוד שבת שני הדסים, וכך ראו כמה חביבות מצוות על ישראל ונתקררה דעתם.',\n",
       " 'index': 0,\n",
       " 'paragraph_index': 0,\n",
       " 'question_id': '425478ad-1fb3-4a1a-a100-230cc56e2ccf',\n",
       " 'question': 'מה אכלו הרשב\"י ובנו במערה?',\n",
       " 'answer': 'חרוב',\n",
       " 'context': 'יהודה בן גרים סיפר על דבריו, ובעקבות זאת נדון ר\\' שמעון למיתה על ידי השלטון הרומאי, ונאלץ לרדת למחתרת. על פי המסורת, התחבאו רשב\"י ובנו רבי אלעזר 12 שנים במערה בפקיעין, וניזונו מעץ חרוב וממעין מים שנבראו להם בדרך נס. כל אותן 12 שנה היו שניהם לומדים תורה כאשר כל גופם מכוסה חול עד צווארם, ורק בזמן התפילה יצאו מהחול והתלבשו. לאחר 12 שנה הגיע אליהו הנביא למערה והודיע לרשב\"י כי קיסר רומא מת וגזרותיו בוטלו. אז יצאו רשב\"י ואלעזר בנו ממקום מחבואם, אך כשראה רשב\"י בעולם אנשים מתבטלים מתלמוד תורה ועוסקים בחרישה ובזריעה נתן בהם עיניו ונשרפו. אז יצאה בת קול מן השמים ואמרה \"להחריב עולמי יצאתם? חזרו למערתכם!\". חזרו רשב\"י ואלעזר בנו למערה לעוד 12 חודשים, שבסופם יצאו מהמערה ופגשו אדם המביא לכבוד שבת שני הדסים, וכך ראו כמה חביבות מצוות על ישראל ונתקררה דעתם.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heq_dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c0f4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model_name = \"answerdotai/ModernBERT-base\"\n",
    "doc_model_name = \"answerdotai/ModernBERT-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad56a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9c9b4fb8c144777b6c3336ac9143dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3198 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2b799f97b94b6ab503b4ba5275d8e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/168 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_q = AutoTokenizer.from_pretrained(query_model_name)\n",
    "tokenizer_d = AutoTokenizer.from_pretrained(doc_model_name)\n",
    "\n",
    "def preprocess(\n",
    "        example, \n",
    "        query='question', \n",
    "        paragraph='context', \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=1024\n",
    "    ):\n",
    "    q = tokenizer_q(\n",
    "        example[query], truncation=truncation, padding=padding, max_length=max_length\n",
    "    )\n",
    "    d = tokenizer_d(\n",
    "        example[paragraph], truncation=truncation, padding=padding, max_length=max_length\n",
    "    )\n",
    "    return {\n",
    "        \"q_input_ids\": q['input_ids'],\n",
    "        \"q_attention_mask\": q['attention_mask'],\n",
    "        \"d_input_ids\": d['input_ids'],\n",
    "        \"d_attention_mask\": d['attention_mask'],\n",
    "    }\n",
    "\n",
    "processed = heq_dataset.map(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702fae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"q_input_ids\": torch.tensor([item[\"q_input_ids\"] for item in batch]),\n",
    "        \"q_attention_mask\": torch.tensor([item[\"q_attention_mask\"] for item in batch]),\n",
    "        \"d_input_ids\": torch.tensor([item[\"d_input_ids\"] for item in batch]),\n",
    "        \"d_attention_mask\": torch.tensor([item[\"d_attention_mask\"] for item in batch]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72f1cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: /home/nlp/achimoa/workspace/ModernBERT/hf/HebrewModernBERT/ModernBERT-Hebrew-base_20250522_1841\n",
      "Loaded tokenizer: /home/nlp/achimoa/workspace/ModernBERT/hf/HebrewModernBERT/ModernBERT-Hebrew-base_20250522_1841\n",
      "Inputs: {'input_ids': tensor([[   2, 8273,    3]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "NaN in output? False\n",
      "BaseModelOutput(last_hidden_state=tensor([[[ 4.7266e-02,  2.5687e-02,  1.0403e-01,  ...,  3.0027e-02,\n",
      "          -2.2502e-02,  1.5941e-02],\n",
      "         [-2.2235e-01,  3.2447e-01, -3.3287e-01,  ...,  4.3003e-01,\n",
      "           8.0599e-02,  2.6916e-01],\n",
      "         [-8.0712e-02,  4.8851e-02, -1.0924e-02,  ..., -1.1590e-01,\n",
      "          -4.3202e-05,  4.0384e-02]]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import ModernBertForMaskedLM, AutoModelForMaskedLM, AutoModel, AutoTokenizer  # adjust import path if needed\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"/home/nlp/achimoa/workspace/ModernBERT/hf/HebrewModernBERT/ModernBERT-Hebrew-base_20250522_1841\"\n",
    "model = AutoModel.from_pretrained(model_name_or_path)\n",
    "print(\"Loaded model:\", model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "print(\"Loaded tokenizer:\", model_name_or_path)\n",
    "inputs = tokenizer(\"בדיקה\", return_tensors=\"pt\")\n",
    "print(\"Inputs:\", inputs)\n",
    "with torch.no_grad():\n",
    "    out = model(**inputs)\n",
    "    print(\"NaN in output?\", torch.isnan(out[0]).any().item())\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InfoNCEDualEncoder(PreTrainedModel):\n",
    "    def __init__(self, config, query_model_name, doc_model_name=None, pooling='cls'):\n",
    "        super().__init__(config)\n",
    "        self.query_encoder = AutoModel.from_pretrained(query_model_name, config=config)\n",
    "        if doc_model_name:\n",
    "            self.doc_encoder = AutoModel.from_pretrained(doc_model_name, config=config)\n",
    "        else:\n",
    "            self.doc_encoder = AutoModel.from_pretrained(query_model_name, config=config)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def encode(self, encoder, input_ids, attention_mask):\n",
    "        output = encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # [CLS] pooling or mean pooling\n",
    "        if self.pooling == 'cls':\n",
    "            return output.last_hidden_state[:, 0]  # [batch, hidden]\n",
    "        elif self.pooling == 'mean':\n",
    "            mask = attention_mask.unsqueeze(-1).expand(output.last_hidden_state.size())\n",
    "            sum_emb = torch.sum(output.last_hidden_state * mask, 1)\n",
    "            sum_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "            return sum_emb / sum_mask\n",
    "        else:\n",
    "            raise ValueError(\"Unknown pooling type\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q_input_ids,\n",
    "        q_attention_mask,\n",
    "        d_input_ids,\n",
    "        d_attention_mask,\n",
    "        labels=None  # not used\n",
    "    ):\n",
    "        # [batch, hidden]\n",
    "        q_emb = self.encode(self.query_encoder, q_input_ids, q_attention_mask)\n",
    "        d_emb = self.encode(self.doc_encoder, d_input_ids, d_attention_mask)\n",
    "\n",
    "        # [batch, batch] similarity matrix\n",
    "        sim_matrix = torch.matmul(q_emb, d_emb.T)  # dot product, or use F.cosine_similarity\n",
    "\n",
    "        # InfoNCE loss\n",
    "        targets = torch.arange(sim_matrix.size(0), device=sim_matrix.device)\n",
    "        loss = F.cross_entropy(sim_matrix, targets)\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": sim_matrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae2c154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/800 03:26 < 15:08, 0.72 it/s, Epoch 0.37/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.153000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(query_model_name)\n",
    "model = InfoNCEDualEncoder(config, query_model_name, doc_model_name, pooling='cls')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs/dual_encoder/dual_encoder_infonce_heq\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed['train'],\n",
    "    eval_dataset=processed['validation'],\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "654795df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'index', 'paragraph_index', 'question_id', 'question', 'answer', 'context', 'q_input_ids', 'q_attention_mask', 'a_input_ids', 'a_attention_mask'],\n",
       "        num_rows: 3198\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['anchor_text', 'positive_text', 'index', 'paragraph_index', 'question_id', 'question', 'answer', 'context', 'q_input_ids', 'q_attention_mask', 'a_input_ids', 'a_attention_mask'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b55e868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
