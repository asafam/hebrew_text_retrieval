{"cells":[{"cell_type":"markdown","id":"2584a5c5","metadata":{"id":"2584a5c5"},"source":["# Training Hebrew Text Encoder"]},{"cell_type":"code","execution_count":2,"id":"fdd4a3f3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":473,"status":"ok","timestamp":1727793285241,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"fdd4a3f3","outputId":"528309b4-d8d8-4572-f916-c3d5f2f7ac6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Wed Oct  2 08:33:00 2024       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:0E:00.0 Off |                    0 |\n","| N/A   31C    P0             67W /  400W |    1210MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   1  NVIDIA A100-SXM4-80GB          Off |   00000000:13:00.0 Off |                    0 |\n","| N/A   30C    P0             60W /  400W |       5MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   2  NVIDIA A100-SXM4-80GB          Off |   00000000:49:00.0 Off |                    0 |\n","| N/A   30C    P0             61W /  400W |       3MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   3  NVIDIA A100-SXM4-80GB          Off |   00000000:4F:00.0 Off |                    0 |\n","| N/A   33C    P0             62W /  400W |       3MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   4  NVIDIA A100-SXM4-80GB          Off |   00000000:91:00.0 Off |                    0 |\n","| N/A   33C    P0             62W /  400W |       3MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   5  NVIDIA A100-SXM4-80GB          Off |   00000000:97:00.0 Off |                    0 |\n","| N/A   30C    P0             62W /  400W |       5MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   6  NVIDIA A100-SXM4-80GB          Off |   00000000:CD:00.0 Off |                    0 |\n","| N/A   32C    P0             63W /  400W |       5MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","|   7  NVIDIA A100-SXM4-80GB          Off |   00000000:D2:00.0 Off |                    0 |\n","| N/A   33C    P0             64W /  400W |       5MiB /  81920MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|    0   N/A  N/A    194643      C   python3.8                                     500MiB |\n","|    0   N/A  N/A   2961972      C   /usr/bin/python3                              690MiB |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"id":"30d5a593","metadata":{},"outputs":[],"source":["import os\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\""]},{"cell_type":"markdown","id":"59ee578c","metadata":{"id":"59ee578c"},"source":["## Setup the environment"]},{"cell_type":"code","execution_count":4,"id":"1335cb4d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23611,"status":"ok","timestamp":1727793309239,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"1335cb4d","outputId":"2b13a175-f20e-46c1-91b6-77f1274b1f8f"},"outputs":[],"source":["!pip install -q -U torch transformers bitsandbytes datasets huggingface_hub accelerate tqdm"]},{"cell_type":"code","execution_count":5,"id":"6594aa8d","metadata":{"executionInfo":{"elapsed":1282,"status":"ok","timestamp":1727793310518,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"6594aa8d"},"outputs":[],"source":["from huggingface_hub import notebook_login\n","import os\n","import sys\n","from datasets import load_dataset"]},{"cell_type":"code","execution_count":6,"id":"aa75b708","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["74f8b298804c43558770b0023e07667d","f860524d831d43c5b38a515cba1d04cf","32d1eec6fad24a77bc386362f2b46712","6877dc049f1f4cfcaa6543722ca86f83","352a9cbea1bf492e97f31b0ea271633e","14db26ed55564ba19047d2bbd86e6845","1ffa46644d9649d79488a67b9807fa17","a1a5e42e0075444d97e6b148647aef75","f629e0e642264e80ba825aba1b0b2a0a","4560b6ae9f7646c0bea921834ca6ceb8","e92da37428de48529f8243fc8d985ab1","198e5d3720554cce85fb2e07748a7502","76ffc23adec545d5a74c297200afaf0a","e09d695bcc134d90a5de448029a576c0","112f57b801e94261a8ec5d5b76c8aca8","70c03e9fb07948b285a587d3ff383bf1","e89d4f3fd687453980a092c7f38e7f07","2d40d2f244a948c5b4e71e08e872c06c","007e143af14b4913b0e9f2295bf1b047","07e97fbb0a584b4fac1095bb57d3f79a","9bfdbe3d159f442593c930f45e46cea6","42fc0328683a41b49bff59ecc7ddf333","4939f27669f84693b3e194d9b9988a50","6331cfbad78b4333b449ebb1cae9768a","4070f80e77974ebcbef6930827281e4d","ece337489ad4425895141c64e9b33f82","cac306aaa83d4aada75976ddc2634181","4116b745dd004517a70887d5fb77c953","98b441436f1a4ff1bb84f51c21a10b59","3a809378f72d479e92f6d8d05573fa7e","a59c458f4b814aa5834a9339f302f3ff","1c4f4b68d5664b11b4381729d9ed50a1"]},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727793310518,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"aa75b708","outputId":"6181309a-9143-4d5d-cf8f-548138803abb"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a99bfe8d3c024c04a20297e39983ffd6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"]},"metadata":{},"output_type":"display_data"}],"source":["os.environ[\"HF_TOKEN\"] = \"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\"\n","notebook_login()"]},{"cell_type":"code","execution_count":7,"id":"zCQcpoxfuz6A","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9146,"status":"ok","timestamp":1727794152997,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"zCQcpoxfuz6A","outputId":"647741ff-6f86-4064-ca49-e3fefb357089"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not running in Google Colab!\n"]}],"source":["def is_running_in_colab():\n","    return 'COLAB_GPU' in os.environ\n","\n","if is_running_in_colab():\n","    # Load the Drive helper and mount\n","    print(\"Mounting google drive...\")\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","else:\n","    print(\"Not running in Google Colab!\")"]},{"cell_type":"code","execution_count":8,"id":"RR6VvegRvAUI","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1247,"status":"ok","timestamp":1727794157861,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"RR6VvegRvAUI","outputId":"4e13bbcb-5afb-4d0b-c950-717b75313b28"},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/nlp/achimoa/projects/hebrew_text_encoder\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":9,"id":"f2734386","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727794157861,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"f2734386","outputId":"9b851440-2930-4c7e-f2c9-ceba5dcf8870"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current working directory set to: /home/nlp/achimoa/projects/hebrew_text_encoder\n"]}],"source":["project_dir = '/home/nlp/achimoa/projects/hebrew_text_encoder'\n","\n","os.chdir(project_dir)\n","print(f\"Current working directory set to: {os.getcwd()}\")\n","\n","\n","if project_dir not in sys.path:\n","    sys.path.insert(0, project_dir)  # Add it to the front of PYTHONPATH\n","    print(f\"PYTHONPATH updated with: {project_dir}\")"]},{"cell_type":"code","execution_count":10,"id":"5ac91230","metadata":{"executionInfo":{"elapsed":376,"status":"ok","timestamp":1727794192179,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"5ac91230"},"outputs":[],"source":["from transformers import AutoModel, AutoTokenizer\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, TensorDataset\n","import logging\n","import os\n","from datasets import DatasetDict, Dataset\n","from tqdm import tqdm\n","import pickle"]},{"cell_type":"markdown","id":"6Kbo83Xuzfd_","metadata":{"id":"6Kbo83Xuzfd_"},"source":["## Generic Helper Code"]},{"cell_type":"code","execution_count":11,"id":"bC4JIY1OHdmR","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1727794159126,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"bC4JIY1OHdmR"},"outputs":[],"source":["def setup_logger(file_path: str):\n","    os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n","    \n","    # Create or retrieve the logger\n","    logger = logging.getLogger('default')\n","\n","    # Remove all existing handlers\n","    if logger.hasHandlers():\n","        for handler in logger.handlers[:]:\n","            logger.removeHandler(handler)\n","\n","    logger.setLevel(logging.DEBUG)\n","    logger.propagate = False\n","\n","    # Stream Handler (for console output)\n","    stream_handler = logging.StreamHandler()\n","    stream_handler.setLevel(logging.DEBUG)\n","    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","    stream_handler.setFormatter(formatter)\n","    logger.addHandler(stream_handler)\n","\n","    # File Handler (for file output)\n","    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n","    file_handler = logging.FileHandler(file_path, delay=False)  # Log file name (you can specify the path)\n","    file_handler.setLevel(logging.DEBUG) # Set the log level for file handler\n","    file_handler.setFormatter(formatter) # Use the same formatter\n","    logger.addHandler(file_handler)\n","\n","    return logger"]},{"cell_type":"code","execution_count":12,"id":"8bad7c38","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1727794160389,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"8bad7c38"},"outputs":[],"source":["def load_checkpoint(model, optimizer, checkpoint_dir, device):\n","    logger = logging.getLogger('default')\n","\n","    latest_checkpoint = None\n","    if os.path.exists(checkpoint_dir):\n","        checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth\")]\n","        if checkpoint_files:\n","            latest_checkpoint = sorted(checkpoint_files)[-1]  # Get the latest checkpoint\n","\n","    if latest_checkpoint:\n","        logger.info(f\"Loading checkpoint {latest_checkpoint}\")\n","        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n","        checkpoint = torch.load(checkpoint_path, map_location=device)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        return checkpoint['epoch']  # return the epoch to resume from\n","\n","    logger.info(\"No checkpoint found. Starting from scratch.\")\n","    return 0  # Start from the first epoch if no checkpoint found\n","\n","\n","# Save model and optimizer state\n","def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n","    logger = logging.getLogger('default')\n","    \n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth\")\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }, checkpoint_path)\n","    logger.info(f\"Checkpoint saved at {checkpoint_path}\")"]},{"cell_type":"markdown","id":"8e90891a","metadata":{"id":"8e90891a"},"source":["## Train the model"]},{"cell_type":"markdown","id":"e6f4a28a","metadata":{"id":"e6f4a28a"},"source":["### Helper functions"]},{"cell_type":"code","execution_count":13,"id":"562a2fd0","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727794192605,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"562a2fd0"},"outputs":[],"source":["def transform_dataset_wiki40b(subsets=['train', 'validation', 'test']):\n","    logger = logging.getLogger('default')\n","    logger.info(\"Transforming Wiki40B dataset\")\n","\n","    dataset = load_dataset(\"wiki40b\", \"he\")\n","    decoded_dataset = dataset.map(lambda x: {'text': decode_text(x['text'])})\n","\n","    def transform_entry(entry):\n","        # Process the 'text' using parse_wiki_article\n","        article = parse_wiki_article(entry['text'])\n","\n","        # Extract anchor_text and positive_text based on the parsed output\n","        anchor_text = article['title']\n","        if 'sections' in article and len(article['sections']) > 0:\n","            anchor_text += \" \" + article['sections'][0]['section']\n","            positive_text = article['sections'][0]['paragraphs'][0]\n","        else:\n","            positive_text = article['abstract'][0]\n","\n","        # Return the transformed data\n","        return {\n","            'anchor_text': 'query : ' + anchor_text,\n","            'positive_text': 'document: ' + positive_text\n","        }\n","\n","    # Apply the transformation to the train, validation, and test subsets\n","    transformed_dataset = {}\n","    for subset in subsets:\n","        # Transform each subset of the dataset using map (this processes each 'text' entry)\n","        logger.info(f\"Transforming {subset} subset\")\n","        transformed_subset = decoded_dataset[subset].map(transform_entry)\n","        transformed_dataset[subset] = transformed_subset\n","\n","    # Return the transformed dataset as a DatasetDict\n","    logger.info(\"Done transforming Wiki40B dataset\")\n","    return DatasetDict(transformed_dataset)\n","\n","\n","def decode_text(text):\n","    decoded_text = bytes(text, \"utf-8\").decode(\"unicode_escape\").encode(\"latin1\").decode(\"utf-8\")\n","    return decoded_text\n","\n","\n","def parse_wiki_article(text):\n","    lines = text.strip().split('\\n')\n","\n","    PARAGRAPH_DIVIDER = '_NEWLINE_'\n","\n","    # Initialize variables\n","    article_dict = {'title': '', 'abstract': '', 'sections': []}\n","    current_section = None\n","    abstract_parsed = False\n","\n","    i = 0\n","    while i < len(lines):\n","        line = lines[i].strip()\n","\n","        if line == \"_START_ARTICLE_\":\n","            # The next line is the title\n","            article_dict['title'] = lines[i + 1].strip()\n","            i += 2  # Move to the next relevant line\n","        elif line == \"_START_PARAGRAPH_\":\n","            # If the abstract has not been parsed and the current section is None, this is the abstract\n","            paragraph = lines[i + 1].strip()\n","            if not abstract_parsed and not current_section:\n","                article_dict['abstract'] = paragraph.split(PARAGRAPH_DIVIDER)\n","                abstract_parsed = True\n","            elif current_section:\n","                current_section['paragraphs'] = paragraph.split(PARAGRAPH_DIVIDER)\n","            i += 2\n","        elif line == \"_START_SECTION_\":\n","            # The next line is the section name\n","            section_name = lines[i + 1].strip()\n","            current_section = {'section': section_name, 'paragraphs': ''}\n","            article_dict['sections'].append(current_section)\n","            i += 2\n","        else:\n","            i += 1  # Move to the next line if none of the cases match\n","\n","    return article_dict\n","\n","\n","def transform_dataset_synthesized(file_path, test_size=0.2):\n","    logger = logging.getLogger('default')\n","    logger.info(\"Transforming synthesized dataset\")\n","    with open(file_path, 'rb') as f:\n","        data = pickle.load(f)\n","\n","    def transform_entry(entry):\n","        # Return the transformed data\n","        return {\n","            'anchor_text': 'query: ' + entry['user_query'],\n","            'positive_text': 'document: ' + entry['positive_document'],\n","            'negative_text': 'document: ' + entry['hard_negative_document'],\n","        }\n","\n","    # Apply the transformation to each entry\n","    transformed_data = list(map(transform_entry, data))\n","\n","    # Convert the list of dictionaries to a Hugging Face Dataset\n","    dataset = Dataset.from_list(transformed_data)\n","\n","    # Split the dataset into train and test sets\n","    train_test_dataset = dataset.train_test_split(test_size=test_size)\n","    train_validation_dataset = DatasetDict({\n","        'train': train_test_dataset['train'],  # Keep the 'train' split\n","        'validation': train_test_dataset['test']  # Rename 'test' to 'validation'\n","    })\n","\n","    logger.info(\"Done transforming synthesized dataset\")\n","    return train_validation_dataset\n","\n","\n","def transform_dataset(dataset_name, **kwargs):\n","    if dataset_name == 'wiki40b':\n","        return transform_dataset_wiki40b(**kwargs)\n","    elif dataset_name == 'synthesized_dataset':\n","        return transform_dataset_synthesized(**kwargs)\n","    else:\n","        raise ValueError(f\"Unknown dataset name: {dataset_name}\")"]},{"cell_type":"code","execution_count":14,"id":"b041f0fc","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1727794193524,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"b041f0fc"},"outputs":[],"source":["class InfoNCELoss(torch.nn.Module):\n","    def __init__(self, temperature=0.07):\n","        \"\"\"\n","        Parameters:\n","        - temperature: Scaling factor applied to the logits before applying the softmax function.\n","        \"\"\"\n","        super(InfoNCELoss, self).__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, anchor, positive, negatives):\n","        \"\"\"\n","        Compute the InfoNCE loss.\n","\n","        Parameters:\n","        - anchor: Tensor of shape (batch_size, embedding_dim) - anchor samples\n","        - positive: Tensor of shape (batch_size, embedding_dim) - positive samples corresponding to each anchor\n","        - negatives: Tensor of shape (batch_size, num_negatives, embedding_dim) - negative samples\n","\n","        Returns:\n","        - loss: Computed InfoNCE loss\n","        \"\"\"\n","        batch_size = anchor.size(0)\n","        num_negatives = negatives.size(1)\n","\n","        # Normalize embeddings to unit vectors\n","        anchor = F.normalize(anchor, dim=-1)\n","        positive = F.normalize(positive, dim=-1)\n","        negatives = F.normalize(negatives, dim=-1)\n","\n","        # Calculate the positive logits (similarity between anchor and positive)\n","        positive_logits = torch.sum(anchor * positive, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n","\n","        # Calculate the negative logits (similarity between anchor and negatives)\n","        negative_logits = torch.bmm(negatives, anchor.unsqueeze(2)).squeeze(2)  # Shape: (batch_size, num_negatives)\n","\n","        # Concatenate positive and negative logits\n","        logits = torch.cat([positive_logits, negative_logits], dim=1)  # Shape: (batch_size, 1 + num_negatives)\n","\n","        # Apply temperature scaling\n","        logits = logits / self.temperature\n","\n","        # Create labels - 0 for the positive samples, as it is the first in the concatenated logits\n","        labels = torch.zeros(batch_size, dtype=torch.long, device=logits.device)\n","\n","        # Compute the InfoNCE loss using cross-entropy\n","        loss = F.cross_entropy(logits, labels)\n","\n","        return loss"]},{"cell_type":"code","execution_count":15,"id":"e09d3885","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1727794194669,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"e09d3885"},"outputs":[],"source":["def validate(model, val_dataloader, criterion, device, epoch, epochs):\n","    model.eval()\n","    total_val_loss = 0.0\n","\n","    # Track progress in the validation loop using tqdm\n","    val_progress = tqdm(enumerate(val_dataloader), desc=f\"Epoch {epoch + 1}/{epochs} [Val]\", leave=False, total=len(val_dataloader))\n","\n","    with torch.no_grad():\n","        for batch_idx, batch in val_progress:\n","\n","            if len(batch) == 4:\n","                anchor_ids, anchor_mask, positive_ids, positive_mask = [x.to(device) for x in batch]\n","\n","                # Forward pass to get the embeddings\n","                anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n","                anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n","                positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                # Set negatives as the other positives in the batch\n","                # Create a matrix where the negatives are shifted versions of positives\n","                batch_size = positive_embeds.size(0)\n","    #             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n","                # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n","                negatives_embeds_list = []\n","\n","                for i in range(batch_size):\n","                    # Exclude the current index `i` using slicing\n","                    negatives_embeds = torch.cat([positive_embeds[:i], positive_embeds[i+1:]], dim=0)\n","\n","                    # Append the result to the list\n","                    negatives_embeds_list.append(negatives_embeds)\n","\n","                # Stack the negatives for each sample in the batch\n","                # Each entry in the batch now has (batch_size - 1) negative embeddings\n","                negatives_embeds = torch.stack(negatives_embeds_list)\n","\n","            elif len(batch) == 6:\n","                anchor_ids, anchor_mask, positive_ids, positive_mask, negative_ids, negative_mask = [x.to(device) for x in batch]\n","\n","                # Forward pass to get the embeddings\n","                anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n","                anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n","                positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                negative_outputs = model(input_ids=negative_ids, attention_mask=negative_mask)\n","                negative_embeds = negative_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                # Set negatives as the other positives in the batch\n","                # Create a matrix where the negatives are shifted versions of positives\n","                batch_size = positive_embeds.size(0)\n","    #             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n","                # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n","                negatives_embeds_list = []\n","\n","                for i in range(batch_size):\n","                    # Exclude the current index `i` using slicing\n","                    negatives_embeds = torch.cat([positive_embeds[:i], negative_embeds, positive_embeds[i+1:]], dim=0)\n","\n","                    # Append the result to the list\n","                    negatives_embeds_list.append(negatives_embeds)\n","\n","                # Stack the negatives for each sample in the batch\n","                # Each entry in the batch now has (batch_size - 1) negative embeddings\n","                negatives_embeds = torch.stack(negatives_embeds_list)\n","\n","            # Compute the validation loss\n","            val_loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n","            total_val_loss += val_loss.item()\n","\n","            # Update tqdm progress bar with the current batch number and average loss\n","            val_progress.set_postfix({\n","                \"Batch\": batch_idx + 1,\n","                \"Val Loss\": total_val_loss / (batch_idx + 1)\n","            })\n","\n","    avg_val_loss = total_val_loss / len(val_dataloader)\n","    return avg_val_loss"]},{"cell_type":"code","execution_count":16,"id":"qFLUHwgsVdyS","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1727794193863,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"qFLUHwgsVdyS"},"outputs":[],"source":["def train(\n","    model,\n","    optimizer,\n","    criterion,\n","    train_dataloader,\n","    val_dataloader,\n","    device,\n","    epochs,\n","    start_epoch=0,\n","    checkpoint_dir='checkpoints',\n","    clip_value = None\n","):\n","    logger = logging.getLogger('default')\n","    logger.info(\"Start training\")\n","\n","    model.to(device)\n","    model.train()\n","    best_val_loss = float('inf')  # Initialize best validation loss to infinity\n","\n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","\n","    for epoch in range(start_epoch, epochs):\n","        total_train_loss = 0.0\n","        model.to(device)\n","        model.train()\n","\n","        # Track progress in the training loop using tqdm\n","        train_progress = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False, total=len(train_dataloader))\n","\n","        for batch_idx, batch in train_progress:\n","\n","            if len(batch) == 4: # in case we have only (anchor, positive)\n","                anchor_ids, anchor_mask, positive_ids, positive_mask = [x.to(device) for x in batch]\n","\n","                # Forward pass to get the embeddings\n","                anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n","                anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n","                positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                # Set negatives as the other positives in the batch\n","                # Create a matrix where the negatives are shifted versions of positives\n","                batch_size = positive_embeds.size(0)\n","    #             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n","                # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n","                negatives_embeds_list = []\n","\n","                for i in range(batch_size):\n","                    # Exclude the current index `i` using slicing\n","                    negatives_embeds = torch.cat([positive_embeds[:i], positive_embeds[i+1:]], dim=0)\n","\n","                    # Append the result to the list\n","                    negatives_embeds_list.append(negatives_embeds)\n","\n","                # Stack the negatives for each sample in the batch\n","                # Each entry in the batch now has (batch_size - 1) negative embeddings\n","                negatives_embeds = torch.stack(negatives_embeds_list)\n","\n","            elif len(batch) == 6: # in case we have only (anchor, positive, negative)\n","                anchor_ids, anchor_mask, positive_ids, positive_mask, negative_ids, negative_mask = [x.to(device) for x in batch]\n","\n","                # Forward pass to get the embeddings\n","                anchor_outputs = model(input_ids=anchor_ids, attention_mask=anchor_mask)\n","                anchor_embeds = anchor_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                positive_outputs = model(input_ids=positive_ids, attention_mask=positive_mask)\n","                positive_embeds = positive_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                negative_outputs = model(input_ids=negative_ids, attention_mask=negative_mask)\n","                negative_embeds = negative_outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n","\n","                # Set negatives as the other positives in the batch\n","                # Create a matrix where the negatives are shifted versions of positives\n","                batch_size = positive_embeds.size(0)\n","    #             negatives_embeds = torch.stack([positive_embeds[i:] + positive_embeds[:i] for i in range(1, batch_size)], dim=0)\n","                # Create the negatives for each index `i` by excluding the positive embedding at index `i`\n","                negatives_embeds_list = []\n","\n","                for i in range(batch_size):\n","                    # Exclude the current index `i` using slicing\n","                    negatives_embeds = torch.cat([positive_embeds[:i], negative_embeds, positive_embeds[i+1:]], dim=0)\n","\n","                    # Append the result to the list\n","                    negatives_embeds_list.append(negatives_embeds)\n","\n","                # Stack the negatives for each sample in the batch\n","                # Each entry in the batch now has (batch_size - 1) negative embeddings\n","                negatives_embeds = torch.stack(negatives_embeds_list)\n","\n","            # Compute the InfoNCE loss\n","            loss = criterion(anchor_embeds, positive_embeds, negatives_embeds)\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            if clip_value is not None:\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n","            optimizer.step()\n","\n","            total_train_loss += loss.item()\n","\n","            # Update tqdm progress bar with the current batch number and average loss\n","            train_progress.set_postfix({\n","                \"Batch\": batch_idx + 1,\n","                \"Train Loss\": total_train_loss / (batch_idx + 1)\n","            })\n","\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","        logger.info(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n","\n","        # Compute validation loss after each epoch\n","        avg_val_loss = validate(model, val_dataloader, criterion, device, epoch, epochs)\n","        logger.info(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss}\")\n","\n","        # Save checkpoint after each epoch\n","        if loss < best_val_loss:\n","            best_val_loss = loss\n","            save_checkpoint(model, optimizer, epoch, checkpoint_dir)"]},{"cell_type":"markdown","id":"q0e6SUDomGug","metadata":{"id":"q0e6SUDomGug"},"source":["### Wiki40b"]},{"cell_type":"code","execution_count":17,"id":"183ab254","metadata":{"executionInfo":{"elapsed":357,"status":"ok","timestamp":1727794196301,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"183ab254"},"outputs":[],"source":["from datetime import datetime\n","from torch.optim import AdamW"]},{"cell_type":"code","execution_count":18,"id":"720181eb","metadata":{"executionInfo":{"elapsed":305,"status":"ok","timestamp":1727794197995,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"720181eb"},"outputs":[],"source":["MODEL_NAME = 'intfloat/multilingual-e5-large'\n","BATCH_SIZE = 64\n","LEARNING_RATE = 5e-5\n","WEIGHT_DECAY = 1e-4\n","CLIP_VALUE = 1.0\n","INFONCE_TEMPERATURE = 0.07\n","EPOCHS = 10"]},{"cell_type":"code","execution_count":19,"id":"7e252173","metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1727794280345,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"7e252173"},"outputs":[],"source":["model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n","log_file = f\"./logs/hte_training_{model_name_slug}_01_wiki40b.log\"\n","logger = setup_logger(log_file)"]},{"cell_type":"code","execution_count":20,"id":"7b19280a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":84087,"status":"ok","timestamp":1727795087347,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"7b19280a","outputId":"772b458a-81d9-4b9e-c56e-06ca8d6bd938"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-02 08:34:51,804 - default - INFO - Using device: cuda\n","2024-10-02 08:34:56,570 - default - INFO - Start train base model: intfloat/multilingual-e5-large\n","2024-10-02 08:34:57,070 - default - INFO - Switching to new dataset: wiki40b\n","2024-10-02 08:34:57,071 - default - INFO - Transforming Wiki40B dataset\n","2024-10-02 08:35:07,541 - default - INFO - Transforming train subset\n","2024-10-02 08:35:07,551 - default - INFO - Transforming validation subset\n","2024-10-02 08:35:07,556 - default - INFO - Done transforming Wiki40B dataset\n","2024-10-02 08:35:07,561 - default - INFO - Tokenizing train dataset\n","2024-10-02 08:36:05,951 - default - INFO - Creating train dataloader\n","2024-10-02 08:36:05,952 - default - INFO - Tokenizing validation dataset\n","2024-10-02 08:36:08,663 - default - INFO - Creating validation dataloader\n","2024-10-02 08:36:08,664 - default - INFO - Loading checkpoint\n","2024-10-02 08:36:08,665 - default - INFO - No checkpoint found. Starting from scratch.\n","2024-10-02 08:36:08,665 - default - INFO - Start training\n","Epoch 1/10 [Train]:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                                                     | 434/2584 [31:56<2:38:07,  4.41s/it, Batch=434, Train Loss=0.396]"]}],"source":["%%time\n","\n","# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# Define model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","model = model.to(device)\n","logger.info(f\"Start train base model: {MODEL_NAME}\")\n","\n","# Initialize the InfoNCE loss and the optimizer\n","criterion = InfoNCELoss(temperature=INFONCE_TEMPERATURE)\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","# Datasets to train on\n","dataset_names = ['wiki40b']\n","\n","# Iterate over datasets and train\n","for dataset_name in dataset_names:\n","    start_datetime = datetime.now()\n","\n","    logger.info(f\"Switching to new dataset: {dataset_name}\")\n","    dataset = transform_dataset(dataset_name, subsets=['train', 'validation'])\n","\n","    # Tokenize the train dataset\n","    logger.info(f\"Tokenizing train dataset\")\n","    anchor_inputs_train = tokenizer(dataset['train']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n","    positive_inputs_train = tokenizer(dataset['train']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n","\n","    # Create DataLoader for training\n","    logger.info(f\"Creating train dataloader\")\n","    train_dataset = TensorDataset(anchor_inputs_train['input_ids'], anchor_inputs_train['attention_mask'],\n","                                  positive_inputs_train['input_ids'], positive_inputs_train['attention_mask'])\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    # Tokenize the validation dataset\n","    logger.info(f\"Tokenizing validation dataset\")\n","    anchor_inputs_val = tokenizer(dataset['validation']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n","    positive_inputs_val = tokenizer(dataset['validation']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n","\n","    # Create DataLoader for validation\n","    logger.info(f\"Creating validation dataloader\")\n","    val_dataset = TensorDataset(anchor_inputs_val['input_ids'], anchor_inputs_val['attention_mask'],\n","                                positive_inputs_val['input_ids'], positive_inputs_val['attention_mask'])\n","    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # Load the latest checkpoint if available and resume training\n","    logger.info(f\"Loading checkpoint\")\n","    checkpoint_dir = f\"checkpoints/{model_name_slug}/checkpoints_01_wiki40b\"\n","    start_epoch = load_checkpoint(model, optimizer, checkpoint_dir=checkpoint_dir, device=device)\n","\n","    # Train the model for this dataset\n","    train(\n","        model=model,\n","        optimizer=optimizer,\n","        criterion=criterion,\n","        train_dataloader=train_dataloader,\n","        val_dataloader=val_dataloader,\n","        device=device,\n","        epochs=EPOCHS,\n","        start_epoch=start_epoch,\n","        checkpoint_dir=checkpoint_dir,\n","        clip_value=CLIP_VALUE\n","    )\n","\n","    end_datetime = datetime.now()\n","    logger.info(f\"Total training on {dataset_name} elapsed time is {(end_datetime - start_datetime).total_seconds()} seconds\")\n","\n","logger.info(f\"End train base model: {MODEL_NAME}\")"]},{"cell_type":"markdown","id":"548b0033","metadata":{},"source":[]},{"cell_type":"markdown","id":"AW4n8Kv_md_K","metadata":{"id":"AW4n8Kv_md_K"},"source":["### Synthesized data"]},{"cell_type":"code","execution_count":null,"id":"cT2qF5m2mgdF","metadata":{"id":"cT2qF5m2mgdF"},"outputs":[],"source":["MODEL_NAME = 'intfloat/multilingual-e5-base'\n","BATCH_SIZE = 16\n","LEARNING_RATE = 5e-5\n","INFONCE_TEMPERATURE = 0.07"]},{"cell_type":"code","execution_count":null,"id":"cm_4MajjmgfX","metadata":{"id":"cm_4MajjmgfX"},"outputs":[],"source":["model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n","log_file = f\"./logs/hte_training_{model_name_slug}_02_synthesized.log\"\n","logger = setup_logger(log_file)"]},{"cell_type":"code","execution_count":null,"id":"EaZWAMDcmghZ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"elapsed":55248,"status":"ok","timestamp":1726130854569,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"EaZWAMDcmghZ","outputId":"4cf7f38c-3132-4f8a-b7ba-05478f97ce5d"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-12 08:46:38,985 - default - INFO - Using device: cuda\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec08f351e68b4f5f8d1134c1d9d2552f","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e8d482ea3b942d98fe0d5cedb025809","version_major":2,"version_minor":0},"text/plain":["sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56d49f41a88642d985a5ece33f5c2275","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f44d05b7ee554644a5e87eddb9e22b39","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81d4a96c231b442e9331394c58023334","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a07c8f582a64c94a8714281c871dc0c","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2024-09-12 08:46:49,938 - default - INFO - Start train base model: intfloat/multilingual-e5-base\n","2024-09-12 08:46:50,945 - default - INFO - Loading checkpoint checkpoint_epoch_2.pth\n","<ipython-input-12-f52da8a64e1d>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n","2024-09-12 08:47:33,963 - default - INFO - Loaded model from epoch 2\n"]}],"source":["# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# Define model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","model = model.to(device)\n","logger.info(f\"Start train base model: {MODEL_NAME}\")\n","\n","# Initialize the InfoNCE loss and the optimizer\n","criterion = InfoNCELoss(temperature=INFONCE_TEMPERATURE)\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","\n","# Load model at checkpoint\n","epoch = load_checkpoint(model=model, optimizer=optimizer, checkpoint_dir='checkpoints/checkpoints_01_wiki40b', device=device)\n","logger.info(f\"Loaded model from epoch {epoch}\")"]},{"cell_type":"code","execution_count":null,"id":"nFMnzH_LPKxL","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1416084,"status":"ok","timestamp":1726132270650,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"nFMnzH_LPKxL","outputId":"01407124-9911-4458-93fb-ba24de0085b4"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-12 08:47:34,022 - default - INFO - Switching to new dataset: synthesized_dataset\n","2024-09-12 08:47:43,775 - default - INFO - No checkpoint found. Starting from scratch.\n","2024-09-12 08:54:49,910 - default - INFO - Epoch 1, Train Loss: 0.4075799766257405\n","2024-09-12 08:55:21,333 - default - INFO - Epoch 1, Validation Loss: 0.29725874787569045\n","2024-09-12 08:55:30,123 - default - INFO - Checkpoint saved at checkpoints/checkpoints_02_synthesized/checkpoint_epoch_0.pth\n","2024-09-12 09:02:35,619 - default - INFO - Epoch 2, Train Loss: 0.1710497047379613\n","2024-09-12 09:03:07,053 - default - INFO - Epoch 2, Validation Loss: 0.2627878464460373\n","2024-09-12 09:03:19,624 - default - INFO - Checkpoint saved at checkpoints/checkpoints_02_synthesized/checkpoint_epoch_1.pth\n","2024-09-12 09:10:25,313 - default - INFO - Epoch 3, Train Loss: 0.11907066453620792\n","2024-09-12 09:10:56,767 - default - INFO - Epoch 3, Validation Loss: 0.26005379532277584\n","2024-09-12 09:11:10,023 - default - INFO - Checkpoint saved at checkpoints/checkpoints_02_synthesized/checkpoint_epoch_2.pth\n","2024-09-12 09:11:10,027 - default - INFO - Total training on synthesized_dataset elapsed time is 1416.004475 seconds\n","2024-09-12 09:11:10,028 - default - INFO - End train base model: intfloat/multilingual-e5-base\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 14min 18s, sys: 9min 20s, total: 23min 39s\n","Wall time: 23min 36s\n"]}],"source":["%%time\n","\n","# Datasets to train on\n","dataset_names = ['synthesized_dataset']\n","\n","# Iterate over datasets and train\n","for dataset_name in dataset_names:\n","    start_datetime = datetime.now()\n","\n","    logger.info(f\"Switching to new dataset: {dataset_name}\")\n","    dataset = transform_dataset(dataset_name, tokenizer=tokenizer, file_path='data/synthetic_data_20240906_0018.pkl')\n","\n","    # Tokenize the train dataset\n","    anchor_inputs_train = tokenizer(dataset['train']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n","    positive_inputs_train = tokenizer(dataset['train']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n","    negative_inputs_train = tokenizer(dataset['train']['negative_text'], return_tensors='pt', padding=True, truncation=True)\n","\n","    # Create DataLoader for training\n","    train_dataset = TensorDataset(anchor_inputs_train['input_ids'], anchor_inputs_train['attention_mask'],\n","                                  positive_inputs_train['input_ids'], positive_inputs_train['attention_mask'],\n","                                  negative_inputs_train['input_ids'], negative_inputs_train['attention_mask'])\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    # Tokenize the validation dataset\n","    anchor_inputs_val = tokenizer(dataset['validation']['anchor_text'], return_tensors='pt', padding=True, truncation=True)\n","    positive_inputs_val = tokenizer(dataset['validation']['positive_text'], return_tensors='pt', padding=True, truncation=True)\n","    negative_inputs_val = tokenizer(dataset['validation']['negative_text'], return_tensors='pt', padding=True, truncation=True)\n","\n","    # Create DataLoader for validation\n","    val_dataset = TensorDataset(anchor_inputs_val['input_ids'], anchor_inputs_val['attention_mask'],\n","                                positive_inputs_val['input_ids'], positive_inputs_val['attention_mask'],\n","                                negative_inputs_val['input_ids'], negative_inputs_val['attention_mask'])\n","    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    # Load the latest checkpoint if available and resume training\n","    checkpoint_dir = \"checkpoints/checkpoints_02_synthesized\"\n","    start_epoch = load_checkpoint(model, optimizer, checkpoint_dir=checkpoint_dir, device=device)\n","\n","    # Train the model for this dataset\n","    train(\n","        model=model,\n","        optimizer=optimizer,\n","        criterion=criterion,\n","        train_dataloader=train_dataloader,\n","        val_dataloader=val_dataloader,\n","        device=device,\n","        epochs=3,\n","        start_epoch=start_epoch,\n","        checkpoint_dir=checkpoint_dir\n","    )\n","\n","    end_datetime = datetime.now()\n","    logger.info(f\"Total training on {dataset_name} elapsed time is {(end_datetime - start_datetime).total_seconds()} seconds\")\n","\n","logger.info(f\"End train base model: {MODEL_NAME}\")"]},{"cell_type":"markdown","id":"rFETCvFPImdT","metadata":{"id":"rFETCvFPImdT"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"id":"JDVaa1VIEN8e","metadata":{"id":"JDVaa1VIEN8e"},"outputs":[],"source":["from datasets import load_dataset, Dataset, DatasetDict\n","from transformers import AutoModel, AutoTokenizer\n","import json\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","import logging\n","import os\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","id":"50RZXdx8MVyg","metadata":{"id":"50RZXdx8MVyg"},"source":["### HebNLI"]},{"cell_type":"code","execution_count":null,"id":"_RbfgggJGQWC","metadata":{"id":"_RbfgggJGQWC"},"outputs":[],"source":["# !git clone https://github.com/NNLP-IL/HebNLI.git eval/HebNLI # do not uncomment -- train jsonl file was manually added"]},{"cell_type":"code","execution_count":null,"id":"jKIFAd3xGs2Q","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6533,"status":"ok","timestamp":1726356545702,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"jKIFAd3xGs2Q","outputId":"35d444d6-bf91-42b9-e34f-a5c2cfefd64c"},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['premise', 'hypothesis', 'label'],\n","        num_rows: 300067\n","    })\n","    validation: Dataset({\n","        features: ['premise', 'hypothesis', 'label'],\n","        num_rows: 1999\n","    })\n","    test: Dataset({\n","        features: ['premise', 'hypothesis', 'label'],\n","        num_rows: 579\n","    })\n","})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["label_mapping = {\n","    'entailment': 0,\n","    'neutral': 1,\n","    'contradiction': 2,\n","}\n","\n","def load_jsonl_data(file_path):\n","    data = []\n","\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        for line in file:\n","            record = json.loads(line)\n","            if 'translation1' in record and 'translation2' in record and 'original_label' in record and record['original_label'] in label_mapping.keys():\n","                data.append({\n","                    'premise': record['translation1'],\n","                    'hypothesis': record['translation2'],\n","                    'label': label_mapping[record['original_label']]\n","                })\n","    return data\n","\n","# Load the data for each subset\n","train_data = load_jsonl_data('eval/HebNLI/HebNLI_train.jsonl')\n","val_data = load_jsonl_data('eval/HebNLI/HebNLI_val.jsonl')\n","test_data = load_jsonl_data('eval/HebNLI/HebNLI_test.jsonl')\n","\n","# Create a DatasetDict containing all the subsets\n","nli_dataset = DatasetDict({\n","    'train': Dataset.from_pandas(pd.DataFrame(train_data)),\n","    'validation': Dataset.from_pandas(pd.DataFrame(val_data)),\n","    'test': Dataset.from_pandas(pd.DataFrame(test_data))\n","})\n","nli_dataset"]},{"cell_type":"code","execution_count":null,"id":"PcnPXVQXmgnZ","metadata":{"id":"PcnPXVQXmgnZ"},"outputs":[],"source":["class CustomNLIModel(nn.Module):\n","    def __init__(self, pretrained_model, hidden_size=768, num_labels=3, freeze_backbone_weights=False):\n","        super(CustomNLIModel, self).__init__()\n","        # Use a pre-trained model like BERT or any custom backbone\n","        self.backbone = pretrained_model\n","\n","        # Freeze the base model's weights\n","        if freeze_backbone_weights:\n","            for param in self.backbone.parameters():\n","                param.requires_grad = False\n","\n","        self.classifier = nn.Linear(hidden_size, num_labels)  # Final classifier layer for entailment/contradiction/neutral\n","\n","    def forward(self, input_ids, attention_mask=None):\n","        # Forward pass through the backbone model (e.g., BERT, RoBERTa)\n","        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.last_hidden_state[:, 0]  # Use [CLS] token representation for classification\n","\n","        # Pass the pooled output through the classifier\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","\n","def tokenize_dataset(dataset, tokenizer):\n","    # Tokenize the dataset\n","    def preprocess_function(examples):\n","        return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding='max_length')\n","\n","    # Tokenize the entire dataset\n","    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n","    return tokenized_dataset\n","\n","\n","def create_dataloader(tokenized_dataset, batch_size=16):\n","    # Convert the dataset to PyTorch tensors\n","    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n","\n","    # Create a DataLoader\n","    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size)\n","    return dataloader\n","\n","\n","def train_nli(model, train_dataloader, val_dataloader, device, epochs=3, lr=2e-5):\n","    # Define loss function and optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = AdamW(model.parameters(), lr=lr)\n","\n","    for epoch in range(epochs):\n","        total_train_loss = 0.0\n","        model.to(device)\n","        model.train()\n","\n","        # Track progress in the training loop using tqdm\n","        train_progress = tqdm(enumerate(train_dataloader), desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False, total=len(train_dataloader))\n","\n","        # Training loop\n","        for batch_idx, batch in train_progress:\n","            total_loss = 0.0\n","            correct = 0\n","            total = 0\n","\n","            # Move batch to device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            # Forward pass\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            # Backward pass and optimization\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_train_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            # Update tqdm progress bar with the current batch number and average loss\n","            train_progress.set_postfix({\n","                \"Batch\": batch_idx + 1,\n","                \"Train Loss\": total_train_loss / (batch_idx + 1)\n","            })\n","\n","        # Print epoch loss and accuracy\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","        train_accuracy = correct / total\n","        logger.info(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n","\n","        # Compute validation loss after each epoch\n","        avg_val_loss, val_accuracy = validate_nli(model, val_dataloader, criterion, device, epoch, epochs)\n","\n","\n","def validate_nli(model, val_dataloader, criterion, device, epoch, epochs):\n","    model.eval()\n","    total_val_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    # Track progress in the validation loop using tqdm\n","    val_progress = tqdm(enumerate(val_dataloader), desc=f\"Epoch {epoch + 1}/{epochs} [Val]\", leave=False, total=len(val_dataloader))\n","\n","    with torch.no_grad():\n","        for batch_idx, batch in val_progress:\n","            # Move batch to device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            # Forward pass\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            total_val_loss += loss.item()\n","\n","            # Calculate accuracy\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    # Calculate average validation loss and accuracy\n","    avg_val_loss = total_val_loss / len(val_dataloader)\n","    val_accuracy = correct / total\n","\n","    logger.info(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n","    return avg_val_loss, val_accuracy"]},{"cell_type":"code","execution_count":null,"id":"S_y0aZXDjTf3","metadata":{"id":"S_y0aZXDjTf3"},"outputs":[],"source":["MODEL_NAME = 'intfloat/multilingual-e5-base'\n","model_name_slug = MODEL_NAME.replace('/', '_').replace('-', '_')\n","\n","BATCH_SIZE = 32\n","LR = 2e-5\n","EPOCHS = 3"]},{"cell_type":"code","execution_count":null,"id":"0pY2R8LOz9sh","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["a59b7c532e954407acd3603d171db2e6","bb1807db12734f3396a4440d8c489b74","6390658c89624f49a3500cc42cb6bebd","9171d2cc920a48c78369faeae7e9270b","a27602f6fa91495cb20a3b2940d5feaa","5c6ec2b95d7b407fa5940f22cac38674","8443d92afc384e11bb35061e7e919247","095b1fdf16054838a2aa095013bf5e80","46a29f2b03b84861b1635db9b60baa58","9fc3e440c06244aea69f870ad18f8d56","112b78607bd74539908eb130a6e74c43","da8e7dcf6b6345e8a317ca1a9abd9e43","33b0c5cd32ef49aebbf8967cbf43ca6a","252947faf2194db19807627731ff64b6","62ee7d58c130460a8d5b3191c9f1a09a","6dd9d64106bd49aaa996b51f668abb30","c49e0b631a434fedac13e3d1446ee674","2249a19886074df1a4fad68bdc8a5778","4bdea7a561be4446a181ef2ce7b879f3","ae776ab7fccf455fbe80fd63dd756a17","17a6de20321e499496bded4ded56491e","d74890aad64e4073a2678e4b45845fcc"]},"executionInfo":{"elapsed":62977,"status":"ok","timestamp":1726356678900,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"0pY2R8LOz9sh","outputId":"1325035c-d07f-41c3-b55a-77789a760bd9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a59b7c532e954407acd3603d171db2e6","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/300067 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"da8e7dcf6b6345e8a317ca1a9abd9e43","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1999 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["# Load and tokenize the train and validation dataset\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","train_tokenized_dataset = tokenize_dataset(nli_dataset['train'], tokenizer)\n","val_tokenized_dataset = tokenize_dataset(nli_dataset['validation'], tokenizer)"]},{"cell_type":"markdown","id":"vgPGZ0mIhDVS","metadata":{"id":"vgPGZ0mIhDVS"},"source":["### Base model"]},{"cell_type":"code","execution_count":null,"id":"I08qQt6GhCpb","metadata":{"id":"I08qQt6GhCpb"},"outputs":[],"source":["log_file = f\"./logs/hte_eval_{model_name_slug}_00_base_hebnli.log\"\n","logger = setup_logger(log_file)\n","\n","# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# Define backbone model\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","model = model.to(device)\n","logger.info(f\"Start train base model: {MODEL_NAME}\")\n","\n","# Define NLI model\n","nli_model = CustomNLIModel(model)\n","\n","# Create the train and validation DataLoader\n","train_dataloader = create_dataloader(train_tokenized_dataset, batch_size=BATCH_SIZE)\n","val_dataloader = create_dataloader(val_tokenized_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"JQoxxu1ysEY6","metadata":{"id":"JQoxxu1ysEY6"},"outputs":[],"source":["train_nli(\n","    model=nli_model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    device=device,\n","    epochs=EPOCHS,\n","    lr=LR\n",")"]},{"cell_type":"markdown","id":"mDvqejgXhF8_","metadata":{"id":"mDvqejgXhF8_"},"source":["### 1st pass (Wiki40b) trained model"]},{"cell_type":"code","execution_count":null,"id":"l288O7Eagj7A","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5024,"status":"ok","timestamp":1726356683910,"user":{"displayName":"Asaf Achi Mordechai","userId":"00322487638289632571"},"user_tz":-180},"id":"l288O7Eagj7A","outputId":"17a6dda4-cf99-4199-ee0c-7516061436e0"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-14 23:31:18,550 - default - INFO - Using device: cuda\n","2024-09-14 23:31:19,245 - default - INFO - Start train base model: intfloat/multilingual-e5-base\n","2024-09-14 23:31:19,249 - default - INFO - Loading checkpoint checkpoint_epoch_2.pth\n","<ipython-input-9-f52da8a64e1d>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n","2024-09-14 23:31:23,325 - default - INFO - Loaded model from epoch 2\n"]}],"source":["log_file = f\"./logs/hte_eval_{model_name_slug}_01_wiki40b_hebnli.log\"\n","logger = setup_logger(log_file)\n","\n","# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# Define backbone model\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","model = model.to(device)\n","logger.info(f\"Start train base model: {MODEL_NAME}\")\n","\n","# Load model at checkpoint\n","optimizer = AdamW(model.parameters(), lr=LR)\n","epoch = load_checkpoint(model=model, optimizer=optimizer, checkpoint_dir='checkpoints/checkpoints_01_wiki40b', device=device)\n","logger.info(f\"Loaded model from epoch {epoch}\")\n","\n","# Define NLI model\n","nli_model = CustomNLIModel(model)\n","\n","# Create the train and validation DataLoader\n","train_dataloader = create_dataloader(train_tokenized_dataset, batch_size=BATCH_SIZE)\n","val_dataloader = create_dataloader(val_tokenized_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"487iYwRQj0kY","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"487iYwRQj0kY","outputId":"2ebdaa54-37be-40f0-f936-a7e5db03db92"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-15 01:26:32,504 - default - INFO - Epoch 1/3, Loss: 0.7150, Accuracy: 0.6667\n","2024-09-15 01:26:47,004 - default - INFO - Validation Loss: 0.6193, Validation Accuracy: 0.7394\n","2024-09-15 03:21:59,500 - default - INFO - Epoch 2/3, Loss: 0.5688, Accuracy: 1.0000\n","2024-09-15 03:22:14,044 - default - INFO - Validation Loss: 0.6165, Validation Accuracy: 0.7674\n","2024-09-15 05:17:30,848 - default - INFO - Epoch 3/3, Loss: 0.4737, Accuracy: 1.0000\n","2024-09-15 05:17:45,449 - default - INFO - Validation Loss: 0.6666, Validation Accuracy: 0.7559\n"]}],"source":["train_nli(\n","    model=nli_model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    device=device,\n","    epochs=EPOCHS,\n","    lr=LR\n",")"]},{"cell_type":"markdown","id":"iDjoggDqhPY_","metadata":{"id":"iDjoggDqhPY_"},"source":["### 2nd pass (Synthesized data) trained model"]},{"cell_type":"code","execution_count":null,"id":"TmRseFZ01PJY","metadata":{"colab":{"background_save":true},"id":"TmRseFZ01PJY","outputId":"14fcdf8b-700b-496e-d7ea-ab3771648cb2"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-09-15 05:17:45,487 - default - INFO - Using device: cuda\n","2024-09-15 05:17:47,875 - default - INFO - Start train base model: intfloat/multilingual-e5-base\n","2024-09-15 05:17:49,426 - default - INFO - Loading checkpoint checkpoint_epoch_2.pth\n","<ipython-input-9-f52da8a64e1d>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(checkpoint_path, map_location=device)\n","2024-09-15 05:18:34,025 - default - INFO - Loaded model from epoch 2\n"]}],"source":["log_file = f\"./logs/hte_eval_{model_name_slug}_02_synthesized_hebnli.log\"\n","logger = setup_logger(log_file)\n","\n","# device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logger.info(f\"Using device: {device}\")\n","\n","# Define backbone model\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","model = AutoModel.from_pretrained(MODEL_NAME)\n","model = model.to(device)\n","logger.info(f\"Start train base model: {MODEL_NAME}\")\n","\n","# Load model at checkpoint\n","optimizer = AdamW(model.parameters(), lr=LR)\n","epoch = load_checkpoint(model=model, optimizer=optimizer, checkpoint_dir='checkpoints/checkpoints_02_synthesized', device=device)\n","logger.info(f\"Loaded model from epoch {epoch}\")\n","\n","# Define NLI model\n","nli_model = CustomNLIModel(model)\n","\n","# Create the train and validation DataLoader\n","train_dataloader = create_dataloader(train_tokenized_dataset, batch_size=BATCH_SIZE)\n","val_dataloader = create_dataloader(val_tokenized_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"id":"67nvFb_AMUmH","metadata":{"colab":{"background_save":true},"id":"67nvFb_AMUmH","outputId":"a3785e98-44ad-460a-d928-09b4f85812dc"},"outputs":[{"name":"stderr","output_type":"stream","text":["Epoch 1/3 [Train]:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14648/18755 [1:30:05<25:15,  2.71it/s, Batch=14648, Train Loss=0.741]"]}],"source":["train_nli(\n","    model=nli_model,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    device=device,\n","    epochs=EPOCHS,\n","    lr=LR\n",")"]},{"cell_type":"code","execution_count":null,"id":"0uuOeOtmMUp_","metadata":{"id":"0uuOeOtmMUp_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"RKouwP5XMUs_","metadata":{"id":"RKouwP5XMUs_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"nHv_W79NwtJX","metadata":{"id":"nHv_W79NwtJX"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["AW4n8Kv_md_K"],"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"https://github.com/asafam/hebrew_text_encoder/blob/main/notebooks/hte_train_model.ipynb","timestamp":1725628368980}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"007e143af14b4913b0e9f2295bf1b047":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07e97fbb0a584b4fac1095bb57d3f79a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"095b1fdf16054838a2aa095013bf5e80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"112b78607bd74539908eb130a6e74c43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"112f57b801e94261a8ec5d5b76c8aca8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"14db26ed55564ba19047d2bbd86e6845":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70c03e9fb07948b285a587d3ff383bf1","placeholder":"â€‹","style":"IPY_MODEL_e89d4f3fd687453980a092c7f38e7f07","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"17a6de20321e499496bded4ded56491e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"198e5d3720554cce85fb2e07748a7502":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4f4b68d5664b11b4381729d9ed50a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ffa46644d9649d79488a67b9807fa17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"2249a19886074df1a4fad68bdc8a5778":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"252947faf2194db19807627731ff64b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bdea7a561be4446a181ef2ce7b879f3","max":1999,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae776ab7fccf455fbe80fd63dd756a17","value":1999}},"2d40d2f244a948c5b4e71e08e872c06c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_007e143af14b4913b0e9f2295bf1b047","placeholder":"â€‹","style":"IPY_MODEL_07e97fbb0a584b4fac1095bb57d3f79a","value":"Connecting..."}},"32d1eec6fad24a77bc386362f2b46712":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"PasswordModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_4560b6ae9f7646c0bea921834ca6ceb8","placeholder":"â€‹","style":"IPY_MODEL_e92da37428de48529f8243fc8d985ab1","value":""}},"33b0c5cd32ef49aebbf8967cbf43ca6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c49e0b631a434fedac13e3d1446ee674","placeholder":"â€‹","style":"IPY_MODEL_2249a19886074df1a4fad68bdc8a5778","value":"Map:â€‡100%"}},"352a9cbea1bf492e97f31b0ea271633e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ButtonModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_e09d695bcc134d90a5de448029a576c0","style":"IPY_MODEL_112f57b801e94261a8ec5d5b76c8aca8","tooltip":""}},"3a809378f72d479e92f6d8d05573fa7e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4070f80e77974ebcbef6930827281e4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4116b745dd004517a70887d5fb77c953":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"42fc0328683a41b49bff59ecc7ddf333":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cac306aaa83d4aada75976ddc2634181","placeholder":"â€‹","style":"IPY_MODEL_4116b745dd004517a70887d5fb77c953","value":"Your token has been saved in your configured git credential helpers (store)."}},"4560b6ae9f7646c0bea921834ca6ceb8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46a29f2b03b84861b1635db9b60baa58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4939f27669f84693b3e194d9b9988a50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b441436f1a4ff1bb84f51c21a10b59","placeholder":"â€‹","style":"IPY_MODEL_3a809378f72d479e92f6d8d05573fa7e","value":"Your token has been saved to /root/.cache/huggingface/token"}},"4bdea7a561be4446a181ef2ce7b879f3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c6ec2b95d7b407fa5940f22cac38674":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62ee7d58c130460a8d5b3191c9f1a09a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17a6de20321e499496bded4ded56491e","placeholder":"â€‹","style":"IPY_MODEL_d74890aad64e4073a2678e4b45845fcc","value":"â€‡1999/1999â€‡[00:00&lt;00:00,â€‡4975.27â€‡examples/s]"}},"6331cfbad78b4333b449ebb1cae9768a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a59c458f4b814aa5834a9339f302f3ff","placeholder":"â€‹","style":"IPY_MODEL_1c4f4b68d5664b11b4381729d9ed50a1","value":"Login successful"}},"6390658c89624f49a3500cc42cb6bebd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_095b1fdf16054838a2aa095013bf5e80","max":300067,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46a29f2b03b84861b1635db9b60baa58","value":300067}},"6877dc049f1f4cfcaa6543722ca86f83":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"CheckboxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_198e5d3720554cce85fb2e07748a7502","style":"IPY_MODEL_76ffc23adec545d5a74c297200afaf0a","value":true}},"6dd9d64106bd49aaa996b51f668abb30":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70c03e9fb07948b285a587d3ff383bf1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74f8b298804c43558770b0023e07667d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_9bfdbe3d159f442593c930f45e46cea6","IPY_MODEL_42fc0328683a41b49bff59ecc7ddf333","IPY_MODEL_4939f27669f84693b3e194d9b9988a50","IPY_MODEL_6331cfbad78b4333b449ebb1cae9768a"],"layout":"IPY_MODEL_1ffa46644d9649d79488a67b9807fa17"}},"76ffc23adec545d5a74c297200afaf0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8443d92afc384e11bb35061e7e919247":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9171d2cc920a48c78369faeae7e9270b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fc3e440c06244aea69f870ad18f8d56","placeholder":"â€‹","style":"IPY_MODEL_112b78607bd74539908eb130a6e74c43","value":"â€‡300067/300067â€‡[01:00&lt;00:00,â€‡5119.06â€‡examples/s]"}},"98b441436f1a4ff1bb84f51c21a10b59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bfdbe3d159f442593c930f45e46cea6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4070f80e77974ebcbef6930827281e4d","placeholder":"â€‹","style":"IPY_MODEL_ece337489ad4425895141c64e9b33f82","value":"Token is valid (permission: write)."}},"9fc3e440c06244aea69f870ad18f8d56":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1a5e42e0075444d97e6b148647aef75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a27602f6fa91495cb20a3b2940d5feaa":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a59b7c532e954407acd3603d171db2e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb1807db12734f3396a4440d8c489b74","IPY_MODEL_6390658c89624f49a3500cc42cb6bebd","IPY_MODEL_9171d2cc920a48c78369faeae7e9270b"],"layout":"IPY_MODEL_a27602f6fa91495cb20a3b2940d5feaa"}},"a59c458f4b814aa5834a9339f302f3ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae776ab7fccf455fbe80fd63dd756a17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bb1807db12734f3396a4440d8c489b74":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5c6ec2b95d7b407fa5940f22cac38674","placeholder":"â€‹","style":"IPY_MODEL_8443d92afc384e11bb35061e7e919247","value":"Map:â€‡100%"}},"c49e0b631a434fedac13e3d1446ee674":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cac306aaa83d4aada75976ddc2634181":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d74890aad64e4073a2678e4b45845fcc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da8e7dcf6b6345e8a317ca1a9abd9e43":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33b0c5cd32ef49aebbf8967cbf43ca6a","IPY_MODEL_252947faf2194db19807627731ff64b6","IPY_MODEL_62ee7d58c130460a8d5b3191c9f1a09a"],"layout":"IPY_MODEL_6dd9d64106bd49aaa996b51f668abb30"}},"e09d695bcc134d90a5de448029a576c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e89d4f3fd687453980a092c7f38e7f07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e92da37428de48529f8243fc8d985ab1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ece337489ad4425895141c64e9b33f82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f629e0e642264e80ba825aba1b0b2a0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f860524d831d43c5b38a515cba1d04cf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1a5e42e0075444d97e6b148647aef75","placeholder":"â€‹","style":"IPY_MODEL_f629e0e642264e80ba825aba1b0b2a0a","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}}}}},"nbformat":4,"nbformat_minor":5}
